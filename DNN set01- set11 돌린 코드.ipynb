{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM ,Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import layers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rate=pd.read_csv('data/USD_KRW.csv')\n",
    "rate.columns=['date', 'rate']\n",
    "rate=rate.set_index('date')\n",
    "\n",
    "rate_train_01=rate.loc['2018-02-01': '2019-01-31' ]\n",
    "rate_train_02=rate.loc['2018-03-01': '2019-02-31' ]\n",
    "rate_train_03=rate.loc['2018-04-01': '2019-03-31' ]\n",
    "rate_train_04=rate.loc['2018-05-01': '2019-04-31' ]\n",
    "rate_train_05=rate.loc['2018-06-01': '2019-05-31' ]\n",
    "rate_train_06=rate.loc['2018-07-01': '2019-06-31' ]\n",
    "rate_train_07=rate.loc['2018-08-01': '2019-07-31' ]\n",
    "rate_train_08=rate.loc['2018-09-01': '2019-08-31' ]\n",
    "rate_train_09=rate.loc['2018-10-01': '2019-09-31' ]\n",
    "rate_train_10=rate.loc['2018-11-01': '2019-10-31' ]\n",
    "rate_train_11=rate.loc['2018-12-01': '2019-11-31' ]\n",
    "#rate_train_12=rate.loc['2019-01-01': '2019-12-31' ]\n",
    "\n",
    "rate_test_01=rate.loc['2019-02-01':'2019-02-31']\n",
    "rate_test_02=rate.loc['2019-03-01':'2019-03-31']\n",
    "rate_test_03=rate.loc['2019-04-01':'2019-04-31']\n",
    "rate_test_04=rate.loc['2019-05-01':'2019-05-31']\n",
    "rate_test_05=rate.loc['2019-06-01':'2019-06-31']\n",
    "rate_test_06=rate.loc['2019-07-01':'2019-07-31']\n",
    "rate_test_07=rate.loc['2019-08-01':'2019-08-31']\n",
    "rate_test_08=rate.loc['2019-09-01':'2019-09-31']\n",
    "rate_test_09=rate.loc['2019-10-01':'2019-10-31']\n",
    "rate_test_10=rate.loc['2019-11-01':'2019-11-31']\n",
    "rate_test_11=rate.loc['2019-12-01':'2019-12-31']\n",
    "#rate_test_12=rate.loc['2020-01-01':'2020-01-31']\n",
    "\n",
    "kolib=pd.read_csv('data/KORIBOR.csv')\n",
    "kolib=kolib[['date', '6month']]\n",
    "kolib.columns=['date','krlibor']\n",
    "kolib=kolib.set_index('date')\n",
    "\n",
    "kolib_train_01=kolib.loc['2018-02-01': '2019-01-31' ]\n",
    "kolib_train_02=kolib.loc['2018-03-01': '2019-02-31' ]\n",
    "kolib_train_03=kolib.loc['2018-04-01': '2019-03-31' ]\n",
    "kolib_train_04=kolib.loc['2018-05-01': '2019-04-31' ]\n",
    "kolib_train_05=kolib.loc['2018-06-01': '2019-05-31' ]\n",
    "kolib_train_06=kolib.loc['2018-07-01': '2019-06-31' ]\n",
    "kolib_train_07=kolib.loc['2018-08-01': '2019-07-31' ]\n",
    "kolib_train_08=kolib.loc['2018-09-01': '2019-08-31' ]\n",
    "kolib_train_09=kolib.loc['2018-10-01': '2019-09-31' ]\n",
    "kolib_train_10=kolib.loc['2018-11-01': '2019-10-31' ]\n",
    "kolib_train_11=kolib.loc['2018-12-01': '2019-11-31' ]\n",
    "#kolib_train_12=kolib.loc['2019-01-01': '2019-12-31' ]\n",
    "\n",
    "kolib_test_01=kolib.loc['2019-02-01':'2019-02-31']\n",
    "kolib_test_02=kolib.loc['2019-03-01':'2019-03-31']\n",
    "kolib_test_03=kolib.loc['2019-04-01':'2019-04-31']\n",
    "kolib_test_04=kolib.loc['2019-05-01':'2019-05-31']\n",
    "kolib_test_05=kolib.loc['2019-06-01':'2019-06-31']\n",
    "kolib_test_06=kolib.loc['2019-07-01':'2019-07-31']\n",
    "kolib_test_07=kolib.loc['2019-08-01':'2019-08-31']\n",
    "kolib_test_08=kolib.loc['2019-09-01':'2019-09-31']\n",
    "kolib_test_09=kolib.loc['2019-10-01':'2019-10-31']\n",
    "kolib_test_10=kolib.loc['2019-11-01':'2019-11-31']\n",
    "kolib_test_11=kolib.loc['2019-12-01':'2019-12-31']\n",
    "#kolib_test_12=kolib.loc['2020-01-01':'2020-01-31']\n",
    "\n",
    "uslib=pd.read_csv('data/libor.csv')\n",
    "uslib.columns=['date','uslibor']\n",
    "uslib=uslib.set_index('date')\n",
    "\n",
    "# 데이터에 일부분에 . 이 존재함. \n",
    "# 국가 공휴일은 삭제하고 필요한 값은 평균을 활용\n",
    "del uslib['uslibor']['2018-12-25']\n",
    "del uslib['uslibor']['2019-01-01']\n",
    "del uslib['uslibor']['2019-05-27']\n",
    "del uslib['uslibor']['2019-12-25']\n",
    "del uslib['uslibor']['2020-01-01']\n",
    "\n",
    "uslib['uslibor'].loc['2018-03-30'] = (float(uslib['uslibor'].loc['2018-03-29']) * 2/3 \n",
    "                                            + float(uslib['uslibor'].loc['2018-04-03']) * 1/3)\n",
    "\n",
    "uslib['uslibor'].loc['2018-04-02'] = (float(uslib['uslibor'].loc['2018-03-29']) * 1/3 \n",
    "                                            + float(uslib['uslibor'].loc['2018-04-03']) * 2/3)\n",
    "\n",
    "uslib['uslibor'].loc['2018-05-07'] = (float(uslib['uslibor'].loc['2018-05-04']) * 1/2 \n",
    "                                            + float(uslib['uslibor'].loc['2018-05-08']) * 1/2)\n",
    "\n",
    "uslib['uslibor'].loc['2018-05-28'] = (float(uslib['uslibor'].loc['2018-05-25']) * 1/2 \n",
    "                                            + float(uslib['uslibor'].loc['2018-05-29']) * 1/2)\n",
    "\n",
    "uslib['uslibor'].loc['2018-08-27'] = (float(uslib['uslibor'].loc['2018-08-24']) * 1/2 \n",
    "                                            + float(uslib['uslibor'].loc['2018-08-28']) * 1/2)\n",
    "\n",
    "uslib['uslibor'].loc['2018-12-26'] = (float(uslib['uslibor'].loc['2018-12-24']) * 2/3 \n",
    "                                            + float(uslib['uslibor'].loc['2018-12-27']) * 1/3)\n",
    "\n",
    "uslib['uslibor'].loc['2019-04-19'] = (float(uslib['uslibor'].loc['2019-04-23']) * 2/3 \n",
    "                                            + float(uslib['uslibor'].loc['2019-04-18']) * 1/3)\n",
    "\n",
    "uslib['uslibor'].loc['2019-04-22'] = (float(uslib['uslibor'].loc['2019-04-18']) * 2/3 \n",
    "                                            + float(uslib['uslibor'].loc['2019-04-23']) * 1/3)\n",
    "\n",
    "uslib['uslibor'].loc['2019-05-06'] = (float(uslib['uslibor'].loc['2019-05-03']) * 1/2\n",
    "                                            + float(uslib['uslibor'].loc['2019-05-07']) * 1/2)\n",
    "\n",
    "uslib['uslibor'].loc['2019-08-26'] = (float(uslib['uslibor'].loc['2019-08-23']) * 1/2\n",
    "                                            + float(uslib['uslibor'].loc['2019-08-27']) * 1/2)\n",
    "\n",
    "uslib['uslibor'].loc['2019-12-26'] = (float(uslib['uslibor'].loc['2019-12-24']) * 2/3 \n",
    "                                            + float(uslib['uslibor'].loc['2019-12-27']) * 1/3)\n",
    "\n",
    "# 값이 문자형으로 되어있어서 실수형으로 변경\n",
    "uslib['uslibor']=uslib['uslibor'].apply(pd.to_numeric)\n",
    "\n",
    "uslib_train_01=uslib.loc['2018-02-01': '2019-01-31' ]\n",
    "uslib_train_02=uslib.loc['2018-03-01': '2019-02-31' ]\n",
    "uslib_train_03=uslib.loc['2018-04-01': '2019-03-31' ]\n",
    "uslib_train_04=uslib.loc['2018-05-01': '2019-04-31' ]\n",
    "uslib_train_05=uslib.loc['2018-06-01': '2019-05-31' ]\n",
    "uslib_train_06=uslib.loc['2018-07-01': '2019-06-31' ]\n",
    "uslib_train_07=uslib.loc['2018-08-01': '2019-07-31' ]\n",
    "uslib_train_08=uslib.loc['2018-09-01': '2019-08-31' ]\n",
    "uslib_train_09=uslib.loc['2018-10-01': '2019-09-31' ]\n",
    "uslib_train_10=uslib.loc['2018-11-01': '2019-10-31' ]\n",
    "uslib_train_11=uslib.loc['2018-12-01': '2019-11-31' ]\n",
    "#uslib_train_12=uslib.loc['2019-01-01': '2019-12-31' ]\n",
    "\n",
    "uslib_test_01=uslib.loc['2019-02-01':'2019-02-31']\n",
    "uslib_test_02=uslib.loc['2019-03-01':'2019-03-31']\n",
    "uslib_test_03=uslib.loc['2019-04-01':'2019-04-31']\n",
    "uslib_test_04=uslib.loc['2019-05-01':'2019-05-31']\n",
    "uslib_test_05=uslib.loc['2019-06-01':'2019-06-31']\n",
    "uslib_test_06=uslib.loc['2019-07-01':'2019-07-31']\n",
    "uslib_test_07=uslib.loc['2019-08-01':'2019-08-31']\n",
    "uslib_test_08=uslib.loc['2019-09-01':'2019-09-31']\n",
    "uslib_test_09=uslib.loc['2019-10-01':'2019-10-31']\n",
    "uslib_test_10=uslib.loc['2019-11-01':'2019-11-31']\n",
    "uslib_test_11=uslib.loc['2019-12-01':'2019-12-31']\n",
    "#uslib_test_12=uslib.loc['2020-01-01':'2020-01-31']\n",
    "\n",
    "snp500=pd.read_csv('data/S&P500.csv')\n",
    "snp500 = snp500[::-1]\n",
    "snp500=snp500[['날짜','종가']]\n",
    "snp500.columns=['date','snp500']\n",
    "\n",
    "\n",
    "snp500_date=[]\n",
    "for i in snp500['date']:\n",
    "    snp500_date.append(i.replace('년 ','-').replace('월 ','-').replace('일',''))\n",
    "\n",
    "snp500_price=[]\n",
    "for i in snp500['snp500']:\n",
    "    i=i.replace(',','')\n",
    "    snp500_price.append(np.float(i))\n",
    "\n",
    "snp500['date']=snp500_date\n",
    "snp500['snp500']=snp500_price\n",
    "snp500=snp500.set_index('date')\n",
    "\n",
    "snp500_train_01=snp500.loc['2018-02-01': '2019-01-31' ]\n",
    "snp500_train_02=snp500.loc['2018-03-01': '2019-02-31' ]\n",
    "snp500_train_03=snp500.loc['2018-04-01': '2019-03-31' ]\n",
    "snp500_train_04=snp500.loc['2018-05-01': '2019-04-31' ]\n",
    "snp500_train_05=snp500.loc['2018-06-01': '2019-05-31' ]\n",
    "snp500_train_06=snp500.loc['2018-07-01': '2019-06-31' ]\n",
    "snp500_train_07=snp500.loc['2018-08-01': '2019-07-31' ]\n",
    "snp500_train_08=snp500.loc['2018-09-01': '2019-08-31' ]\n",
    "snp500_train_09=snp500.loc['2018-10-01': '2019-09-31' ]\n",
    "snp500_train_10=snp500.loc['2018-11-01': '2019-10-31' ]\n",
    "snp500_train_11=snp500.loc['2018-12-01': '2019-11-31' ]\n",
    "#snp500_train_12=snp500.loc['2019-01-01': '2019-12-31' ]\n",
    "\n",
    "snp500_test_01=snp500.loc['2019-02-01':'2019-02-31']\n",
    "snp500_test_02=snp500.loc['2019-03-01':'2019-03-31']\n",
    "snp500_test_03=snp500.loc['2019-04-01':'2019-04-31']\n",
    "snp500_test_04=snp500.loc['2019-05-01':'2019-05-31']\n",
    "snp500_test_05=snp500.loc['2019-06-01':'2019-06-31']\n",
    "snp500_test_06=snp500.loc['2019-07-01':'2019-07-31']\n",
    "snp500_test_07=snp500.loc['2019-08-01':'2019-08-31']\n",
    "snp500_test_08=snp500.loc['2019-09-01':'2019-09-31']\n",
    "snp500_test_09=snp500.loc['2019-10-01':'2019-10-31']\n",
    "snp500_test_10=snp500.loc['2019-11-01':'2019-11-31']\n",
    "snp500_test_11=snp500.loc['2019-12-01':'2019-12-31']\n",
    "#snp500_test_12=snp500.loc['2020-01-01':'2020-01-31']\n",
    "\n",
    "kospi200=pd.read_csv('data/KOSPI200.csv')\n",
    "kospi200=kospi200[['날짜','종가']]\n",
    "kospi200.columns=['date','kospi200']\n",
    "kospi200 = kospi200[::-1]\n",
    "\n",
    "kospi_date=[]\n",
    "for i in kospi200['date']:\n",
    "    kospi_date.append(i.replace('년 ','-').replace('월 ','-').replace('일',''))\n",
    "\n",
    "kospi200['date']=kospi_date\n",
    "kospi200=kospi200.set_index('date')\n",
    "\n",
    "kospi200_train_01=kospi200.loc['2018-02-01': '2019-01-31' ]\n",
    "kospi200_train_02=kospi200.loc['2018-03-01': '2019-02-31' ]\n",
    "kospi200_train_03=kospi200.loc['2018-04-01': '2019-03-31' ]\n",
    "kospi200_train_04=kospi200.loc['2018-05-01': '2019-04-31' ]\n",
    "kospi200_train_05=kospi200.loc['2018-06-01': '2019-05-31' ]\n",
    "kospi200_train_06=kospi200.loc['2018-07-01': '2019-06-31' ]\n",
    "kospi200_train_07=kospi200.loc['2018-08-01': '2019-07-31' ]\n",
    "kospi200_train_08=kospi200.loc['2018-09-01': '2019-08-31' ]\n",
    "kospi200_train_09=kospi200.loc['2018-10-01': '2019-09-31' ]\n",
    "kospi200_train_10=kospi200.loc['2018-11-01': '2019-10-31' ]\n",
    "kospi200_train_11=kospi200.loc['2018-12-01': '2019-11-31' ]\n",
    "#kospi200_train_12=kospi200.loc['2019-01-01': '2019-12-31' ]\n",
    "\n",
    "kospi200_test_01=kospi200.loc['2019-02-01':'2019-02-31']\n",
    "kospi200_test_02=kospi200.loc['2019-03-01':'2019-03-31']\n",
    "kospi200_test_03=kospi200.loc['2019-04-01':'2019-04-31']\n",
    "kospi200_test_04=kospi200.loc['2019-05-01':'2019-05-31']\n",
    "kospi200_test_05=kospi200.loc['2019-06-01':'2019-06-31']\n",
    "kospi200_test_06=kospi200.loc['2019-07-01':'2019-07-31']\n",
    "kospi200_test_07=kospi200.loc['2019-08-01':'2019-08-31']\n",
    "kospi200_test_08=kospi200.loc['2019-09-01':'2019-09-31']\n",
    "kospi200_test_09=kospi200.loc['2019-10-01':'2019-10-31']\n",
    "kospi200_test_10=kospi200.loc['2019-11-01':'2019-11-31']\n",
    "kospi200_test_11=kospi200.loc['2019-12-01':'2019-12-31']\n",
    "#kospi200_test_12=kospi200.loc['2020-01-01':'2020-01-31']\n",
    "\n",
    "data_train_01=pd.concat([rate_train_01,kolib_train_01,uslib_train_01,snp500_train_01,kospi200_train_01], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_01=pd.concat([rate_test_01,kolib_test_01,uslib_test_01,snp500_test_01,kospi200_test_01], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_02=pd.concat([rate_train_02,kolib_train_02,uslib_train_02,snp500_train_02,kospi200_train_02], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_02=pd.concat([rate_test_02,kolib_test_02,uslib_test_02,snp500_test_02,kospi200_test_02], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_03=pd.concat([rate_train_03,kolib_train_03,uslib_train_03,snp500_train_03,kospi200_train_03], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_03=pd.concat([rate_test_03,kolib_test_03,uslib_test_03,snp500_test_03,kospi200_test_03], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_04=pd.concat([rate_train_04,kolib_train_04,uslib_train_04,snp500_train_04,kospi200_train_04], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_04=pd.concat([rate_test_04,kolib_test_04,uslib_test_04,snp500_test_04,kospi200_test_04], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_05=pd.concat([rate_train_05,kolib_train_05,uslib_train_05,snp500_train_05,kospi200_train_05], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_05=pd.concat([rate_test_05,kolib_test_05,uslib_test_05,snp500_test_05,kospi200_test_05], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_06=pd.concat([rate_train_06,kolib_train_06,uslib_train_06,snp500_train_06,kospi200_train_06], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_06=pd.concat([rate_test_06,kolib_test_06,uslib_test_06,snp500_test_06,kospi200_test_06], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_07=pd.concat([rate_train_07,kolib_train_07,uslib_train_07,snp500_train_07,kospi200_train_07], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_07=pd.concat([rate_test_07,kolib_test_07,uslib_test_07,snp500_test_07,kospi200_test_07], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_08=pd.concat([rate_train_08,kolib_train_08,uslib_train_08,snp500_train_08,kospi200_train_08], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_08=pd.concat([rate_test_08,kolib_test_08,uslib_test_08,snp500_test_08,kospi200_test_08], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_09=pd.concat([rate_train_09,kolib_train_09,uslib_train_09,snp500_train_09,kospi200_train_09], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_09=pd.concat([rate_test_09,kolib_test_09,uslib_test_09,snp500_test_09,kospi200_test_09], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_10=pd.concat([rate_train_10,kolib_train_10,uslib_train_10,snp500_train_10,kospi200_train_10], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_10=pd.concat([rate_test_10,kolib_test_10,uslib_test_10,snp500_test_10,kospi200_test_10], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "data_train_11=pd.concat([rate_train_11,kolib_train_11,uslib_train_11,snp500_train_11,kospi200_train_11], \n",
    "                       axis=1,  join = 'inner', sort=False)\n",
    "data_test_11=pd.concat([rate_test_11,kolib_test_11,uslib_test_11,snp500_test_11,kospi200_test_11], \n",
    "                      axis=1,  join = 'inner', sort=False)\n",
    "#ata_train_12=pd.concat([rate_train_12,kolib_train_12,uslib_train_12,snp500_train_12,kospi200_train_12], \n",
    "#                      axis=1,  join = 'inner', sort=False)\n",
    "#ata_test_12=pd.concat([rate_test_12,kolib_test_12,uslib_test_12,snp500_test_12,kospi200_test_12], \n",
    "#                     axis=1,  join = 'inner', sort=False)\n",
    "data_train_01['y']=data_train_01['rate'].shift(-1)\n",
    "data_train_02['y']=data_train_02['rate'].shift(-1)\n",
    "data_train_03['y']=data_train_03['rate'].shift(-1)\n",
    "data_train_04['y']=data_train_04['rate'].shift(-1)\n",
    "data_train_05['y']=data_train_05['rate'].shift(-1)\n",
    "data_train_06['y']=data_train_06['rate'].shift(-1)\n",
    "data_train_07['y']=data_train_07['rate'].shift(-1)\n",
    "data_train_08['y']=data_train_08['rate'].shift(-1)\n",
    "data_train_09['y']=data_train_09['rate'].shift(-1)\n",
    "data_train_10['y']=data_train_10['rate'].shift(-1)\n",
    "data_train_11['y']=data_train_11['rate'].shift(-1)\n",
    "#data_train_12['y']=data_train_01['rate'].shift(-1)\n",
    "data_test_01['y']=data_test_01['rate'].shift(-1)\n",
    "data_test_02['y']=data_test_02['rate'].shift(-1)\n",
    "data_test_03['y']=data_test_03['rate'].shift(-1)\n",
    "data_test_04['y']=data_test_04['rate'].shift(-1)\n",
    "data_test_05['y']=data_test_05['rate'].shift(-1)\n",
    "data_test_06['y']=data_test_06['rate'].shift(-1)\n",
    "data_test_07['y']=data_test_07['rate'].shift(-1)\n",
    "data_test_08['y']=data_test_08['rate'].shift(-1)\n",
    "data_test_09['y']=data_test_09['rate'].shift(-1)\n",
    "data_test_10['y']=data_test_10['rate'].shift(-1)\n",
    "data_test_11['y']=data_test_11['rate'].shift(-1)\n",
    "#data_test_12['y']=data_test_12['rate'].shift(-1)\n",
    "data_train_01['y'][-1]=rate_test_01.iloc[0]\n",
    "data_train_01=data_train_01[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_02['y'][-1]=rate_test_02.iloc[0]\n",
    "data_train_02=data_train_02[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_03['y'][-1]=rate_test_03.iloc[0]\n",
    "data_train_03=data_train_03[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_04['y'][-1]=rate_test_04.iloc[0]\n",
    "data_train_04=data_train_04[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_05['y'][-1]=rate_test_05.iloc[0]\n",
    "data_train_05=data_train_05[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_06['y'][-1]=rate_test_06.iloc[0]\n",
    "data_train_06=data_train_06[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_07['y'][-1]=rate_test_07.iloc[0]\n",
    "data_train_07=data_train_07[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_08['y'][-1]=rate_test_08.iloc[0]\n",
    "data_train_08=data_train_08[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_09['y'][-1]=rate_test_09.iloc[0]\n",
    "data_train_09=data_train_09[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_10['y'][-1]=rate_test_10.iloc[0]\n",
    "data_train_10=data_train_10[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_train_11['y'][-1]=rate_test_11.iloc[0]\n",
    "data_train_11=data_train_11[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "#data_train_12['y'][-1]=rate_test_12.iloc[0]\n",
    "#data_train_12=data_train_12[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "\n",
    "data_test_01['y'][-1]=rate_test_01.iloc[0]\n",
    "data_test_01=data_test_01[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_01=data_test_01[:-1]\n",
    "\n",
    "data_test_02['y'][-1]=rate_test_02.iloc[0]\n",
    "data_test_02=data_test_02[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_02=data_test_02[:-1]\n",
    "\n",
    "data_test_03['y'][-1]=rate_test_03.iloc[0]\n",
    "data_test_03=data_test_03[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_03=data_test_03[:-1]\n",
    "\n",
    "data_test_04['y'][-1]=rate_test_04.iloc[0]\n",
    "data_test_04=data_test_04[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_04=data_test_04[:-1]\n",
    "\n",
    "data_test_05['y'][-1]=rate_test_05.iloc[0]\n",
    "data_test_05=data_test_05[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_05=data_test_05[:-1]\n",
    "\n",
    "data_test_06['y'][-1]=rate_test_06.iloc[0]\n",
    "data_test_06=data_test_06[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_06=data_test_06[:-1]\n",
    "\n",
    "data_test_07['y'][-1]=rate_test_07.iloc[0]\n",
    "data_test_07=data_test_07[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_07=data_test_07[:-1]\n",
    "\n",
    "data_test_08['y'][-1]=rate_test_08.iloc[0]\n",
    "data_test_08=data_test_08[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_08=data_test_08[:-1]\n",
    "\n",
    "data_test_09['y'][-1]=rate_test_09.iloc[0]\n",
    "data_test_09=data_test_09[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_09=data_test_09[:-1]\n",
    "\n",
    "data_test_10['y'][-1]=rate_test_10.iloc[0]\n",
    "data_test_10=data_test_10[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_10=data_test_10[:-1]\n",
    "\n",
    "data_test_11['y'][-1]=rate_test_11.iloc[0]\n",
    "data_test_11=data_test_11[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "data_test_11=data_test_11[:-1]\n",
    "\n",
    "#data_test_12['y'][-1]=rate_test_12.iloc[0]\n",
    "#data_test_12=data_test_12[['krlibor','uslibor','snp500','kospi200','y']]\n",
    "#data_test_12=data_test_12[:-1]\n",
    "train_input_01 = data_train_01[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_01 = data_train_01['y']\n",
    "test_input_01 = data_test_01[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_01 = data_test_01['y']\n",
    "\n",
    "train_input_02 = data_train_02[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_02 = data_train_02['y']\n",
    "test_input_02 = data_test_02[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_02 = data_test_02['y']\n",
    "\n",
    "train_input_03= data_train_03[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_03 = data_train_03['y']\n",
    "test_input_03 = data_test_03[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_03 = data_test_03['y']\n",
    "\n",
    "train_input_04= data_train_04[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_04 = data_train_04['y']\n",
    "test_input_04 = data_test_04[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_04 = data_test_04['y']\n",
    "\n",
    "train_input_05= data_train_05[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_05 = data_train_05['y']\n",
    "test_input_05 = data_test_05[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_05 = data_test_05['y']\n",
    "\n",
    "train_input_06= data_train_06[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_06 = data_train_06['y']\n",
    "test_input_06 = data_test_06[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_06 = data_test_06['y']\n",
    "\n",
    "train_input_07 = data_train_07[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_07 = data_train_07['y']\n",
    "test_input_07 = data_test_07[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_07 = data_test_07['y']\n",
    "\n",
    "train_input_08 = data_train_08[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_08 = data_train_08['y']\n",
    "test_input_08 = data_test_08[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_08 = data_test_08['y']\n",
    "\n",
    "train_input_09 = data_train_09[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_09 = data_train_09['y']\n",
    "test_input_09 = data_test_09[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_09 = data_test_09['y']\n",
    "\n",
    "train_input_10 = data_train_10[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_10 = data_train_10['y']\n",
    "test_input_10 = data_test_10[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_10 = data_test_10['y']\n",
    "\n",
    "train_input_11 = data_train_11[['krlibor','uslibor','snp500','kospi200']]\n",
    "train_target_11 = data_train_11['y']\n",
    "test_input_11 = data_test_11[['krlibor','uslibor','snp500','kospi200']]\n",
    "test_target_11 = data_test_11['y']\n",
    "\n",
    "#train_input_12 = data_train_12[['krlibor','uslibor','snp500','kospi200']]\n",
    "#train_target_12 = data_train_12['y']\n",
    "#test_input_12 = data_test_12[['krlibor','uslibor','snp500','kospi200']]\n",
    "#test_target_12 = data_test_12['y']\n",
    "mean_01 = train_input_01.mean(axis=0)\n",
    "std_01 = train_input_01.std(axis=0)\n",
    "train_input_01 -= mean_01\n",
    "train_input_01 /= std_01\n",
    "test_input_01 -= mean_01\n",
    "test_input_01 /= std_01\n",
    "\n",
    "mean_02 = train_input_02.mean(axis=0)\n",
    "std_02 = train_input_02.std(axis=0)\n",
    "train_input_02 -= mean_02\n",
    "train_input_02 /= std_02\n",
    "test_input_02 -= mean_02\n",
    "test_input_02 /= std_02\n",
    "\n",
    "mean_03 = train_input_03.mean(axis=0)\n",
    "std_03 = train_input_03.std(axis=0)\n",
    "train_input_03 -= mean_03\n",
    "train_input_03 /= std_03\n",
    "test_input_03 -= mean_03\n",
    "test_input_03 /= std_03\n",
    "\n",
    "mean_04 = train_input_04.mean(axis=0)\n",
    "std_04 = train_input_04.std(axis=0)\n",
    "train_input_04 -= mean_04\n",
    "train_input_04 /= std_04\n",
    "test_input_04 -= mean_04\n",
    "test_input_04 /= std_04\n",
    "\n",
    "mean_05 = train_input_05.mean(axis=0)\n",
    "std_05 = train_input_05.std(axis=0)\n",
    "train_input_05 -= mean_05\n",
    "train_input_05 /= std_05\n",
    "test_input_05 -= mean_05\n",
    "test_input_05 /= std_05\n",
    "\n",
    "mean_06 = train_input_06.mean(axis=0)\n",
    "std_06 = train_input_06.std(axis=0)\n",
    "train_input_06 -= mean_06\n",
    "train_input_06 /= std_06\n",
    "test_input_06 -= mean_06\n",
    "test_input_06 /= std_06\n",
    "\n",
    "mean_07 = train_input_07.mean(axis=0)\n",
    "std_07 = train_input_07.std(axis=0)\n",
    "train_input_07 -= mean_07\n",
    "train_input_07 /= std_07\n",
    "test_input_07 -= mean_07\n",
    "test_input_07 /= std_07\n",
    "\n",
    "mean_08 = train_input_08.mean(axis=0)\n",
    "std_08 = train_input_08.std(axis=0)\n",
    "train_input_08 -= mean_08\n",
    "train_input_08 /= std_08\n",
    "test_input_08 -= mean_08\n",
    "test_input_08 /= std_08\n",
    "\n",
    "mean_09 = train_input_09.mean(axis=0)\n",
    "std_09 = train_input_09.std(axis=0)\n",
    "train_input_09 -= mean_09\n",
    "train_input_09 /= std_09\n",
    "test_input_09 -= mean_09\n",
    "test_input_09 /= std_09\n",
    "\n",
    "mean_10 = train_input_10.mean(axis=0)\n",
    "std_10 = train_input_10.std(axis=0)\n",
    "train_input_10 -= mean_10\n",
    "train_input_10 /= std_10\n",
    "test_input_10 -= mean_10\n",
    "test_input_10 /= std_10\n",
    "\n",
    "mean_11 = train_input_11.mean(axis=0)\n",
    "std_11 = train_input_11.std(axis=0)\n",
    "train_input_11 -= mean_11\n",
    "train_input_11 /= std_11\n",
    "test_input_11 -= mean_11\n",
    "test_input_11 /= std_11\n",
    "\n",
    "#mean_12 = train_input_12.mean(axis=0)\n",
    "#std_12 = train_input_12.std(axis=0)\n",
    "#train_input_12 -= mean_12\n",
    "#train_input_12 /= std_12\n",
    "#test_input_12 -= mean_12\n",
    "#test_input_12 /= std_12\n",
    "\n",
    "train_input_01=np.array(train_input_01)\n",
    "train_input_01=train_input_01.reshape(train_input_01.shape[0],4)\n",
    "\n",
    "train_input_02=np.array(train_input_02)\n",
    "train_input_02=train_input_02.reshape(train_input_02.shape[0],4)\n",
    "\n",
    "train_input_03=np.array(train_input_03)\n",
    "train_input_03=train_input_03.reshape(train_input_03.shape[0],4)\n",
    "\n",
    "train_input_04=np.array(train_input_04)\n",
    "train_input_04=train_input_04.reshape(train_input_04.shape[0],4)\n",
    "\n",
    "train_input_05=np.array(train_input_05)\n",
    "train_input_05=train_input_05.reshape(train_input_05.shape[0],4)\n",
    "\n",
    "train_input_06=np.array(train_input_06)\n",
    "train_input_06=train_input_06.reshape(train_input_06.shape[0],4)\n",
    "\n",
    "train_input_07=np.array(train_input_07)\n",
    "train_input_07=train_input_07.reshape(train_input_07.shape[0],4)\n",
    "\n",
    "train_input_08=np.array(train_input_08)\n",
    "train_input_08=train_input_08.reshape(train_input_08.shape[0],4)\n",
    "\n",
    "train_input_09=np.array(train_input_09)\n",
    "train_input_09=train_input_09.reshape(train_input_09.shape[0],4)\n",
    "\n",
    "train_input_10=np.array(train_input_10)\n",
    "train_input_10=train_input_10.reshape(train_input_10.shape[0],4)\n",
    "\n",
    "train_input_11=np.array(train_input_11)\n",
    "train_input_11=train_input_11.reshape(train_input_11.shape[0],4)\n",
    "\n",
    "#train_input_12=np.array(train_input_12)\n",
    "#train_input_12=train_input_12.reshape(train_input_12.shape[0],4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 작성하기 (DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "165/165 [==============================] - 1s 3ms/step - loss: 1016088.9708 - val_loss: 3582023.3908\n",
      "Epoch 2/1000\n",
      "165/165 [==============================] - 0s 135us/step - loss: 272339.7689 - val_loss: 5033187.6655\n",
      "Epoch 3/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 165213.0560 - val_loss: 3787740.1796\n",
      "Epoch 4/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 135635.0338 - val_loss: 4001260.8415\n",
      "Epoch 5/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 111218.8725 - val_loss: 3418946.7905\n",
      "Epoch 6/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 94133.4930 - val_loss: 2582123.1937\n",
      "Epoch 7/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 78035.1917 - val_loss: 2356826.3750\n",
      "Epoch 8/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 62533.9790 - val_loss: 1953561.3600\n",
      "Epoch 9/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 51773.2679 - val_loss: 1351267.4129\n",
      "Epoch 10/1000\n",
      "165/165 [==============================] - 0s 141us/step - loss: 41716.1191 - val_loss: 1120004.6857\n",
      "Epoch 11/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 30350.3568 - val_loss: 849786.2584\n",
      "Epoch 12/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 24172.5880 - val_loss: 618742.8206\n",
      "Epoch 13/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 18689.0763 - val_loss: 498796.9635\n",
      "Epoch 14/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 14781.4877 - val_loss: 390861.0119\n",
      "Epoch 15/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 12295.9606 - val_loss: 246034.4521\n",
      "Epoch 16/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 8150.6419 - val_loss: 174175.4070\n",
      "Epoch 17/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 6478.0788 - val_loss: 139015.1692\n",
      "Epoch 18/1000\n",
      "165/165 [==============================] - 0s 142us/step - loss: 4909.7692 - val_loss: 96869.6072\n",
      "Epoch 19/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 4008.2410 - val_loss: 73979.2244\n",
      "Epoch 20/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 3332.7017 - val_loss: 55492.8695\n",
      "Epoch 21/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 2641.5287 - val_loss: 38044.2944\n",
      "Epoch 22/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 1893.7489 - val_loss: 24079.7680\n",
      "Epoch 23/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 1564.1405 - val_loss: 16910.2754\n",
      "Epoch 24/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 1305.5823 - val_loss: 11931.1918\n",
      "Epoch 25/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 1185.5129 - val_loss: 7809.5986\n",
      "Epoch 26/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 719.5036 - val_loss: 5639.5873\n",
      "Epoch 27/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 828.0162 - val_loss: 3255.3765\n",
      "Epoch 28/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 617.1613 - val_loss: 2529.7040\n",
      "Epoch 29/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 527.9586 - val_loss: 1530.1669\n",
      "Epoch 30/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 597.5237 - val_loss: 708.7683\n",
      "Epoch 31/1000\n",
      "165/165 [==============================] - 0s 179us/step - loss: 610.1616 - val_loss: 927.1014\n",
      "Epoch 32/1000\n",
      "165/165 [==============================] - 0s 178us/step - loss: 566.9991 - val_loss: 879.0549\n",
      "Epoch 33/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 531.5423 - val_loss: 469.0710\n",
      "Epoch 34/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 381.4708 - val_loss: 205.9535\n",
      "Epoch 35/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 408.9157 - val_loss: 136.1910\n",
      "Epoch 36/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 487.0694 - val_loss: 130.2518\n",
      "Epoch 37/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 446.0183 - val_loss: 160.2799\n",
      "Epoch 38/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 478.8953 - val_loss: 267.2092\n",
      "Epoch 39/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 429.5072 - val_loss: 132.9699\n",
      "Epoch 40/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 444.5558 - val_loss: 154.0758\n",
      "Epoch 41/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 425.0939 - val_loss: 95.2883\n",
      "Epoch 42/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 387.9342 - val_loss: 149.7494\n",
      "Epoch 43/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 408.9691 - val_loss: 289.7378\n",
      "Epoch 44/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 330.5109 - val_loss: 84.6625\n",
      "Epoch 45/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 470.2883 - val_loss: 105.1419\n",
      "Epoch 46/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 381.6565 - val_loss: 193.6125\n",
      "Epoch 47/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 456.3550 - val_loss: 313.8360\n",
      "Epoch 48/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 436.1003 - val_loss: 290.7163\n",
      "Epoch 49/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 430.4601 - val_loss: 513.7525\n",
      "Epoch 50/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 421.5500 - val_loss: 197.7900\n",
      "Epoch 51/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 440.2987 - val_loss: 261.0942\n",
      "Epoch 52/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 392.6523 - val_loss: 258.8244\n",
      "Epoch 53/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 440.0573 - val_loss: 347.6122\n",
      "Epoch 54/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 380.4749 - val_loss: 409.2997\n",
      "Epoch 55/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 441.2935 - val_loss: 962.9030\n",
      "Epoch 56/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 461.4045 - val_loss: 448.8608\n",
      "Epoch 57/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 469.7956 - val_loss: 256.0016\n",
      "Epoch 58/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 409.3232 - val_loss: 272.3567\n",
      "Epoch 59/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 424.4254 - val_loss: 407.9746\n",
      "Epoch 60/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 430.3497 - val_loss: 337.0459\n",
      "Epoch 61/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 426.2856 - val_loss: 603.5427\n",
      "Epoch 62/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 396.5087 - val_loss: 851.7799\n",
      "Epoch 63/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 484.5205 - val_loss: 282.9895\n",
      "Epoch 64/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 513.9089 - val_loss: 648.3254\n",
      "Epoch 65/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 411.4912 - val_loss: 284.0706\n",
      "Epoch 66/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 451.8460 - val_loss: 310.3869\n",
      "Epoch 67/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 451.8626 - val_loss: 121.6164\n",
      "Epoch 68/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 442.1050 - val_loss: 204.9721\n",
      "Epoch 69/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 493.1797 - val_loss: 171.0336\n",
      "Epoch 70/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 370.0531 - val_loss: 345.7311\n",
      "Epoch 71/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 366.6751 - val_loss: 534.7610\n",
      "Epoch 72/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 463.9542 - val_loss: 459.9727\n",
      "Epoch 73/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 418.8534 - val_loss: 392.6689\n",
      "Epoch 74/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 433.4671 - val_loss: 742.1854\n",
      "Epoch 75/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 433.3959 - val_loss: 255.9678\n",
      "Epoch 76/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 437.7151 - val_loss: 626.8901\n",
      "Epoch 77/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 381.7518 - val_loss: 302.3869\n",
      "Epoch 78/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 397.2041 - val_loss: 573.4312\n",
      "Epoch 79/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 356.8620 - val_loss: 449.6149\n",
      "Epoch 80/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 376.0394 - val_loss: 562.0609\n",
      "Epoch 81/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 480.7282 - val_loss: 261.2389\n",
      "Epoch 82/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 476.6267 - val_loss: 1124.5144\n",
      "Epoch 83/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 456.7010 - val_loss: 617.9761\n",
      "Epoch 84/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 379.0792 - val_loss: 946.6939\n",
      "Epoch 85/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 498.7921 - val_loss: 1029.2429\n",
      "Epoch 86/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 546.1576 - val_loss: 725.8665\n",
      "Epoch 87/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 357.5526 - val_loss: 881.8214\n",
      "Epoch 88/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 375.8656 - val_loss: 308.1816\n",
      "Epoch 89/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 440.3156 - val_loss: 578.5722\n",
      "Epoch 90/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 490.1750 - val_loss: 218.7642\n",
      "Epoch 91/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 404.4187 - val_loss: 397.4492\n",
      "Epoch 92/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 525.3478 - val_loss: 395.7355\n",
      "Epoch 93/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 417.7586 - val_loss: 490.6159\n",
      "Epoch 94/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 386.5695 - val_loss: 631.7667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f024ca01910>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_01 = Sequential()\n",
    "\n",
    "model_01.add(layers.Dense(units=1024, input_shape=(4,), ))\n",
    "model_01.add(Dropout(0.1))\n",
    "\n",
    "model_01.add(layers.Dense(units=1024))\n",
    "model_01.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model_01.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_01.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_01.fit(train_input_01, train_target_01, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 1069296.1677 - val_loss: 3207506.6461\n",
      "Epoch 2/1000\n",
      "164/164 [==============================] - 0s 142us/step - loss: 443524.1804 - val_loss: 4173350.8820\n",
      "Epoch 3/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 296348.7718 - val_loss: 3120827.3028\n",
      "Epoch 4/1000\n",
      "164/164 [==============================] - 0s 143us/step - loss: 207463.6657 - val_loss: 2487742.7042\n",
      "Epoch 5/1000\n",
      "164/164 [==============================] - 0s 145us/step - loss: 140099.9213 - val_loss: 1866330.7887\n",
      "Epoch 6/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 83338.8086 - val_loss: 1196605.3200\n",
      "Epoch 7/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 43969.7699 - val_loss: 680706.5018\n",
      "Epoch 8/1000\n",
      "164/164 [==============================] - 0s 143us/step - loss: 21626.0641 - val_loss: 432893.3121\n",
      "Epoch 9/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 14702.3191 - val_loss: 288031.2300\n",
      "Epoch 10/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 11051.1328 - val_loss: 240769.8671\n",
      "Epoch 11/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 9752.5804 - val_loss: 195627.9142\n",
      "Epoch 12/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 7866.0058 - val_loss: 176286.1545\n",
      "Epoch 13/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 8227.0303 - val_loss: 147930.4603\n",
      "Epoch 14/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 6753.8527 - val_loss: 129433.4011\n",
      "Epoch 15/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 5920.2426 - val_loss: 118999.5589\n",
      "Epoch 16/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 5624.2126 - val_loss: 109054.0424\n",
      "Epoch 17/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 5201.5998 - val_loss: 96302.5246\n",
      "Epoch 18/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 4373.6146 - val_loss: 87311.6056\n",
      "Epoch 19/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 4526.8347 - val_loss: 53259.7124\n",
      "Epoch 20/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 4120.0317 - val_loss: 56769.1195\n",
      "Epoch 21/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 3284.6905 - val_loss: 42943.2050\n",
      "Epoch 22/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 2884.0561 - val_loss: 34041.9345\n",
      "Epoch 23/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 3053.0601 - val_loss: 23637.2298\n",
      "Epoch 24/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 2616.8835 - val_loss: 28640.4536\n",
      "Epoch 25/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 2074.6195 - val_loss: 30204.1693\n",
      "Epoch 26/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 2054.5997 - val_loss: 28188.9963\n",
      "Epoch 27/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 1769.2823 - val_loss: 21865.6666\n",
      "Epoch 28/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 1977.6172 - val_loss: 20459.9149\n",
      "Epoch 29/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 1618.5976 - val_loss: 17413.1935\n",
      "Epoch 30/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 1514.3681 - val_loss: 17317.6025\n",
      "Epoch 31/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 1214.2825 - val_loss: 15733.3620\n",
      "Epoch 32/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 1169.1195 - val_loss: 10758.8058\n",
      "Epoch 33/1000\n",
      "164/164 [==============================] - 0s 140us/step - loss: 1122.0522 - val_loss: 8449.7135\n",
      "Epoch 34/1000\n",
      "164/164 [==============================] - 0s 145us/step - loss: 998.5203 - val_loss: 11542.0691\n",
      "Epoch 35/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 1217.5074 - val_loss: 10128.0160\n",
      "Epoch 36/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 904.8545 - val_loss: 9545.3252\n",
      "Epoch 37/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 706.8355 - val_loss: 6136.2554\n",
      "Epoch 38/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 932.6558 - val_loss: 5867.3905\n",
      "Epoch 39/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 708.4927 - val_loss: 6235.3112\n",
      "Epoch 40/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 797.7786 - val_loss: 3514.0952\n",
      "Epoch 41/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 758.8758 - val_loss: 3233.2584\n",
      "Epoch 42/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 738.4847 - val_loss: 2698.1191\n",
      "Epoch 43/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 580.2535 - val_loss: 1755.0921\n",
      "Epoch 44/1000\n",
      "164/164 [==============================] - 0s 136us/step - loss: 676.9051 - val_loss: 2190.8960\n",
      "Epoch 45/1000\n",
      "164/164 [==============================] - 0s 145us/step - loss: 608.0535 - val_loss: 2215.1055\n",
      "Epoch 46/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 574.7723 - val_loss: 2126.7152\n",
      "Epoch 47/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 524.4582 - val_loss: 1865.3867\n",
      "Epoch 48/1000\n",
      "164/164 [==============================] - 0s 139us/step - loss: 448.4369 - val_loss: 1127.8102\n",
      "Epoch 49/1000\n",
      "164/164 [==============================] - 0s 145us/step - loss: 525.5506 - val_loss: 1088.6817\n",
      "Epoch 50/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 562.1876 - val_loss: 931.1610\n",
      "Epoch 51/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 528.9842 - val_loss: 938.4942\n",
      "Epoch 52/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 508.4269 - val_loss: 723.6399\n",
      "Epoch 53/1000\n",
      "164/164 [==============================] - 0s 144us/step - loss: 550.6813 - val_loss: 1238.8719\n",
      "Epoch 54/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 525.7064 - val_loss: 681.9210\n",
      "Epoch 55/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 381.7272 - val_loss: 277.2628\n",
      "Epoch 56/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 472.2100 - val_loss: 362.6526\n",
      "Epoch 57/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 529.0681 - val_loss: 284.8823\n",
      "Epoch 58/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 417.2435 - val_loss: 343.2093\n",
      "Epoch 59/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 452.3426 - val_loss: 306.2492\n",
      "Epoch 60/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 464.8519 - val_loss: 381.1877\n",
      "Epoch 61/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 482.1194 - val_loss: 190.8631\n",
      "Epoch 62/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 402.0720 - val_loss: 185.9312\n",
      "Epoch 63/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 430.4556 - val_loss: 178.5591\n",
      "Epoch 64/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 542.7185 - val_loss: 468.1437\n",
      "Epoch 65/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 432.2767 - val_loss: 291.9546\n",
      "Epoch 66/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 410.6879 - val_loss: 155.4023\n",
      "Epoch 67/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 474.2440 - val_loss: 163.9247\n",
      "Epoch 68/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 377.0424 - val_loss: 203.4013\n",
      "Epoch 69/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 498.5041 - val_loss: 78.1192\n",
      "Epoch 70/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 439.8167 - val_loss: 104.4930\n",
      "Epoch 71/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 459.3743 - val_loss: 142.8327\n",
      "Epoch 72/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 519.0369 - val_loss: 67.9890\n",
      "Epoch 73/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 346.9509 - val_loss: 117.8688\n",
      "Epoch 74/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 390.9248 - val_loss: 97.5811\n",
      "Epoch 75/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 484.6152 - val_loss: 365.0039\n",
      "Epoch 76/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 486.6047 - val_loss: 111.6278\n",
      "Epoch 77/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 444.7444 - val_loss: 133.9890\n",
      "Epoch 78/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 415.1329 - val_loss: 75.7430\n",
      "Epoch 79/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 520.0027 - val_loss: 68.1884\n",
      "Epoch 80/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 356.5691 - val_loss: 80.9419\n",
      "Epoch 81/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 467.6856 - val_loss: 249.0702\n",
      "Epoch 82/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 439.4491 - val_loss: 198.8694\n",
      "Epoch 83/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 416.2555 - val_loss: 413.2775\n",
      "Epoch 84/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 364.9547 - val_loss: 117.7664\n",
      "Epoch 85/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 343.6805 - val_loss: 222.0304\n",
      "Epoch 86/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 392.7428 - val_loss: 124.4233\n",
      "Epoch 87/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 424.4267 - val_loss: 93.1265\n",
      "Epoch 88/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 440.2089 - val_loss: 75.2544\n",
      "Epoch 89/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 409.9513 - val_loss: 173.5543\n",
      "Epoch 90/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 377.0807 - val_loss: 147.5091\n",
      "Epoch 91/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 391.6254 - val_loss: 68.3428\n",
      "Epoch 92/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 448.4072 - val_loss: 203.2891\n",
      "Epoch 93/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 449.7938 - val_loss: 223.5763\n",
      "Epoch 94/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 505.3366 - val_loss: 301.7100\n",
      "Epoch 95/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 381.4126 - val_loss: 261.3457\n",
      "Epoch 96/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 400.6062 - val_loss: 291.9903\n",
      "Epoch 97/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 424.0421 - val_loss: 94.4524\n",
      "Epoch 98/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 541.6613 - val_loss: 240.6190\n",
      "Epoch 99/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 505.9933 - val_loss: 703.1182\n",
      "Epoch 100/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 439.4755 - val_loss: 261.4534\n",
      "Epoch 101/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 421.2497 - val_loss: 309.9757\n",
      "Epoch 102/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 448.3378 - val_loss: 361.3348\n",
      "Epoch 103/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 464.7809 - val_loss: 180.2397\n",
      "Epoch 104/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 424.1288 - val_loss: 343.4683\n",
      "Epoch 105/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 390.1256 - val_loss: 282.9195\n",
      "Epoch 106/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 397.5088 - val_loss: 104.3017\n",
      "Epoch 107/1000\n",
      "164/164 [==============================] - 0s 176us/step - loss: 416.1506 - val_loss: 601.8205\n",
      "Epoch 108/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 420.9224 - val_loss: 352.1281\n",
      "Epoch 109/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 426.5020 - val_loss: 453.0090\n",
      "Epoch 110/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 413.5681 - val_loss: 219.0872\n",
      "Epoch 111/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 468.2549 - val_loss: 310.0559\n",
      "Epoch 112/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 504.0895 - val_loss: 377.5919\n",
      "Epoch 113/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 339.7799 - val_loss: 428.6427\n",
      "Epoch 114/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 396.6530 - val_loss: 412.7291\n",
      "Epoch 115/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 635.7803 - val_loss: 535.7600\n",
      "Epoch 116/1000\n",
      "164/164 [==============================] - 0s 179us/step - loss: 450.8729 - val_loss: 403.0109\n",
      "Epoch 117/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 347.2316 - val_loss: 356.2549\n",
      "Epoch 118/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 427.1164 - val_loss: 218.2294\n",
      "Epoch 119/1000\n",
      "164/164 [==============================] - 0s 179us/step - loss: 419.3820 - val_loss: 268.6913\n",
      "Epoch 120/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 446.5480 - val_loss: 92.4216\n",
      "Epoch 121/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 460.9847 - val_loss: 303.3257\n",
      "Epoch 122/1000\n",
      "164/164 [==============================] - 0s 176us/step - loss: 439.2787 - val_loss: 189.8407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f024a1b6c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_02 = Sequential()\n",
    "\n",
    "model_02.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_02.add(Dropout(0.1))\n",
    "model_02.add(layers.Dense(units=1024))\n",
    "model_02.add(Dropout(0.1))\n",
    "model_02.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_02.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_02.fit(train_input_02, train_target_02, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 1108749.5899 - val_loss: 1966123.5264\n",
      "Epoch 2/1000\n",
      "164/164 [==============================] - 0s 138us/step - loss: 618993.5381 - val_loss: 2242784.0335\n",
      "Epoch 3/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 283759.0787 - val_loss: 1489272.8754\n",
      "Epoch 4/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 100286.7465 - val_loss: 843419.1725\n",
      "Epoch 5/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 35257.7059 - val_loss: 545337.8974\n",
      "Epoch 6/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 20780.1792 - val_loss: 380786.5414\n",
      "Epoch 7/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 15217.5377 - val_loss: 316098.4963\n",
      "Epoch 8/1000\n",
      "164/164 [==============================] - 0s 139us/step - loss: 13803.5179 - val_loss: 265285.9195\n",
      "Epoch 9/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 13451.9578 - val_loss: 254521.8512\n",
      "Epoch 10/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 11614.2053 - val_loss: 287006.3411\n",
      "Epoch 11/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 11033.2843 - val_loss: 175224.0048\n",
      "Epoch 12/1000\n",
      "164/164 [==============================] - 0s 145us/step - loss: 8943.8982 - val_loss: 180544.7833\n",
      "Epoch 13/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 8469.6547 - val_loss: 139008.4870\n",
      "Epoch 14/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 7741.1731 - val_loss: 170929.9880\n",
      "Epoch 15/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 6347.6873 - val_loss: 135070.0545\n",
      "Epoch 16/1000\n",
      "164/164 [==============================] - 0s 146us/step - loss: 5957.8629 - val_loss: 142621.0869\n",
      "Epoch 17/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 5924.3518 - val_loss: 124711.5521\n",
      "Epoch 18/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 5262.2976 - val_loss: 93842.8600\n",
      "Epoch 19/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 4642.1393 - val_loss: 94210.2383\n",
      "Epoch 20/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 4525.5678 - val_loss: 103009.5467\n",
      "Epoch 21/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 4295.6048 - val_loss: 69445.7534\n",
      "Epoch 22/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 3429.7882 - val_loss: 70444.5560\n",
      "Epoch 23/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 3578.2567 - val_loss: 57678.9631\n",
      "Epoch 24/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 3155.7476 - val_loss: 70193.3032\n",
      "Epoch 25/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 2920.5248 - val_loss: 52264.4669\n",
      "Epoch 26/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 2561.9695 - val_loss: 47295.0703\n",
      "Epoch 27/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 2316.7825 - val_loss: 31742.8419\n",
      "Epoch 28/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 2399.1607 - val_loss: 34989.9750\n",
      "Epoch 29/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 1895.4182 - val_loss: 39588.5055\n",
      "Epoch 30/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 1963.1060 - val_loss: 31796.3918\n",
      "Epoch 31/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 1870.6236 - val_loss: 37090.7854\n",
      "Epoch 32/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 1824.7235 - val_loss: 29171.3275\n",
      "Epoch 33/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 1598.0445 - val_loss: 25597.8958\n",
      "Epoch 34/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 1406.9140 - val_loss: 17411.5967\n",
      "Epoch 35/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 1421.0762 - val_loss: 13399.4754\n",
      "Epoch 36/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 1163.7004 - val_loss: 18165.3740\n",
      "Epoch 37/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 1135.1700 - val_loss: 14282.9720\n",
      "Epoch 38/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 1114.4429 - val_loss: 10858.7536\n",
      "Epoch 39/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 974.7933 - val_loss: 12438.0676\n",
      "Epoch 40/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 867.8794 - val_loss: 11241.7195\n",
      "Epoch 41/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 1000.0830 - val_loss: 11718.7455\n",
      "Epoch 42/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 802.5904 - val_loss: 9222.1172\n",
      "Epoch 43/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 761.8216 - val_loss: 7100.9080\n",
      "Epoch 44/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 639.8186 - val_loss: 8380.9832\n",
      "Epoch 45/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 688.5555 - val_loss: 5870.6938\n",
      "Epoch 46/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 648.8945 - val_loss: 6394.6644\n",
      "Epoch 47/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 564.9069 - val_loss: 6101.6917\n",
      "Epoch 48/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 727.2599 - val_loss: 7298.1111\n",
      "Epoch 49/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 627.2393 - val_loss: 4777.7389\n",
      "Epoch 50/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 580.2837 - val_loss: 4107.4706\n",
      "Epoch 51/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 580.1156 - val_loss: 4954.2736\n",
      "Epoch 52/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 561.0875 - val_loss: 3969.3510\n",
      "Epoch 53/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 621.7971 - val_loss: 4159.1383\n",
      "Epoch 54/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 620.7774 - val_loss: 4560.3735\n",
      "Epoch 55/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 572.8606 - val_loss: 3266.3387\n",
      "Epoch 56/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 503.3761 - val_loss: 3687.3320\n",
      "Epoch 57/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 607.8699 - val_loss: 2106.9447\n",
      "Epoch 58/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 553.8702 - val_loss: 3331.2881\n",
      "Epoch 59/1000\n",
      "164/164 [==============================] - 0s 140us/step - loss: 618.2975 - val_loss: 2570.7275\n",
      "Epoch 60/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 486.5719 - val_loss: 3696.7359\n",
      "Epoch 61/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 574.9550 - val_loss: 2264.1175\n",
      "Epoch 62/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 504.8100 - val_loss: 2716.5090\n",
      "Epoch 63/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 574.5416 - val_loss: 973.8370\n",
      "Epoch 64/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 516.9601 - val_loss: 976.4036\n",
      "Epoch 65/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 394.5974 - val_loss: 1304.8755\n",
      "Epoch 66/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 584.2317 - val_loss: 687.5263\n",
      "Epoch 67/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 458.3402 - val_loss: 1172.4305\n",
      "Epoch 68/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 409.8655 - val_loss: 1928.8705\n",
      "Epoch 69/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 525.1842 - val_loss: 1590.2030\n",
      "Epoch 70/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 448.7826 - val_loss: 1497.5259\n",
      "Epoch 71/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 497.9396 - val_loss: 832.9387\n",
      "Epoch 72/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 478.1278 - val_loss: 638.1052\n",
      "Epoch 73/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 471.9069 - val_loss: 650.2953\n",
      "Epoch 74/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 543.9885 - val_loss: 520.4891\n",
      "Epoch 75/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 416.9217 - val_loss: 672.1025\n",
      "Epoch 76/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 592.7478 - val_loss: 731.1058\n",
      "Epoch 77/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 381.1629 - val_loss: 546.4448\n",
      "Epoch 78/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 440.8248 - val_loss: 351.9507\n",
      "Epoch 79/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 456.9904 - val_loss: 565.6835\n",
      "Epoch 80/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 459.6214 - val_loss: 249.0449\n",
      "Epoch 81/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 360.5395 - val_loss: 185.7763\n",
      "Epoch 82/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 420.0394 - val_loss: 322.0747\n",
      "Epoch 83/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 504.0083 - val_loss: 434.2509\n",
      "Epoch 84/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 464.2648 - val_loss: 493.0590\n",
      "Epoch 85/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 345.0852 - val_loss: 600.7296\n",
      "Epoch 86/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 415.7681 - val_loss: 460.8203\n",
      "Epoch 87/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 374.7310 - val_loss: 1128.0718\n",
      "Epoch 88/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 473.8824 - val_loss: 582.1464\n",
      "Epoch 89/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 435.4399 - val_loss: 422.8865\n",
      "Epoch 90/1000\n",
      "164/164 [==============================] - 0s 182us/step - loss: 381.8079 - val_loss: 377.3267\n",
      "Epoch 91/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 428.6361 - val_loss: 292.1078\n",
      "Epoch 92/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 458.5881 - val_loss: 949.7190\n",
      "Epoch 93/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 520.8752 - val_loss: 396.7221\n",
      "Epoch 94/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 415.1005 - val_loss: 533.0354\n",
      "Epoch 95/1000\n",
      "164/164 [==============================] - 0s 146us/step - loss: 575.4830 - val_loss: 327.4688\n",
      "Epoch 96/1000\n",
      "164/164 [==============================] - 0s 144us/step - loss: 418.5047 - val_loss: 131.4217\n",
      "Epoch 97/1000\n",
      "164/164 [==============================] - 0s 144us/step - loss: 398.4653 - val_loss: 197.1894\n",
      "Epoch 98/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 481.0154 - val_loss: 302.3723\n",
      "Epoch 99/1000\n",
      "164/164 [==============================] - 0s 140us/step - loss: 413.0432 - val_loss: 255.7136\n",
      "Epoch 100/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 429.2944 - val_loss: 507.5642\n",
      "Epoch 101/1000\n",
      "164/164 [==============================] - 0s 144us/step - loss: 492.3243 - val_loss: 210.2456\n",
      "Epoch 102/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 369.3644 - val_loss: 337.1721\n",
      "Epoch 103/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 423.3297 - val_loss: 408.4151\n",
      "Epoch 104/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 463.1198 - val_loss: 403.1613\n",
      "Epoch 105/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 430.4804 - val_loss: 102.3010\n",
      "Epoch 106/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 382.0036 - val_loss: 144.1853\n",
      "Epoch 107/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 454.4482 - val_loss: 144.8783\n",
      "Epoch 108/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 462.2562 - val_loss: 311.1965\n",
      "Epoch 109/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 470.5578 - val_loss: 163.9545\n",
      "Epoch 110/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 391.7649 - val_loss: 215.7763\n",
      "Epoch 111/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 453.1083 - val_loss: 250.5951\n",
      "Epoch 112/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 392.0906 - val_loss: 146.7356\n",
      "Epoch 113/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 442.6745 - val_loss: 117.7962\n",
      "Epoch 114/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 388.3385 - val_loss: 157.9336\n",
      "Epoch 115/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 397.6186 - val_loss: 160.3301\n",
      "Epoch 116/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 403.9717 - val_loss: 465.9415\n",
      "Epoch 117/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 459.7931 - val_loss: 302.7699\n",
      "Epoch 118/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 437.1965 - val_loss: 163.1746\n",
      "Epoch 119/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 497.5655 - val_loss: 168.8126\n",
      "Epoch 120/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 547.1101 - val_loss: 171.8376\n",
      "Epoch 121/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 407.7472 - val_loss: 510.7901\n",
      "Epoch 122/1000\n",
      "164/164 [==============================] - 0s 184us/step - loss: 540.4417 - val_loss: 136.0971\n",
      "Epoch 123/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 391.7462 - val_loss: 262.1126\n",
      "Epoch 124/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 463.9696 - val_loss: 410.9717\n",
      "Epoch 125/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 496.4545 - val_loss: 215.9133\n",
      "Epoch 126/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 511.4059 - val_loss: 127.6234\n",
      "Epoch 127/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 362.5405 - val_loss: 85.6417\n",
      "Epoch 128/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 402.4856 - val_loss: 158.7416\n",
      "Epoch 129/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 430.7346 - val_loss: 297.1085\n",
      "Epoch 130/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 460.9095 - val_loss: 160.2835\n",
      "Epoch 131/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 474.2978 - val_loss: 166.8824\n",
      "Epoch 132/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 475.4552 - val_loss: 155.0954\n",
      "Epoch 133/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 397.7766 - val_loss: 143.7105\n",
      "Epoch 134/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 504.3605 - val_loss: 439.5468\n",
      "Epoch 135/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 391.1738 - val_loss: 208.5549\n",
      "Epoch 136/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 348.5180 - val_loss: 176.2229\n",
      "Epoch 137/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 395.8948 - val_loss: 190.9895\n",
      "Epoch 138/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 394.0401 - val_loss: 274.2596\n",
      "Epoch 139/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 480.5881 - val_loss: 639.2145\n",
      "Epoch 140/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 503.4187 - val_loss: 100.9736\n",
      "Epoch 141/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 445.9987 - val_loss: 161.3465\n",
      "Epoch 142/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 300.1356 - val_loss: 501.8644\n",
      "Epoch 143/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 346.4079 - val_loss: 192.3505\n",
      "Epoch 144/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 439.7492 - val_loss: 298.8572\n",
      "Epoch 145/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 379.6527 - val_loss: 253.1283\n",
      "Epoch 146/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 393.6027 - val_loss: 216.2383\n",
      "Epoch 147/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 412.9607 - val_loss: 122.5828\n",
      "Epoch 148/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 448.0214 - val_loss: 224.2727\n",
      "Epoch 149/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 501.8121 - val_loss: 203.9281\n",
      "Epoch 150/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 506.3050 - val_loss: 119.1337\n",
      "Epoch 151/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 502.4874 - val_loss: 299.8849\n",
      "Epoch 152/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 522.7255 - val_loss: 237.7501\n",
      "Epoch 153/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 513.1811 - val_loss: 188.8444\n",
      "Epoch 154/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 363.1101 - val_loss: 124.6251\n",
      "Epoch 155/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 396.0027 - val_loss: 161.2757\n",
      "Epoch 156/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 457.5331 - val_loss: 235.2384\n",
      "Epoch 157/1000\n",
      "164/164 [==============================] - 0s 178us/step - loss: 497.1594 - val_loss: 192.1704\n",
      "Epoch 158/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 469.2440 - val_loss: 233.3066\n",
      "Epoch 159/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 358.3217 - val_loss: 444.4648\n",
      "Epoch 160/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 514.1303 - val_loss: 148.6600\n",
      "Epoch 161/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 401.2678 - val_loss: 109.5388\n",
      "Epoch 162/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 398.1620 - val_loss: 187.6818\n",
      "Epoch 163/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 388.1903 - val_loss: 97.3856\n",
      "Epoch 164/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 406.1839 - val_loss: 190.8166\n",
      "Epoch 165/1000\n",
      "164/164 [==============================] - 0s 176us/step - loss: 511.2647 - val_loss: 482.5373\n",
      "Epoch 166/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 447.3389 - val_loss: 257.9052\n",
      "Epoch 167/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 427.8084 - val_loss: 167.7151\n",
      "Epoch 168/1000\n",
      "164/164 [==============================] - 0s 176us/step - loss: 427.3958 - val_loss: 143.2645\n",
      "Epoch 169/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 545.2782 - val_loss: 287.3242\n",
      "Epoch 170/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 432.3853 - val_loss: 146.0424\n",
      "Epoch 171/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 492.7268 - val_loss: 572.8527\n",
      "Epoch 172/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 537.6321 - val_loss: 115.8734\n",
      "Epoch 173/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 463.0568 - val_loss: 132.4540\n",
      "Epoch 174/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 332.0211 - val_loss: 214.7636\n",
      "Epoch 175/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 424.6613 - val_loss: 411.8369\n",
      "Epoch 176/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 506.8556 - val_loss: 194.5813\n",
      "Epoch 177/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 408.7309 - val_loss: 90.0561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f02365c2750>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_03 = Sequential()\n",
    "\n",
    "model_03.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_03.add(Dropout(0.1))\n",
    "model_03.add(layers.Dense(units=1024))\n",
    "model_03.add(Dropout(0.1))\n",
    "\n",
    "model_03.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_03.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_03.fit(train_input_03, train_target_03, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 164 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "164/164 [==============================] - 0s 1ms/step - loss: 1192562.3659 - val_loss: 1336623.1461\n",
      "Epoch 2/1000\n",
      "164/164 [==============================] - 0s 140us/step - loss: 702758.5930 - val_loss: 1132548.1092\n",
      "Epoch 3/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 168687.0831 - val_loss: 954674.2359\n",
      "Epoch 4/1000\n",
      "164/164 [==============================] - 0s 146us/step - loss: 85843.8020 - val_loss: 724935.1514\n",
      "Epoch 5/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 56009.1012 - val_loss: 531542.3732\n",
      "Epoch 6/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 50573.5335 - val_loss: 489585.0863\n",
      "Epoch 7/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 42545.3323 - val_loss: 432342.3609\n",
      "Epoch 8/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 34211.8974 - val_loss: 417999.7975\n",
      "Epoch 9/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 30383.0125 - val_loss: 351444.9261\n",
      "Epoch 10/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 27329.6699 - val_loss: 302272.4489\n",
      "Epoch 11/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 23411.1828 - val_loss: 271156.9102\n",
      "Epoch 12/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 21961.3770 - val_loss: 223539.1180\n",
      "Epoch 13/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 18900.5698 - val_loss: 237898.9283\n",
      "Epoch 14/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 17063.3823 - val_loss: 178149.4023\n",
      "Epoch 15/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 14582.1447 - val_loss: 160131.6769\n",
      "Epoch 16/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 12269.3498 - val_loss: 140087.6444\n",
      "Epoch 17/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 11208.6692 - val_loss: 128113.3585\n",
      "Epoch 18/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 9481.8329 - val_loss: 120512.6382\n",
      "Epoch 19/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 8675.6045 - val_loss: 92887.9987\n",
      "Epoch 20/1000\n",
      "164/164 [==============================] - 0s 144us/step - loss: 7256.3325 - val_loss: 73145.0108\n",
      "Epoch 21/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 6652.8421 - val_loss: 69953.9472\n",
      "Epoch 22/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 5909.5996 - val_loss: 62451.2425\n",
      "Epoch 23/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 4873.4671 - val_loss: 53086.3121\n",
      "Epoch 24/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 4455.2012 - val_loss: 47761.2320\n",
      "Epoch 25/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 3739.6454 - val_loss: 43158.2182\n",
      "Epoch 26/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 3546.1779 - val_loss: 40170.9539\n",
      "Epoch 27/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 3355.8196 - val_loss: 27498.4398\n",
      "Epoch 28/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 3059.1313 - val_loss: 28699.0426\n",
      "Epoch 29/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 2260.0795 - val_loss: 28412.1353\n",
      "Epoch 30/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 1978.1539 - val_loss: 22718.5676\n",
      "Epoch 31/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 1858.9480 - val_loss: 18724.0926\n",
      "Epoch 32/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 1875.0869 - val_loss: 17472.5403\n",
      "Epoch 33/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 1720.6215 - val_loss: 16976.0982\n",
      "Epoch 34/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 1441.6661 - val_loss: 13798.5920\n",
      "Epoch 35/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 1419.7406 - val_loss: 11901.6619\n",
      "Epoch 36/1000\n",
      "164/164 [==============================] - 0s 146us/step - loss: 1308.2325 - val_loss: 11531.0398\n",
      "Epoch 37/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 988.6864 - val_loss: 9539.5516\n",
      "Epoch 38/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 1097.4823 - val_loss: 9419.7191\n",
      "Epoch 39/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 992.9482 - val_loss: 8659.9391\n",
      "Epoch 40/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 754.4483 - val_loss: 6478.9894\n",
      "Epoch 41/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 994.9368 - val_loss: 5661.8292\n",
      "Epoch 42/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 866.8005 - val_loss: 4674.7473\n",
      "Epoch 43/1000\n",
      "164/164 [==============================] - 0s 145us/step - loss: 805.4514 - val_loss: 4533.6484\n",
      "Epoch 44/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 686.8307 - val_loss: 4077.3822\n",
      "Epoch 45/1000\n",
      "164/164 [==============================] - 0s 131us/step - loss: 584.8983 - val_loss: 4055.0845\n",
      "Epoch 46/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 770.2852 - val_loss: 3142.4341\n",
      "Epoch 47/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 537.6446 - val_loss: 3065.7435\n",
      "Epoch 48/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 544.3644 - val_loss: 2617.2992\n",
      "Epoch 49/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 510.8853 - val_loss: 2021.6638\n",
      "Epoch 50/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 580.7736 - val_loss: 2373.6184\n",
      "Epoch 51/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 510.0870 - val_loss: 2430.6481\n",
      "Epoch 52/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 534.4869 - val_loss: 1921.9906\n",
      "Epoch 53/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 480.4535 - val_loss: 1411.4683\n",
      "Epoch 54/1000\n",
      "164/164 [==============================] - 0s 151us/step - loss: 506.8322 - val_loss: 1658.2642\n",
      "Epoch 55/1000\n",
      "164/164 [==============================] - 0s 142us/step - loss: 485.9742 - val_loss: 1592.7423\n",
      "Epoch 56/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 640.7771 - val_loss: 756.7409\n",
      "Epoch 57/1000\n",
      "164/164 [==============================] - 0s 149us/step - loss: 580.1813 - val_loss: 750.7336\n",
      "Epoch 58/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 539.9636 - val_loss: 987.7439\n",
      "Epoch 59/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 398.5037 - val_loss: 1212.2532\n",
      "Epoch 60/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 493.2579 - val_loss: 961.2405\n",
      "Epoch 61/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 337.9602 - val_loss: 947.2122\n",
      "Epoch 62/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 420.6195 - val_loss: 881.1977\n",
      "Epoch 63/1000\n",
      "164/164 [==============================] - 0s 148us/step - loss: 441.5629 - val_loss: 1001.6320\n",
      "Epoch 64/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 566.2538 - val_loss: 714.4983\n",
      "Epoch 65/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 380.1648 - val_loss: 772.4884\n",
      "Epoch 66/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 468.2160 - val_loss: 969.4889\n",
      "Epoch 67/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 371.5816 - val_loss: 622.5098\n",
      "Epoch 68/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 434.7725 - val_loss: 683.0921\n",
      "Epoch 69/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 456.1576 - val_loss: 759.5773\n",
      "Epoch 70/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 503.2101 - val_loss: 616.2346\n",
      "Epoch 71/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 446.6038 - val_loss: 343.8481\n",
      "Epoch 72/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 379.6850 - val_loss: 674.9218\n",
      "Epoch 73/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 595.5205 - val_loss: 518.5398\n",
      "Epoch 74/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 409.2621 - val_loss: 431.6116\n",
      "Epoch 75/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 610.0488 - val_loss: 341.6556\n",
      "Epoch 76/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 450.3534 - val_loss: 410.1883\n",
      "Epoch 77/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 502.9545 - val_loss: 378.5414\n",
      "Epoch 78/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 377.2754 - val_loss: 260.7820\n",
      "Epoch 79/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 397.3711 - val_loss: 408.2835\n",
      "Epoch 80/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 429.9702 - val_loss: 505.0441\n",
      "Epoch 81/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 410.9778 - val_loss: 336.1402\n",
      "Epoch 82/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 486.4879 - val_loss: 436.3338\n",
      "Epoch 83/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 407.0951 - val_loss: 429.3092\n",
      "Epoch 84/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 437.6188 - val_loss: 303.9414\n",
      "Epoch 85/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 431.4523 - val_loss: 243.3297\n",
      "Epoch 86/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 322.2842 - val_loss: 413.0165\n",
      "Epoch 87/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 450.8172 - val_loss: 392.3917\n",
      "Epoch 88/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 502.3140 - val_loss: 237.0133\n",
      "Epoch 89/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 481.2596 - val_loss: 376.4780\n",
      "Epoch 90/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 387.4290 - val_loss: 312.6417\n",
      "Epoch 91/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 418.3372 - val_loss: 295.9687\n",
      "Epoch 92/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 321.4753 - val_loss: 331.3438\n",
      "Epoch 93/1000\n",
      "164/164 [==============================] - 0s 150us/step - loss: 557.3619 - val_loss: 205.5738\n",
      "Epoch 94/1000\n",
      "164/164 [==============================] - 0s 152us/step - loss: 524.3418 - val_loss: 225.2222\n",
      "Epoch 95/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 400.8305 - val_loss: 392.3232\n",
      "Epoch 96/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 347.3275 - val_loss: 299.5390\n",
      "Epoch 97/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 540.5550 - val_loss: 247.6551\n",
      "Epoch 98/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 491.9460 - val_loss: 289.6046\n",
      "Epoch 99/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 440.6201 - val_loss: 261.2376\n",
      "Epoch 100/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 406.0656 - val_loss: 294.8732\n",
      "Epoch 101/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 594.5646 - val_loss: 228.7561\n",
      "Epoch 102/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 430.9102 - val_loss: 372.2010\n",
      "Epoch 103/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 409.5686 - val_loss: 281.5228\n",
      "Epoch 104/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 429.2061 - val_loss: 314.9435\n",
      "Epoch 105/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 601.4339 - val_loss: 256.8739\n",
      "Epoch 106/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 497.2495 - val_loss: 196.6804\n",
      "Epoch 107/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 487.4612 - val_loss: 279.5347\n",
      "Epoch 108/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 346.9892 - val_loss: 184.1175\n",
      "Epoch 109/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 500.7580 - val_loss: 384.5368\n",
      "Epoch 110/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 449.5864 - val_loss: 263.7932\n",
      "Epoch 111/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 435.1495 - val_loss: 328.6544\n",
      "Epoch 112/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 395.4403 - val_loss: 311.0527\n",
      "Epoch 113/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 445.5424 - val_loss: 293.3251\n",
      "Epoch 114/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 372.6826 - val_loss: 448.8398\n",
      "Epoch 115/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 474.4466 - val_loss: 269.6330\n",
      "Epoch 116/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 414.7806 - val_loss: 381.3228\n",
      "Epoch 117/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 436.2187 - val_loss: 374.7459\n",
      "Epoch 118/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 388.0630 - val_loss: 543.1218\n",
      "Epoch 119/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 473.7386 - val_loss: 364.8993\n",
      "Epoch 120/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 451.1923 - val_loss: 201.4042\n",
      "Epoch 121/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 429.3058 - val_loss: 453.5447\n",
      "Epoch 122/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 384.0870 - val_loss: 529.2116\n",
      "Epoch 123/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 517.6966 - val_loss: 495.5633\n",
      "Epoch 124/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 473.0863 - val_loss: 352.7930\n",
      "Epoch 125/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 495.7179 - val_loss: 269.5912\n",
      "Epoch 126/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 489.3752 - val_loss: 243.7841\n",
      "Epoch 127/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 461.5483 - val_loss: 594.7345\n",
      "Epoch 128/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 422.5028 - val_loss: 272.0675\n",
      "Epoch 129/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 442.5710 - val_loss: 385.1481\n",
      "Epoch 130/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 438.8280 - val_loss: 459.2936\n",
      "Epoch 131/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 442.4368 - val_loss: 299.0362\n",
      "Epoch 132/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 454.6825 - val_loss: 262.3413\n",
      "Epoch 133/1000\n",
      "164/164 [==============================] - 0s 157us/step - loss: 429.7827 - val_loss: 328.0912\n",
      "Epoch 134/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 444.8321 - val_loss: 293.4010\n",
      "Epoch 135/1000\n",
      "164/164 [==============================] - 0s 154us/step - loss: 353.1138 - val_loss: 318.9723\n",
      "Epoch 136/1000\n",
      "164/164 [==============================] - 0s 153us/step - loss: 392.6971 - val_loss: 300.4905\n",
      "Epoch 137/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 459.0065 - val_loss: 336.1208\n",
      "Epoch 138/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 438.3451 - val_loss: 657.6638\n",
      "Epoch 139/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 543.7294 - val_loss: 333.9354\n",
      "Epoch 140/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 429.5857 - val_loss: 278.2912\n",
      "Epoch 141/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 366.6752 - val_loss: 387.7540\n",
      "Epoch 142/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 445.4469 - val_loss: 203.2079\n",
      "Epoch 143/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 504.7366 - val_loss: 325.3365\n",
      "Epoch 144/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 448.8223 - val_loss: 219.0882\n",
      "Epoch 145/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 423.4292 - val_loss: 291.4955\n",
      "Epoch 146/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 429.6966 - val_loss: 218.7295\n",
      "Epoch 147/1000\n",
      "164/164 [==============================] - 0s 156us/step - loss: 499.6590 - val_loss: 320.7553\n",
      "Epoch 148/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 440.1682 - val_loss: 418.5413\n",
      "Epoch 149/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 457.5734 - val_loss: 259.9681\n",
      "Epoch 150/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 441.8412 - val_loss: 240.0453\n",
      "Epoch 151/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 405.3656 - val_loss: 110.3440\n",
      "Epoch 152/1000\n",
      "164/164 [==============================] - 0s 147us/step - loss: 436.1412 - val_loss: 347.7859\n",
      "Epoch 153/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 406.6332 - val_loss: 354.6219\n",
      "Epoch 154/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 380.8460 - val_loss: 279.5861\n",
      "Epoch 155/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 400.4182 - val_loss: 430.0841\n",
      "Epoch 156/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 375.5121 - val_loss: 453.2351\n",
      "Epoch 157/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 440.4480 - val_loss: 300.1006\n",
      "Epoch 158/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 381.0566 - val_loss: 556.4317\n",
      "Epoch 159/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 550.1982 - val_loss: 944.8793\n",
      "Epoch 160/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 500.3985 - val_loss: 408.4970\n",
      "Epoch 161/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 477.2443 - val_loss: 323.3447\n",
      "Epoch 162/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 433.9096 - val_loss: 392.0119\n",
      "Epoch 163/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 446.7876 - val_loss: 640.5071\n",
      "Epoch 164/1000\n",
      "164/164 [==============================] - 0s 161us/step - loss: 561.8119 - val_loss: 396.1609\n",
      "Epoch 165/1000\n",
      "164/164 [==============================] - 0s 160us/step - loss: 354.7245 - val_loss: 330.3707\n",
      "Epoch 166/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 402.4706 - val_loss: 432.9846\n",
      "Epoch 167/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 412.8063 - val_loss: 347.9694\n",
      "Epoch 168/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 446.9028 - val_loss: 223.3206\n",
      "Epoch 169/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 506.4317 - val_loss: 214.2065\n",
      "Epoch 170/1000\n",
      "164/164 [==============================] - 0s 170us/step - loss: 410.5664 - val_loss: 260.2145\n",
      "Epoch 171/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 418.1038 - val_loss: 330.3580\n",
      "Epoch 172/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 440.4568 - val_loss: 349.7168\n",
      "Epoch 173/1000\n",
      "164/164 [==============================] - 0s 166us/step - loss: 493.3518 - val_loss: 224.0547\n",
      "Epoch 174/1000\n",
      "164/164 [==============================] - 0s 171us/step - loss: 464.5413 - val_loss: 256.9121\n",
      "Epoch 175/1000\n",
      "164/164 [==============================] - 0s 173us/step - loss: 407.5511 - val_loss: 355.3604\n",
      "Epoch 176/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 392.7460 - val_loss: 290.4168\n",
      "Epoch 177/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 366.6085 - val_loss: 249.5976\n",
      "Epoch 178/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 390.9539 - val_loss: 368.1648\n",
      "Epoch 179/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 430.0710 - val_loss: 359.1262\n",
      "Epoch 180/1000\n",
      "164/164 [==============================] - 0s 163us/step - loss: 421.2717 - val_loss: 337.8114\n",
      "Epoch 181/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 438.4428 - val_loss: 480.7445\n",
      "Epoch 182/1000\n",
      "164/164 [==============================] - 0s 174us/step - loss: 377.4112 - val_loss: 270.4725\n",
      "Epoch 183/1000\n",
      "164/164 [==============================] - 0s 159us/step - loss: 374.1173 - val_loss: 329.8757\n",
      "Epoch 184/1000\n",
      "164/164 [==============================] - 0s 155us/step - loss: 406.4781 - val_loss: 310.6660\n",
      "Epoch 185/1000\n",
      "164/164 [==============================] - 0s 168us/step - loss: 389.5354 - val_loss: 189.3492\n",
      "Epoch 186/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 392.0764 - val_loss: 337.9106\n",
      "Epoch 187/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 418.3779 - val_loss: 406.2286\n",
      "Epoch 188/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 497.9464 - val_loss: 380.0166\n",
      "Epoch 189/1000\n",
      "164/164 [==============================] - 0s 167us/step - loss: 454.2432 - val_loss: 384.1269\n",
      "Epoch 190/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 468.1169 - val_loss: 401.2369\n",
      "Epoch 191/1000\n",
      "164/164 [==============================] - 0s 175us/step - loss: 538.5769 - val_loss: 218.7491\n",
      "Epoch 192/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 471.4581 - val_loss: 289.7391\n",
      "Epoch 193/1000\n",
      "164/164 [==============================] - 0s 162us/step - loss: 499.6068 - val_loss: 347.2725\n",
      "Epoch 194/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 483.7285 - val_loss: 203.0209\n",
      "Epoch 195/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 405.2658 - val_loss: 267.9833\n",
      "Epoch 196/1000\n",
      "164/164 [==============================] - 0s 158us/step - loss: 467.8650 - val_loss: 258.6833\n",
      "Epoch 197/1000\n",
      "164/164 [==============================] - 0s 165us/step - loss: 403.0848 - val_loss: 205.1130\n",
      "Epoch 198/1000\n",
      "164/164 [==============================] - 0s 164us/step - loss: 423.8230 - val_loss: 241.4651\n",
      "Epoch 199/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 360.1998 - val_loss: 211.2553\n",
      "Epoch 200/1000\n",
      "164/164 [==============================] - 0s 169us/step - loss: 399.3053 - val_loss: 309.5677\n",
      "Epoch 201/1000\n",
      "164/164 [==============================] - 0s 172us/step - loss: 392.9765 - val_loss: 348.9673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f023651b150>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_04 = Sequential()\n",
    "\n",
    "model_04.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_04.add(Dropout(0.1))\n",
    "model_04.add(layers.Dense(units=1024))\n",
    "model_04.add(Dropout(0.1))\n",
    "\n",
    "model_04.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_04.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_04.fit(train_input_04, train_target_04, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1225770.6189 - val_loss: 1424388.3187\n",
      "Epoch 2/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 765673.6600 - val_loss: 1627494.2782\n",
      "Epoch 3/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 247132.5150 - val_loss: 1335459.7870\n",
      "Epoch 4/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 126079.1021 - val_loss: 974231.5493\n",
      "Epoch 5/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 96782.8669 - val_loss: 746639.4313\n",
      "Epoch 6/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 68865.4960 - val_loss: 575808.6232\n",
      "Epoch 7/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 54445.9124 - val_loss: 438261.6241\n",
      "Epoch 8/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 42678.0626 - val_loss: 360309.2526\n",
      "Epoch 9/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 32960.9120 - val_loss: 268269.0801\n",
      "Epoch 10/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 27548.4443 - val_loss: 218097.7438\n",
      "Epoch 11/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 21010.1209 - val_loss: 157059.6721\n",
      "Epoch 12/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 16320.3132 - val_loss: 139030.1875\n",
      "Epoch 13/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 13234.4012 - val_loss: 111401.7304\n",
      "Epoch 14/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 9874.6300 - val_loss: 93705.0398\n",
      "Epoch 15/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 8430.5275 - val_loss: 70335.8305\n",
      "Epoch 16/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 6504.5661 - val_loss: 59995.7632\n",
      "Epoch 17/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 4943.5798 - val_loss: 45390.9425\n",
      "Epoch 18/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 4072.9681 - val_loss: 34208.9912\n",
      "Epoch 19/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 3088.2773 - val_loss: 28098.5026\n",
      "Epoch 20/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 2506.6432 - val_loss: 20118.4071\n",
      "Epoch 21/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 2020.6190 - val_loss: 18649.0695\n",
      "Epoch 22/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 1523.2583 - val_loss: 15118.1669\n",
      "Epoch 23/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 1358.6128 - val_loss: 11609.9831\n",
      "Epoch 24/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 1046.6498 - val_loss: 9335.1678\n",
      "Epoch 25/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 987.6452 - val_loss: 8030.2263\n",
      "Epoch 26/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 891.7634 - val_loss: 6405.6073\n",
      "Epoch 27/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 808.2414 - val_loss: 5212.3667\n",
      "Epoch 28/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 615.1640 - val_loss: 4423.0299\n",
      "Epoch 29/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 593.9697 - val_loss: 3476.3856\n",
      "Epoch 30/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 638.0024 - val_loss: 2641.3401\n",
      "Epoch 31/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 611.6104 - val_loss: 2330.6525\n",
      "Epoch 32/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 440.4783 - val_loss: 1789.5427\n",
      "Epoch 33/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 424.7611 - val_loss: 1767.4634\n",
      "Epoch 34/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 433.4011 - val_loss: 1749.5429\n",
      "Epoch 35/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 503.9707 - val_loss: 1434.3244\n",
      "Epoch 36/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 451.5199 - val_loss: 1360.2468\n",
      "Epoch 37/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 508.0724 - val_loss: 995.1386\n",
      "Epoch 38/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 446.7194 - val_loss: 832.9298\n",
      "Epoch 39/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 558.3016 - val_loss: 1240.3622\n",
      "Epoch 40/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 575.9819 - val_loss: 967.5635\n",
      "Epoch 41/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 407.7283 - val_loss: 978.4849\n",
      "Epoch 42/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 406.5693 - val_loss: 915.2773\n",
      "Epoch 43/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 413.1056 - val_loss: 946.5683\n",
      "Epoch 44/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 495.9231 - val_loss: 549.9548\n",
      "Epoch 45/1000\n",
      "165/165 [==============================] - 0s 140us/step - loss: 429.5181 - val_loss: 691.6642\n",
      "Epoch 46/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 534.1573 - val_loss: 495.0483\n",
      "Epoch 47/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 391.0773 - val_loss: 776.7040\n",
      "Epoch 48/1000\n",
      "165/165 [==============================] - 0s 141us/step - loss: 462.2257 - val_loss: 666.8455\n",
      "Epoch 49/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 480.2848 - val_loss: 474.2243\n",
      "Epoch 50/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 489.4090 - val_loss: 794.3845\n",
      "Epoch 51/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 449.3315 - val_loss: 483.7548\n",
      "Epoch 52/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 377.8640 - val_loss: 459.9957\n",
      "Epoch 53/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 417.4141 - val_loss: 415.1930\n",
      "Epoch 54/1000\n",
      "165/165 [==============================] - 0s 143us/step - loss: 438.4582 - val_loss: 479.7067\n",
      "Epoch 55/1000\n",
      "165/165 [==============================] - 0s 143us/step - loss: 460.5153 - val_loss: 325.2387\n",
      "Epoch 56/1000\n",
      "165/165 [==============================] - 0s 142us/step - loss: 384.5182 - val_loss: 457.1709\n",
      "Epoch 57/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 511.7065 - val_loss: 494.4398\n",
      "Epoch 58/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 369.6745 - val_loss: 397.7427\n",
      "Epoch 59/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 400.8428 - val_loss: 527.2082\n",
      "Epoch 60/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 537.5500 - val_loss: 597.1670\n",
      "Epoch 61/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 475.0393 - val_loss: 529.6625\n",
      "Epoch 62/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 485.7753 - val_loss: 635.4759\n",
      "Epoch 63/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 437.3582 - val_loss: 760.4677\n",
      "Epoch 64/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 462.2147 - val_loss: 413.4486\n",
      "Epoch 65/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 410.0760 - val_loss: 577.6872\n",
      "Epoch 66/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 495.2631 - val_loss: 694.3563\n",
      "Epoch 67/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 453.0042 - val_loss: 448.9119\n",
      "Epoch 68/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 461.4119 - val_loss: 503.2518\n",
      "Epoch 69/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 423.4264 - val_loss: 546.9676\n",
      "Epoch 70/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 394.0718 - val_loss: 423.3257\n",
      "Epoch 71/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 446.9773 - val_loss: 340.6750\n",
      "Epoch 72/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 402.3978 - val_loss: 398.8038\n",
      "Epoch 73/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 387.1174 - val_loss: 399.7842\n",
      "Epoch 74/1000\n",
      "165/165 [==============================] - 0s 139us/step - loss: 481.1463 - val_loss: 707.3905\n",
      "Epoch 75/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 429.4665 - val_loss: 451.0135\n",
      "Epoch 76/1000\n",
      "165/165 [==============================] - 0s 141us/step - loss: 393.8653 - val_loss: 549.5977\n",
      "Epoch 77/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 385.9036 - val_loss: 491.4038\n",
      "Epoch 78/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 394.4466 - val_loss: 527.0371\n",
      "Epoch 79/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 406.8673 - val_loss: 581.2840\n",
      "Epoch 80/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 496.5900 - val_loss: 721.1439\n",
      "Epoch 81/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 433.8414 - val_loss: 471.2124\n",
      "Epoch 82/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 439.7606 - val_loss: 443.2872\n",
      "Epoch 83/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 377.6191 - val_loss: 506.8944\n",
      "Epoch 84/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 420.0108 - val_loss: 755.3484\n",
      "Epoch 85/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 356.1197 - val_loss: 773.3224\n",
      "Epoch 86/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 332.5737 - val_loss: 689.0865\n",
      "Epoch 87/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 425.2146 - val_loss: 648.0571\n",
      "Epoch 88/1000\n",
      "165/165 [==============================] - 0s 180us/step - loss: 420.7438 - val_loss: 558.5611\n",
      "Epoch 89/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 455.8607 - val_loss: 538.7029\n",
      "Epoch 90/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 401.9165 - val_loss: 696.1126\n",
      "Epoch 91/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 456.0060 - val_loss: 579.7158\n",
      "Epoch 92/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 356.7085 - val_loss: 660.4613\n",
      "Epoch 93/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 396.6908 - val_loss: 517.7341\n",
      "Epoch 94/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 486.2840 - val_loss: 382.6160\n",
      "Epoch 95/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 463.1422 - val_loss: 491.0094\n",
      "Epoch 96/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 390.8094 - val_loss: 485.1319\n",
      "Epoch 97/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 394.6531 - val_loss: 579.5237\n",
      "Epoch 98/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 443.3603 - val_loss: 526.0358\n",
      "Epoch 99/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 419.6790 - val_loss: 341.9394\n",
      "Epoch 100/1000\n",
      "165/165 [==============================] - 0s 181us/step - loss: 435.9528 - val_loss: 383.5116\n",
      "Epoch 101/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 428.6902 - val_loss: 768.3937\n",
      "Epoch 102/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 438.2541 - val_loss: 403.3529\n",
      "Epoch 103/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 397.4850 - val_loss: 460.4411\n",
      "Epoch 104/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 357.2522 - val_loss: 667.3380\n",
      "Epoch 105/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 470.0939 - val_loss: 503.5923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f02362e7550>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_05 = Sequential()\n",
    "\n",
    "model_05.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_05.add(Dropout(0.1))\n",
    "model_05.add(layers.Dense(units=1024))\n",
    "model_05.add(Dropout(0.1))\n",
    "\n",
    "model_05.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_05.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_05.fit(train_input_05, train_target_05, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1178243.8413 - val_loss: 1788938.7711\n",
      "Epoch 2/1000\n",
      "165/165 [==============================] - 0s 142us/step - loss: 488906.2761 - val_loss: 2804858.6831\n",
      "Epoch 3/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 115600.4384 - val_loss: 2666542.8873\n",
      "Epoch 4/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 81123.6524 - val_loss: 2511846.4859\n",
      "Epoch 5/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 70135.7967 - val_loss: 2121865.0211\n",
      "Epoch 6/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 64213.5630 - val_loss: 2024741.3028\n",
      "Epoch 7/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 59424.3307 - val_loss: 1856068.2148\n",
      "Epoch 8/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 54332.9011 - val_loss: 1587520.0915\n",
      "Epoch 9/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 49819.7274 - val_loss: 1367633.3239\n",
      "Epoch 10/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 41098.2385 - val_loss: 1255787.6514\n",
      "Epoch 11/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 36491.0437 - val_loss: 1142474.7746\n",
      "Epoch 12/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 33172.2169 - val_loss: 1063921.4366\n",
      "Epoch 13/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 29978.7124 - val_loss: 914545.4313\n",
      "Epoch 14/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 25597.1207 - val_loss: 825326.0035\n",
      "Epoch 15/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 25383.8521 - val_loss: 776158.8187\n",
      "Epoch 16/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 22187.9560 - val_loss: 684487.1162\n",
      "Epoch 17/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 19858.6943 - val_loss: 643943.9930\n",
      "Epoch 18/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 18312.6303 - val_loss: 596624.9419\n",
      "Epoch 19/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 17234.0913 - val_loss: 529776.5687\n",
      "Epoch 20/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 15038.2155 - val_loss: 487041.4745\n",
      "Epoch 21/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 13555.0203 - val_loss: 426754.3063\n",
      "Epoch 22/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 12546.6408 - val_loss: 386954.7597\n",
      "Epoch 23/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 11799.0766 - val_loss: 348906.3856\n",
      "Epoch 24/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 10329.2567 - val_loss: 308438.3600\n",
      "Epoch 25/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 9150.5833 - val_loss: 284518.3996\n",
      "Epoch 26/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 7462.4479 - val_loss: 275936.7095\n",
      "Epoch 27/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 7490.1709 - val_loss: 242885.9247\n",
      "Epoch 28/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 6311.3671 - val_loss: 213353.6523\n",
      "Epoch 29/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 6581.8445 - val_loss: 197005.7170\n",
      "Epoch 30/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 5644.3710 - val_loss: 182295.3341\n",
      "Epoch 31/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 4943.8491 - val_loss: 156128.7478\n",
      "Epoch 32/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 4273.9029 - val_loss: 141930.0766\n",
      "Epoch 33/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 4054.9398 - val_loss: 128473.8556\n",
      "Epoch 34/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 3827.9523 - val_loss: 110503.3169\n",
      "Epoch 35/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 3320.7063 - val_loss: 106327.2027\n",
      "Epoch 36/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 2860.8909 - val_loss: 95059.2912\n",
      "Epoch 37/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 2712.6914 - val_loss: 87882.9861\n",
      "Epoch 38/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 2443.8266 - val_loss: 71556.1930\n",
      "Epoch 39/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 1883.0122 - val_loss: 65622.0957\n",
      "Epoch 40/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 1784.9476 - val_loss: 59041.8170\n",
      "Epoch 41/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 1698.6934 - val_loss: 52988.7887\n",
      "Epoch 42/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 1648.7871 - val_loss: 44549.6465\n",
      "Epoch 43/1000\n",
      "165/165 [==============================] - 0s 139us/step - loss: 1772.0967 - val_loss: 43821.8771\n",
      "Epoch 44/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 1073.2174 - val_loss: 39292.4542\n",
      "Epoch 45/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 1154.4149 - val_loss: 36012.8697\n",
      "Epoch 46/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 1143.9040 - val_loss: 31028.0710\n",
      "Epoch 47/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 1040.2588 - val_loss: 28747.6132\n",
      "Epoch 48/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 938.6226 - val_loss: 23420.8562\n",
      "Epoch 49/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 831.9179 - val_loss: 21857.0787\n",
      "Epoch 50/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 817.3690 - val_loss: 19641.9354\n",
      "Epoch 51/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 714.4107 - val_loss: 17726.3581\n",
      "Epoch 52/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 613.9901 - val_loss: 16082.7955\n",
      "Epoch 53/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 636.2733 - val_loss: 12801.9231\n",
      "Epoch 54/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 583.8332 - val_loss: 13920.1346\n",
      "Epoch 55/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 764.9986 - val_loss: 13671.7929\n",
      "Epoch 56/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 491.9384 - val_loss: 12286.3702\n",
      "Epoch 57/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 493.6448 - val_loss: 9405.9701\n",
      "Epoch 58/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 477.7805 - val_loss: 9981.5657\n",
      "Epoch 59/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 494.5883 - val_loss: 9798.0221\n",
      "Epoch 60/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 546.9038 - val_loss: 8337.6647\n",
      "Epoch 61/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 476.1584 - val_loss: 6268.5298\n",
      "Epoch 62/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 492.4251 - val_loss: 6422.4488\n",
      "Epoch 63/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 423.4698 - val_loss: 5930.3270\n",
      "Epoch 64/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 425.4258 - val_loss: 5712.1587\n",
      "Epoch 65/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 436.6245 - val_loss: 5203.7232\n",
      "Epoch 66/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 400.3329 - val_loss: 4661.6611\n",
      "Epoch 67/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 535.7305 - val_loss: 4521.4337\n",
      "Epoch 68/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 481.6207 - val_loss: 4005.8396\n",
      "Epoch 69/1000\n",
      "165/165 [==============================] - 0s 140us/step - loss: 394.4865 - val_loss: 3781.5270\n",
      "Epoch 70/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 400.1673 - val_loss: 3568.3156\n",
      "Epoch 71/1000\n",
      "165/165 [==============================] - 0s 134us/step - loss: 330.1731 - val_loss: 3856.8143\n",
      "Epoch 72/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 565.7527 - val_loss: 3437.3417\n",
      "Epoch 73/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 460.7254 - val_loss: 3123.5087\n",
      "Epoch 74/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 406.0114 - val_loss: 2411.8123\n",
      "Epoch 75/1000\n",
      "165/165 [==============================] - 0s 141us/step - loss: 427.9754 - val_loss: 2907.2827\n",
      "Epoch 76/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 367.1479 - val_loss: 2633.2057\n",
      "Epoch 77/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 352.1872 - val_loss: 2771.1692\n",
      "Epoch 78/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 360.3090 - val_loss: 2757.2317\n",
      "Epoch 79/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 351.1364 - val_loss: 2627.3111\n",
      "Epoch 80/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 430.7160 - val_loss: 2542.0104\n",
      "Epoch 81/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 422.4094 - val_loss: 2268.8921\n",
      "Epoch 82/1000\n",
      "165/165 [==============================] - 0s 135us/step - loss: 338.9287 - val_loss: 2273.0335\n",
      "Epoch 83/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 323.5389 - val_loss: 2003.8500\n",
      "Epoch 84/1000\n",
      "165/165 [==============================] - 0s 137us/step - loss: 420.9000 - val_loss: 2593.6903\n",
      "Epoch 85/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 371.1513 - val_loss: 2228.4981\n",
      "Epoch 86/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 409.0742 - val_loss: 1682.9507\n",
      "Epoch 87/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 607.6192 - val_loss: 1983.6248\n",
      "Epoch 88/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 392.4389 - val_loss: 2061.6448\n",
      "Epoch 89/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 394.4599 - val_loss: 2339.8102\n",
      "Epoch 90/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 461.9127 - val_loss: 2106.7319\n",
      "Epoch 91/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 391.6362 - val_loss: 1938.4061\n",
      "Epoch 92/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 391.9951 - val_loss: 1717.5104\n",
      "Epoch 93/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 412.8960 - val_loss: 1680.5073\n",
      "Epoch 94/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 406.5083 - val_loss: 1968.1978\n",
      "Epoch 95/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 384.0688 - val_loss: 1824.9273\n",
      "Epoch 96/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 490.1567 - val_loss: 1885.3742\n",
      "Epoch 97/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 412.8275 - val_loss: 1770.7681\n",
      "Epoch 98/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 361.2639 - val_loss: 1766.2544\n",
      "Epoch 99/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 428.2909 - val_loss: 1925.8153\n",
      "Epoch 100/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 360.2506 - val_loss: 2539.5036\n",
      "Epoch 101/1000\n",
      "165/165 [==============================] - 0s 139us/step - loss: 367.3670 - val_loss: 2039.3040\n",
      "Epoch 102/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 445.8426 - val_loss: 1433.5548\n",
      "Epoch 103/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 425.1949 - val_loss: 1869.0348\n",
      "Epoch 104/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 358.2206 - val_loss: 1835.6097\n",
      "Epoch 105/1000\n",
      "165/165 [==============================] - 0s 135us/step - loss: 471.1340 - val_loss: 1608.7014\n",
      "Epoch 106/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 341.6409 - val_loss: 1701.5812\n",
      "Epoch 107/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 329.4521 - val_loss: 2110.8955\n",
      "Epoch 108/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 423.5682 - val_loss: 1915.0372\n",
      "Epoch 109/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 356.9673 - val_loss: 1642.1573\n",
      "Epoch 110/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 319.7240 - val_loss: 2214.5950\n",
      "Epoch 111/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 379.8037 - val_loss: 1956.8040\n",
      "Epoch 112/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 332.2905 - val_loss: 1807.1536\n",
      "Epoch 113/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 314.6991 - val_loss: 1856.1507\n",
      "Epoch 114/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 416.4163 - val_loss: 2219.0940\n",
      "Epoch 115/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 363.7831 - val_loss: 2185.4415\n",
      "Epoch 116/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 448.2648 - val_loss: 2192.8550\n",
      "Epoch 117/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 413.8990 - val_loss: 2015.7884\n",
      "Epoch 118/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 424.1820 - val_loss: 2098.3079\n",
      "Epoch 119/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 461.0218 - val_loss: 2115.6955\n",
      "Epoch 120/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 397.2723 - val_loss: 1955.2070\n",
      "Epoch 121/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 367.1929 - val_loss: 1769.2065\n",
      "Epoch 122/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 412.4512 - val_loss: 1619.0277\n",
      "Epoch 123/1000\n",
      "165/165 [==============================] - 0s 139us/step - loss: 389.9136 - val_loss: 1339.8958\n",
      "Epoch 124/1000\n",
      "165/165 [==============================] - 0s 143us/step - loss: 407.1905 - val_loss: 1434.1773\n",
      "Epoch 125/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 350.7936 - val_loss: 1436.1825\n",
      "Epoch 126/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 394.7125 - val_loss: 1516.3070\n",
      "Epoch 127/1000\n",
      "165/165 [==============================] - 0s 138us/step - loss: 364.4444 - val_loss: 1604.5481\n",
      "Epoch 128/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 418.3439 - val_loss: 1855.3961\n",
      "Epoch 129/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 487.8869 - val_loss: 1585.4772\n",
      "Epoch 130/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 457.8800 - val_loss: 1927.6521\n",
      "Epoch 131/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 381.9936 - val_loss: 2145.1853\n",
      "Epoch 132/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 449.6508 - val_loss: 1635.4787\n",
      "Epoch 133/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 427.2304 - val_loss: 1406.0061\n",
      "Epoch 134/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 357.9475 - val_loss: 1918.8410\n",
      "Epoch 135/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 463.9783 - val_loss: 1800.8789\n",
      "Epoch 136/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 394.6662 - val_loss: 1555.1553\n",
      "Epoch 137/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 374.5907 - val_loss: 1631.6923\n",
      "Epoch 138/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 389.3546 - val_loss: 1869.7392\n",
      "Epoch 139/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 343.7322 - val_loss: 1478.1912\n",
      "Epoch 140/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 357.3089 - val_loss: 1448.3975\n",
      "Epoch 141/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 365.7269 - val_loss: 1587.1909\n",
      "Epoch 142/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 369.1791 - val_loss: 1415.7333\n",
      "Epoch 143/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 411.1054 - val_loss: 1586.1222\n",
      "Epoch 144/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 337.5958 - val_loss: 1647.0689\n",
      "Epoch 145/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 425.7741 - val_loss: 1914.0541\n",
      "Epoch 146/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 427.6666 - val_loss: 1534.7429\n",
      "Epoch 147/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 374.6496 - val_loss: 1241.3212\n",
      "Epoch 148/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 342.3390 - val_loss: 1590.0263\n",
      "Epoch 149/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 405.5354 - val_loss: 1313.0910\n",
      "Epoch 150/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 328.8498 - val_loss: 1157.1174\n",
      "Epoch 151/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 395.3595 - val_loss: 1522.3117\n",
      "Epoch 152/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 447.3582 - val_loss: 1178.6769\n",
      "Epoch 153/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 363.0669 - val_loss: 1571.3393\n",
      "Epoch 154/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 389.5034 - val_loss: 1434.1335\n",
      "Epoch 155/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 355.4962 - val_loss: 1305.7344\n",
      "Epoch 156/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 483.4925 - val_loss: 1364.1177\n",
      "Epoch 157/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 393.2512 - val_loss: 1374.7533\n",
      "Epoch 158/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 432.9926 - val_loss: 1400.3708\n",
      "Epoch 159/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 372.2192 - val_loss: 1190.6127\n",
      "Epoch 160/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 336.6678 - val_loss: 1353.8571\n",
      "Epoch 161/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 381.4644 - val_loss: 1458.1791\n",
      "Epoch 162/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 371.0237 - val_loss: 1583.3016\n",
      "Epoch 163/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 425.3528 - val_loss: 1422.1006\n",
      "Epoch 164/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 406.9976 - val_loss: 1327.3691\n",
      "Epoch 165/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 341.5834 - val_loss: 1323.9171\n",
      "Epoch 166/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 348.6603 - val_loss: 1119.2212\n",
      "Epoch 167/1000\n",
      "165/165 [==============================] - 0s 179us/step - loss: 439.4738 - val_loss: 1456.7993\n",
      "Epoch 168/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 386.1579 - val_loss: 1368.2174\n",
      "Epoch 169/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 294.3158 - val_loss: 1326.7537\n",
      "Epoch 170/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 398.5848 - val_loss: 1502.3978\n",
      "Epoch 171/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 382.4647 - val_loss: 1633.0042\n",
      "Epoch 172/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 414.2538 - val_loss: 1575.7716\n",
      "Epoch 173/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 394.5788 - val_loss: 1722.7672\n",
      "Epoch 174/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 404.5177 - val_loss: 1740.1501\n",
      "Epoch 175/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 387.8176 - val_loss: 1655.8580\n",
      "Epoch 176/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 417.7270 - val_loss: 1764.7995\n",
      "Epoch 177/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 355.4929 - val_loss: 1893.7017\n",
      "Epoch 178/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 365.9003 - val_loss: 1723.8220\n",
      "Epoch 179/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 360.2002 - val_loss: 1848.8589\n",
      "Epoch 180/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 369.3164 - val_loss: 1794.8597\n",
      "Epoch 181/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 448.4938 - val_loss: 1882.0382\n",
      "Epoch 182/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 360.2761 - val_loss: 1845.9337\n",
      "Epoch 183/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 465.2383 - val_loss: 1765.4007\n",
      "Epoch 184/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 366.8820 - val_loss: 1899.7319\n",
      "Epoch 185/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 319.6876 - val_loss: 1751.8332\n",
      "Epoch 186/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 402.8344 - val_loss: 1878.3201\n",
      "Epoch 187/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 403.5202 - val_loss: 1713.9343\n",
      "Epoch 188/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 399.2522 - val_loss: 1718.3967\n",
      "Epoch 189/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 404.1170 - val_loss: 1742.5686\n",
      "Epoch 190/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 348.0758 - val_loss: 1643.4049\n",
      "Epoch 191/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 358.0313 - val_loss: 1955.5877\n",
      "Epoch 192/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 408.0433 - val_loss: 1881.6524\n",
      "Epoch 193/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 412.3508 - val_loss: 1679.8436\n",
      "Epoch 194/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 479.7900 - val_loss: 1959.7697\n",
      "Epoch 195/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 382.0044 - val_loss: 1825.3718\n",
      "Epoch 196/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 346.3278 - val_loss: 1995.4559\n",
      "Epoch 197/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 380.5362 - val_loss: 2084.0895\n",
      "Epoch 198/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 355.1447 - val_loss: 1805.6727\n",
      "Epoch 199/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 368.1252 - val_loss: 1644.7012\n",
      "Epoch 200/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 422.8600 - val_loss: 1700.4573\n",
      "Epoch 201/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 425.1544 - val_loss: 1930.8000\n",
      "Epoch 202/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 412.9576 - val_loss: 1718.8902\n",
      "Epoch 203/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 391.1760 - val_loss: 2302.7861\n",
      "Epoch 204/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 535.3089 - val_loss: 1897.8142\n",
      "Epoch 205/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 381.4071 - val_loss: 1805.2764\n",
      "Epoch 206/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 336.2872 - val_loss: 1691.1653\n",
      "Epoch 207/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 385.7359 - val_loss: 1587.2160\n",
      "Epoch 208/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 423.1585 - val_loss: 1775.0726\n",
      "Epoch 209/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 377.3567 - val_loss: 1655.9703\n",
      "Epoch 210/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 448.1361 - val_loss: 1425.5925\n",
      "Epoch 211/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 467.7784 - val_loss: 1564.1220\n",
      "Epoch 212/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 415.5930 - val_loss: 1921.5652\n",
      "Epoch 213/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 420.6415 - val_loss: 1578.5121\n",
      "Epoch 214/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 428.6622 - val_loss: 1729.7642\n",
      "Epoch 215/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 416.0539 - val_loss: 1953.9556\n",
      "Epoch 216/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 374.0698 - val_loss: 1637.4049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f021bf9b710>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_06 = Sequential()\n",
    "\n",
    "model_06.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_06.add(Dropout(0.1))\n",
    "model_06.add(layers.Dense(units=1024))\n",
    "model_06.add(Dropout(0.1))\n",
    "\n",
    "model_06.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_06.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_06.fit(train_input_06, train_target_06, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165 samples, validate on 72 samples\n",
      "Epoch 1/1000\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1130930.4417 - val_loss: 2803159.6111\n",
      "Epoch 2/1000\n",
      "165/165 [==============================] - 0s 131us/step - loss: 415611.0414 - val_loss: 3811587.8889\n",
      "Epoch 3/1000\n",
      "165/165 [==============================] - 0s 141us/step - loss: 104387.6343 - val_loss: 3997307.5000\n",
      "Epoch 4/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 69487.7028 - val_loss: 3981185.8889\n",
      "Epoch 5/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 68443.0022 - val_loss: 3295541.1667\n",
      "Epoch 6/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 55422.7132 - val_loss: 2880344.4722\n",
      "Epoch 7/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 50604.6571 - val_loss: 2863342.2222\n",
      "Epoch 8/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 47370.8098 - val_loss: 2579937.7778\n",
      "Epoch 9/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 41500.5422 - val_loss: 2374383.4167\n",
      "Epoch 10/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 39550.3940 - val_loss: 2136472.9722\n",
      "Epoch 11/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 35322.2344 - val_loss: 1971705.7500\n",
      "Epoch 12/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 34551.5449 - val_loss: 1787026.0000\n",
      "Epoch 13/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 30957.1167 - val_loss: 1786727.1111\n",
      "Epoch 14/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 29882.0966 - val_loss: 1505274.7500\n",
      "Epoch 15/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 25483.9888 - val_loss: 1420497.4167\n",
      "Epoch 16/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 23489.7157 - val_loss: 1313820.4583\n",
      "Epoch 17/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 21094.2346 - val_loss: 1188569.4167\n",
      "Epoch 18/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 20350.5458 - val_loss: 1078334.6389\n",
      "Epoch 19/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 18066.1171 - val_loss: 981760.4028\n",
      "Epoch 20/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 16964.2481 - val_loss: 941595.8611\n",
      "Epoch 21/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 14892.1529 - val_loss: 850831.9722\n",
      "Epoch 22/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 13999.8748 - val_loss: 782097.6111\n",
      "Epoch 23/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 12941.2500 - val_loss: 718848.8472\n",
      "Epoch 24/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 12093.1928 - val_loss: 620505.0208\n",
      "Epoch 25/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 10791.2904 - val_loss: 567884.8750\n",
      "Epoch 26/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 9831.4222 - val_loss: 550706.1458\n",
      "Epoch 27/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 9055.9273 - val_loss: 456798.1250\n",
      "Epoch 28/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 8325.6530 - val_loss: 438051.6458\n",
      "Epoch 29/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 7844.4166 - val_loss: 387227.6944\n",
      "Epoch 30/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 6274.4756 - val_loss: 342885.1667\n",
      "Epoch 31/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 5944.5043 - val_loss: 306320.0521\n",
      "Epoch 32/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 5284.9744 - val_loss: 281320.3993\n",
      "Epoch 33/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 4662.2334 - val_loss: 247106.3021\n",
      "Epoch 34/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 3953.0471 - val_loss: 226327.9097\n",
      "Epoch 35/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 4117.9481 - val_loss: 206197.0000\n",
      "Epoch 36/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 3782.6254 - val_loss: 168190.3993\n",
      "Epoch 37/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 3344.6948 - val_loss: 156365.1163\n",
      "Epoch 38/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 2788.3326 - val_loss: 140040.6302\n",
      "Epoch 39/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 2329.8479 - val_loss: 126559.4861\n",
      "Epoch 40/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 2618.3341 - val_loss: 113810.4774\n",
      "Epoch 41/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 1980.2378 - val_loss: 102722.8090\n",
      "Epoch 42/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 2090.3126 - val_loss: 88562.7639\n",
      "Epoch 43/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 1907.7675 - val_loss: 78612.5521\n",
      "Epoch 44/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 1614.8640 - val_loss: 70376.5868\n",
      "Epoch 45/1000\n",
      "165/165 [==============================] - 0s 142us/step - loss: 1294.0504 - val_loss: 64073.9062\n",
      "Epoch 46/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 1300.8959 - val_loss: 59175.8247\n",
      "Epoch 47/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 1008.1278 - val_loss: 51648.0278\n",
      "Epoch 48/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 1023.3172 - val_loss: 44937.5964\n",
      "Epoch 49/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 937.1473 - val_loss: 41970.2109\n",
      "Epoch 50/1000\n",
      "165/165 [==============================] - 0s 142us/step - loss: 952.0917 - val_loss: 33019.1189\n",
      "Epoch 51/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 987.9061 - val_loss: 33470.8438\n",
      "Epoch 52/1000\n",
      "165/165 [==============================] - 0s 143us/step - loss: 835.2657 - val_loss: 28680.6845\n",
      "Epoch 53/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 787.1867 - val_loss: 24111.6497\n",
      "Epoch 54/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 692.9804 - val_loss: 23104.3572\n",
      "Epoch 55/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 623.7216 - val_loss: 21561.0720\n",
      "Epoch 56/1000\n",
      "165/165 [==============================] - 0s 146us/step - loss: 649.0357 - val_loss: 17548.1168\n",
      "Epoch 57/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 632.7615 - val_loss: 15948.4336\n",
      "Epoch 58/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 593.8297 - val_loss: 12877.5551\n",
      "Epoch 59/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 562.0949 - val_loss: 11190.2370\n",
      "Epoch 60/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 598.3362 - val_loss: 10378.6387\n",
      "Epoch 61/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 518.8620 - val_loss: 10282.6146\n",
      "Epoch 62/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 506.1170 - val_loss: 9669.2493\n",
      "Epoch 63/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 410.2924 - val_loss: 10277.7556\n",
      "Epoch 64/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 601.9114 - val_loss: 8298.6315\n",
      "Epoch 65/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 466.0092 - val_loss: 8334.3507\n",
      "Epoch 66/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 424.0583 - val_loss: 5794.9112\n",
      "Epoch 67/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 377.0333 - val_loss: 5590.3056\n",
      "Epoch 68/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 436.3519 - val_loss: 5019.9409\n",
      "Epoch 69/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 414.9088 - val_loss: 4482.9708\n",
      "Epoch 70/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 421.8694 - val_loss: 4853.1965\n",
      "Epoch 71/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 401.1717 - val_loss: 4216.8337\n",
      "Epoch 72/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 386.5280 - val_loss: 3554.1547\n",
      "Epoch 73/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 426.1229 - val_loss: 3702.3580\n",
      "Epoch 74/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 354.6056 - val_loss: 3755.1909\n",
      "Epoch 75/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 463.7492 - val_loss: 3265.1692\n",
      "Epoch 76/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 418.0405 - val_loss: 2770.9832\n",
      "Epoch 77/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 392.9465 - val_loss: 2940.9331\n",
      "Epoch 78/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 376.2070 - val_loss: 2904.1069\n",
      "Epoch 79/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 379.9524 - val_loss: 3059.2142\n",
      "Epoch 80/1000\n",
      "165/165 [==============================] - 0s 178us/step - loss: 414.8165 - val_loss: 2700.1576\n",
      "Epoch 81/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 335.2212 - val_loss: 2731.5533\n",
      "Epoch 82/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 368.8678 - val_loss: 2484.7436\n",
      "Epoch 83/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 349.7475 - val_loss: 2427.9171\n",
      "Epoch 84/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 340.1553 - val_loss: 2312.5077\n",
      "Epoch 85/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 385.9465 - val_loss: 2247.8545\n",
      "Epoch 86/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 355.1113 - val_loss: 1763.2452\n",
      "Epoch 87/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 442.8435 - val_loss: 1761.5929\n",
      "Epoch 88/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 319.5621 - val_loss: 1685.4876\n",
      "Epoch 89/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 270.7018 - val_loss: 1748.1123\n",
      "Epoch 90/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 376.0991 - val_loss: 1545.9976\n",
      "Epoch 91/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 357.0732 - val_loss: 1681.0763\n",
      "Epoch 92/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 431.5846 - val_loss: 1350.7268\n",
      "Epoch 93/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 447.2085 - val_loss: 1272.3216\n",
      "Epoch 94/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 380.1068 - val_loss: 1234.6317\n",
      "Epoch 95/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 348.6649 - val_loss: 1173.1491\n",
      "Epoch 96/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 387.8912 - val_loss: 1153.6769\n",
      "Epoch 97/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 356.6922 - val_loss: 1013.0581\n",
      "Epoch 98/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 383.5826 - val_loss: 963.1662\n",
      "Epoch 99/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 420.0940 - val_loss: 1426.3472\n",
      "Epoch 100/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 405.0751 - val_loss: 967.1228\n",
      "Epoch 101/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 355.4671 - val_loss: 1007.6937\n",
      "Epoch 102/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 351.7687 - val_loss: 856.8945\n",
      "Epoch 103/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 358.8788 - val_loss: 1023.9433\n",
      "Epoch 104/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 403.4187 - val_loss: 958.8062\n",
      "Epoch 105/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 349.6371 - val_loss: 952.1617\n",
      "Epoch 106/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 467.9314 - val_loss: 771.4363\n",
      "Epoch 107/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 340.7848 - val_loss: 1089.9759\n",
      "Epoch 108/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 352.0943 - val_loss: 824.8560\n",
      "Epoch 109/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 348.7621 - val_loss: 971.6227\n",
      "Epoch 110/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 390.7346 - val_loss: 957.1059\n",
      "Epoch 111/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 361.6010 - val_loss: 757.7186\n",
      "Epoch 112/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 371.3301 - val_loss: 1003.2054\n",
      "Epoch 113/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 307.3412 - val_loss: 979.5404\n",
      "Epoch 114/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 359.3698 - val_loss: 761.6096\n",
      "Epoch 115/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 350.3537 - val_loss: 718.1305\n",
      "Epoch 116/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 460.2823 - val_loss: 585.8214\n",
      "Epoch 117/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 414.1171 - val_loss: 741.3096\n",
      "Epoch 118/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 352.6267 - val_loss: 749.2948\n",
      "Epoch 119/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 365.2921 - val_loss: 1056.6809\n",
      "Epoch 120/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 397.6135 - val_loss: 886.1872\n",
      "Epoch 121/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 405.7101 - val_loss: 862.8574\n",
      "Epoch 122/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 387.5463 - val_loss: 788.1920\n",
      "Epoch 123/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 337.4431 - val_loss: 752.8863\n",
      "Epoch 124/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 428.8599 - val_loss: 990.5205\n",
      "Epoch 125/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 433.5443 - val_loss: 920.4104\n",
      "Epoch 126/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 351.3460 - val_loss: 933.8278\n",
      "Epoch 127/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 361.6900 - val_loss: 1109.0656\n",
      "Epoch 128/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 449.1627 - val_loss: 936.2008\n",
      "Epoch 129/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 318.7630 - val_loss: 933.8082\n",
      "Epoch 130/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 400.5585 - val_loss: 955.0926\n",
      "Epoch 131/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 294.9353 - val_loss: 1095.0521\n",
      "Epoch 132/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 338.5514 - val_loss: 720.4413\n",
      "Epoch 133/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 434.8275 - val_loss: 971.1102\n",
      "Epoch 134/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 402.1327 - val_loss: 1302.3134\n",
      "Epoch 135/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 362.6169 - val_loss: 1095.9777\n",
      "Epoch 136/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 388.0049 - val_loss: 923.1101\n",
      "Epoch 137/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 420.7422 - val_loss: 954.1897\n",
      "Epoch 138/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 467.2629 - val_loss: 958.1911\n",
      "Epoch 139/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 393.3135 - val_loss: 726.5192\n",
      "Epoch 140/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 368.7834 - val_loss: 732.1890\n",
      "Epoch 141/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 388.4320 - val_loss: 911.0346\n",
      "Epoch 142/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 373.0133 - val_loss: 995.8644\n",
      "Epoch 143/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 337.5451 - val_loss: 975.8391\n",
      "Epoch 144/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 483.5655 - val_loss: 810.0881\n",
      "Epoch 145/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 330.6523 - val_loss: 974.2363\n",
      "Epoch 146/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 401.3333 - val_loss: 984.4324\n",
      "Epoch 147/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 364.9864 - val_loss: 986.4032\n",
      "Epoch 148/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 395.1270 - val_loss: 885.5019\n",
      "Epoch 149/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 328.5002 - val_loss: 839.1525\n",
      "Epoch 150/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 430.7013 - val_loss: 958.1861\n",
      "Epoch 151/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 395.6890 - val_loss: 1109.3396\n",
      "Epoch 152/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 429.0505 - val_loss: 1034.2702\n",
      "Epoch 153/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 404.9607 - val_loss: 1217.0781\n",
      "Epoch 154/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 358.0092 - val_loss: 1064.7869\n",
      "Epoch 155/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 416.4573 - val_loss: 968.5148\n",
      "Epoch 156/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 348.6313 - val_loss: 1151.4077\n",
      "Epoch 157/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 396.3392 - val_loss: 1057.3947\n",
      "Epoch 158/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 388.8771 - val_loss: 1133.5543\n",
      "Epoch 159/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 355.1903 - val_loss: 1202.8633\n",
      "Epoch 160/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 304.3427 - val_loss: 1059.2543\n",
      "Epoch 161/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 353.4449 - val_loss: 1133.3816\n",
      "Epoch 162/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 474.6834 - val_loss: 982.2337\n",
      "Epoch 163/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 362.1460 - val_loss: 1329.1063\n",
      "Epoch 164/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 394.5958 - val_loss: 1606.4250\n",
      "Epoch 165/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 423.9319 - val_loss: 1667.9575\n",
      "Epoch 166/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 518.5665 - val_loss: 1146.6488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f021bdb6290>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_07 = Sequential()\n",
    "\n",
    "model_07.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_07.add(Dropout(0.1))\n",
    "model_07.add(layers.Dense(units=1024))\n",
    "model_07.add(Dropout(0.1))\n",
    "                          \n",
    "\n",
    "model_07.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_07.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_07.fit(train_input_07, train_target_07, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165 samples, validate on 71 samples\n",
      "Epoch 1/1000\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1125994.3598 - val_loss: 3198748.3662\n",
      "Epoch 2/1000\n",
      "165/165 [==============================] - 0s 141us/step - loss: 329968.3777 - val_loss: 5036800.2113\n",
      "Epoch 3/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 117046.6744 - val_loss: 5482092.7887\n",
      "Epoch 4/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 84290.7683 - val_loss: 4688402.8592\n",
      "Epoch 5/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 71031.1839 - val_loss: 4223907.8873\n",
      "Epoch 6/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 62073.0542 - val_loss: 4096097.2817\n",
      "Epoch 7/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 55524.1188 - val_loss: 3540640.9859\n",
      "Epoch 8/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 48490.8853 - val_loss: 3165495.2324\n",
      "Epoch 9/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 42654.9097 - val_loss: 3053655.0211\n",
      "Epoch 10/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 38802.5975 - val_loss: 2939284.6690\n",
      "Epoch 11/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 37126.0773 - val_loss: 2459533.2958\n",
      "Epoch 12/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 31090.8703 - val_loss: 2220084.7113\n",
      "Epoch 13/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 27979.3780 - val_loss: 1982795.3169\n",
      "Epoch 14/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 24902.2019 - val_loss: 1796291.4859\n",
      "Epoch 15/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 22003.5397 - val_loss: 1514693.3134\n",
      "Epoch 16/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 19567.8025 - val_loss: 1369590.2606\n",
      "Epoch 17/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 18681.0568 - val_loss: 1221620.2218\n",
      "Epoch 18/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 16269.8688 - val_loss: 1066157.4507\n",
      "Epoch 19/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 15948.8403 - val_loss: 1092448.7254\n",
      "Epoch 20/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 14123.8131 - val_loss: 910424.7782\n",
      "Epoch 21/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 13044.6133 - val_loss: 788252.9419\n",
      "Epoch 22/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 9891.0491 - val_loss: 695901.5651\n",
      "Epoch 23/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 9650.8164 - val_loss: 639911.8204\n",
      "Epoch 24/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 8808.6901 - val_loss: 537297.1919\n",
      "Epoch 25/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 8189.5061 - val_loss: 522693.3680\n",
      "Epoch 26/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 7453.0733 - val_loss: 462205.4736\n",
      "Epoch 27/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 6780.2858 - val_loss: 389455.0942\n",
      "Epoch 28/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 5748.0658 - val_loss: 350359.8548\n",
      "Epoch 29/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 5150.5216 - val_loss: 299564.3099\n",
      "Epoch 30/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 4872.5852 - val_loss: 263894.0018\n",
      "Epoch 31/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 4166.1975 - val_loss: 220009.9137\n",
      "Epoch 32/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 4004.8046 - val_loss: 201824.8490\n",
      "Epoch 33/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 3329.3963 - val_loss: 182103.1853\n",
      "Epoch 34/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 3458.4468 - val_loss: 155033.8913\n",
      "Epoch 35/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 2684.3000 - val_loss: 135324.4793\n",
      "Epoch 36/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 2570.4105 - val_loss: 118771.8979\n",
      "Epoch 37/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 2273.0004 - val_loss: 103263.6118\n",
      "Epoch 38/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 1937.8024 - val_loss: 87156.4091\n",
      "Epoch 39/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 1997.2368 - val_loss: 85480.3303\n",
      "Epoch 40/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 1590.3960 - val_loss: 67165.7421\n",
      "Epoch 41/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 1523.0916 - val_loss: 52939.8268\n",
      "Epoch 42/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 1499.4643 - val_loss: 45695.2682\n",
      "Epoch 43/1000\n",
      "165/165 [==============================] - 0s 138us/step - loss: 1231.5138 - val_loss: 40132.7203\n",
      "Epoch 44/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 1262.7744 - val_loss: 32785.1607\n",
      "Epoch 45/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 927.2357 - val_loss: 27617.8469\n",
      "Epoch 46/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 993.3763 - val_loss: 24442.6930\n",
      "Epoch 47/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 929.9622 - val_loss: 19281.1633\n",
      "Epoch 48/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 826.7975 - val_loss: 15996.8643\n",
      "Epoch 49/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 626.5978 - val_loss: 15751.5760\n",
      "Epoch 50/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 797.5196 - val_loss: 11668.5303\n",
      "Epoch 51/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 668.8527 - val_loss: 8840.7534\n",
      "Epoch 52/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 618.5107 - val_loss: 5620.9199\n",
      "Epoch 53/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 630.7600 - val_loss: 4173.0326\n",
      "Epoch 54/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 555.6568 - val_loss: 3916.1690\n",
      "Epoch 55/1000\n",
      "165/165 [==============================] - 0s 143us/step - loss: 588.2412 - val_loss: 2861.6890\n",
      "Epoch 56/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 544.3419 - val_loss: 2676.2977\n",
      "Epoch 57/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 529.0503 - val_loss: 1693.6262\n",
      "Epoch 58/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 474.1461 - val_loss: 1452.5232\n",
      "Epoch 59/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 544.1970 - val_loss: 836.4917\n",
      "Epoch 60/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 537.8920 - val_loss: 625.0490\n",
      "Epoch 61/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 487.8053 - val_loss: 628.0808\n",
      "Epoch 62/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 547.2987 - val_loss: 331.2503\n",
      "Epoch 63/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 533.3385 - val_loss: 277.1895\n",
      "Epoch 64/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 491.5942 - val_loss: 185.3824\n",
      "Epoch 65/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 543.6217 - val_loss: 215.6078\n",
      "Epoch 66/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 378.3670 - val_loss: 161.7186\n",
      "Epoch 67/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 360.6678 - val_loss: 229.6715\n",
      "Epoch 68/1000\n",
      "165/165 [==============================] - 0s 179us/step - loss: 457.9233 - val_loss: 208.3255\n",
      "Epoch 69/1000\n",
      "165/165 [==============================] - 0s 182us/step - loss: 470.2112 - val_loss: 295.2500\n",
      "Epoch 70/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 450.1189 - val_loss: 478.1652\n",
      "Epoch 71/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 500.6116 - val_loss: 362.4535\n",
      "Epoch 72/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 417.2016 - val_loss: 534.0114\n",
      "Epoch 73/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 488.0981 - val_loss: 283.5712\n",
      "Epoch 74/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 449.4352 - val_loss: 553.3648\n",
      "Epoch 75/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 521.8067 - val_loss: 680.5791\n",
      "Epoch 76/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 543.7195 - val_loss: 581.4703\n",
      "Epoch 77/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 479.8872 - val_loss: 1149.6509\n",
      "Epoch 78/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 440.8060 - val_loss: 775.7513\n",
      "Epoch 79/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 473.6002 - val_loss: 751.3262\n",
      "Epoch 80/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 447.8531 - val_loss: 437.1116\n",
      "Epoch 81/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 461.4645 - val_loss: 384.1713\n",
      "Epoch 82/1000\n",
      "165/165 [==============================] - 0s 185us/step - loss: 464.3013 - val_loss: 574.7895\n",
      "Epoch 83/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 427.4485 - val_loss: 825.8573\n",
      "Epoch 84/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 389.4842 - val_loss: 894.5616\n",
      "Epoch 85/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 427.5508 - val_loss: 1318.4964\n",
      "Epoch 86/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 366.7463 - val_loss: 1199.0159\n",
      "Epoch 87/1000\n",
      "165/165 [==============================] - 0s 180us/step - loss: 402.5742 - val_loss: 1075.9756\n",
      "Epoch 88/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 435.2194 - val_loss: 1027.9646\n",
      "Epoch 89/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 579.6941 - val_loss: 1002.0662\n",
      "Epoch 90/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 425.8236 - val_loss: 1029.3903\n",
      "Epoch 91/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 435.1128 - val_loss: 1538.0837\n",
      "Epoch 92/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 469.8192 - val_loss: 1546.9843\n",
      "Epoch 93/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 353.2546 - val_loss: 1623.8640\n",
      "Epoch 94/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 417.4826 - val_loss: 1418.2641\n",
      "Epoch 95/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 392.8529 - val_loss: 1591.4008\n",
      "Epoch 96/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 376.5836 - val_loss: 2087.5658\n",
      "Epoch 97/1000\n",
      "165/165 [==============================] - 0s 176us/step - loss: 422.1100 - val_loss: 1221.9693\n",
      "Epoch 98/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 383.4541 - val_loss: 1893.4466\n",
      "Epoch 99/1000\n",
      "165/165 [==============================] - 0s 179us/step - loss: 459.3580 - val_loss: 1563.6958\n",
      "Epoch 100/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 408.3105 - val_loss: 1013.5239\n",
      "Epoch 101/1000\n",
      "165/165 [==============================] - 0s 179us/step - loss: 485.8522 - val_loss: 917.8185\n",
      "Epoch 102/1000\n",
      "165/165 [==============================] - 0s 174us/step - loss: 516.0108 - val_loss: 950.2532\n",
      "Epoch 103/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 435.5720 - val_loss: 1271.6789\n",
      "Epoch 104/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 504.1697 - val_loss: 1212.7704\n",
      "Epoch 105/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 419.8742 - val_loss: 893.4954\n",
      "Epoch 106/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 480.2874 - val_loss: 1521.5548\n",
      "Epoch 107/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 400.4406 - val_loss: 1558.4136\n",
      "Epoch 108/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 497.9933 - val_loss: 1339.4806\n",
      "Epoch 109/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 433.0780 - val_loss: 1404.1703\n",
      "Epoch 110/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 371.7205 - val_loss: 1394.5501\n",
      "Epoch 111/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 414.8474 - val_loss: 1130.9226\n",
      "Epoch 112/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 467.2168 - val_loss: 1011.3504\n",
      "Epoch 113/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 477.4485 - val_loss: 1159.2209\n",
      "Epoch 114/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 419.6423 - val_loss: 1574.7210\n",
      "Epoch 115/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 416.7320 - val_loss: 1257.5575\n",
      "Epoch 116/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 492.0042 - val_loss: 1039.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f021bc63950>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_08 = Sequential()\n",
    "\n",
    "model_08.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_08.add(Dropout(0.1))\n",
    "model_08.add(layers.Dense(units=1024))\n",
    "model_08.add(Dropout(0.1))\n",
    "\n",
    "model_08.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_08.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_08.fit(train_input_08, train_target_08, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples, validate on 72 samples\n",
      "Epoch 1/1000\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1097882.9262 - val_loss: 3696219.3056\n",
      "Epoch 2/1000\n",
      "166/166 [==============================] - 0s 138us/step - loss: 314955.9674 - val_loss: 4681134.1667\n",
      "Epoch 3/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 208661.1493 - val_loss: 4468978.5556\n",
      "Epoch 4/1000\n",
      "166/166 [==============================] - 0s 140us/step - loss: 157084.9226 - val_loss: 4274307.3333\n",
      "Epoch 5/1000\n",
      "166/166 [==============================] - 0s 148us/step - loss: 117679.2956 - val_loss: 3397781.9444\n",
      "Epoch 6/1000\n",
      "166/166 [==============================] - 0s 154us/step - loss: 87955.8797 - val_loss: 2738543.8889\n",
      "Epoch 7/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 64803.1157 - val_loss: 2229158.6389\n",
      "Epoch 8/1000\n",
      "166/166 [==============================] - 0s 152us/step - loss: 50732.2683 - val_loss: 1795279.7222\n",
      "Epoch 9/1000\n",
      "166/166 [==============================] - 0s 140us/step - loss: 39803.9175 - val_loss: 1384206.4583\n",
      "Epoch 10/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 30192.8709 - val_loss: 1079269.9861\n",
      "Epoch 11/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 24184.9263 - val_loss: 954381.4722\n",
      "Epoch 12/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 21517.5179 - val_loss: 716496.9931\n",
      "Epoch 13/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 17577.5914 - val_loss: 586061.1181\n",
      "Epoch 14/1000\n",
      "166/166 [==============================] - 0s 160us/step - loss: 14651.1872 - val_loss: 467827.9167\n",
      "Epoch 15/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 13499.1749 - val_loss: 395130.8160\n",
      "Epoch 16/1000\n",
      "166/166 [==============================] - 0s 148us/step - loss: 11482.6381 - val_loss: 321154.3576\n",
      "Epoch 17/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 9816.0833 - val_loss: 281952.0278\n",
      "Epoch 18/1000\n",
      "166/166 [==============================] - 0s 144us/step - loss: 8518.1150 - val_loss: 219291.0868\n",
      "Epoch 19/1000\n",
      "166/166 [==============================] - 0s 154us/step - loss: 7443.6805 - val_loss: 168686.9149\n",
      "Epoch 20/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 6530.0810 - val_loss: 155448.1580\n",
      "Epoch 21/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 5704.1347 - val_loss: 126756.2257\n",
      "Epoch 22/1000\n",
      "166/166 [==============================] - 0s 167us/step - loss: 5115.8420 - val_loss: 109858.2604\n",
      "Epoch 23/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 4210.2257 - val_loss: 96022.4757\n",
      "Epoch 24/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 4034.6882 - val_loss: 77130.1059\n",
      "Epoch 25/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 3338.8300 - val_loss: 60262.9323\n",
      "Epoch 26/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 2588.9381 - val_loss: 45131.7387\n",
      "Epoch 27/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 2438.6408 - val_loss: 33814.3880\n",
      "Epoch 28/1000\n",
      "166/166 [==============================] - 0s 174us/step - loss: 2197.4710 - val_loss: 29218.6369\n",
      "Epoch 29/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 1733.7862 - val_loss: 16444.0540\n",
      "Epoch 30/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 1755.0795 - val_loss: 13072.7641\n",
      "Epoch 31/1000\n",
      "166/166 [==============================] - 0s 148us/step - loss: 1363.4325 - val_loss: 11629.1080\n",
      "Epoch 32/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 1278.6838 - val_loss: 9355.4057\n",
      "Epoch 33/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 1206.2819 - val_loss: 6414.5594\n",
      "Epoch 34/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 940.7477 - val_loss: 6247.3874\n",
      "Epoch 35/1000\n",
      "166/166 [==============================] - 0s 167us/step - loss: 925.6313 - val_loss: 4767.2488\n",
      "Epoch 36/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 804.0600 - val_loss: 2505.4755\n",
      "Epoch 37/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 737.4088 - val_loss: 1494.0012\n",
      "Epoch 38/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 776.6073 - val_loss: 1258.1967\n",
      "Epoch 39/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 652.9881 - val_loss: 519.9399\n",
      "Epoch 40/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 610.7701 - val_loss: 498.3517\n",
      "Epoch 41/1000\n",
      "166/166 [==============================] - 0s 181us/step - loss: 667.8823 - val_loss: 154.9612\n",
      "Epoch 42/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 625.4228 - val_loss: 162.4820\n",
      "Epoch 43/1000\n",
      "166/166 [==============================] - 0s 153us/step - loss: 463.5391 - val_loss: 198.9483\n",
      "Epoch 44/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 478.0765 - val_loss: 339.8617\n",
      "Epoch 45/1000\n",
      "166/166 [==============================] - 0s 171us/step - loss: 538.4701 - val_loss: 617.9247\n",
      "Epoch 46/1000\n",
      "166/166 [==============================] - 0s 146us/step - loss: 589.0330 - val_loss: 523.5722\n",
      "Epoch 47/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 514.6032 - val_loss: 910.3898\n",
      "Epoch 48/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 487.0627 - val_loss: 423.5563\n",
      "Epoch 49/1000\n",
      "166/166 [==============================] - 0s 138us/step - loss: 443.8467 - val_loss: 856.7631\n",
      "Epoch 50/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 338.5683 - val_loss: 1454.0469\n",
      "Epoch 51/1000\n",
      "166/166 [==============================] - 0s 140us/step - loss: 425.0146 - val_loss: 1731.3125\n",
      "Epoch 52/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 460.8992 - val_loss: 1374.3224\n",
      "Epoch 53/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 496.0678 - val_loss: 2576.3095\n",
      "Epoch 54/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 508.3849 - val_loss: 1858.9946\n",
      "Epoch 55/1000\n",
      "166/166 [==============================] - 0s 160us/step - loss: 361.8047 - val_loss: 1986.2978\n",
      "Epoch 56/1000\n",
      "166/166 [==============================] - 0s 143us/step - loss: 455.8671 - val_loss: 1889.0043\n",
      "Epoch 57/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 422.3605 - val_loss: 1648.4348\n",
      "Epoch 58/1000\n",
      "166/166 [==============================] - 0s 144us/step - loss: 383.9581 - val_loss: 2354.3738\n",
      "Epoch 59/1000\n",
      "166/166 [==============================] - 0s 141us/step - loss: 355.5964 - val_loss: 2849.1506\n",
      "Epoch 60/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 375.0347 - val_loss: 2162.0357\n",
      "Epoch 61/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 467.3532 - val_loss: 2658.5912\n",
      "Epoch 62/1000\n",
      "166/166 [==============================] - 0s 154us/step - loss: 374.1892 - val_loss: 3577.7810\n",
      "Epoch 63/1000\n",
      "166/166 [==============================] - 0s 174us/step - loss: 401.6048 - val_loss: 2844.9149\n",
      "Epoch 64/1000\n",
      "166/166 [==============================] - 0s 180us/step - loss: 413.0966 - val_loss: 2287.9016\n",
      "Epoch 65/1000\n",
      "166/166 [==============================] - 0s 147us/step - loss: 453.6491 - val_loss: 3555.0704\n",
      "Epoch 66/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 338.1807 - val_loss: 3651.6518\n",
      "Epoch 67/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 407.4323 - val_loss: 3965.5033\n",
      "Epoch 68/1000\n",
      "166/166 [==============================] - 0s 141us/step - loss: 394.8000 - val_loss: 3403.6480\n",
      "Epoch 69/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 507.0002 - val_loss: 4247.3461\n",
      "Epoch 70/1000\n",
      "166/166 [==============================] - 0s 168us/step - loss: 407.1781 - val_loss: 4308.5397\n",
      "Epoch 71/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 370.9894 - val_loss: 3758.9211\n",
      "Epoch 72/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 421.6808 - val_loss: 4014.5400\n",
      "Epoch 73/1000\n",
      "166/166 [==============================] - 0s 147us/step - loss: 380.5437 - val_loss: 2525.3584\n",
      "Epoch 74/1000\n",
      "166/166 [==============================] - 0s 152us/step - loss: 415.7783 - val_loss: 3269.1945\n",
      "Epoch 75/1000\n",
      "166/166 [==============================] - 0s 142us/step - loss: 448.3766 - val_loss: 3952.4267\n",
      "Epoch 76/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 417.4372 - val_loss: 3618.7335\n",
      "Epoch 77/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 440.2418 - val_loss: 3069.8471\n",
      "Epoch 78/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 494.1384 - val_loss: 3704.5520\n",
      "Epoch 79/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 492.4339 - val_loss: 3425.1347\n",
      "Epoch 80/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 417.2119 - val_loss: 3976.5306\n",
      "Epoch 81/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 433.7886 - val_loss: 4028.5337\n",
      "Epoch 82/1000\n",
      "166/166 [==============================] - 0s 167us/step - loss: 437.8253 - val_loss: 3399.3213\n",
      "Epoch 83/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 430.9100 - val_loss: 3724.1023\n",
      "Epoch 84/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 425.0377 - val_loss: 2545.8594\n",
      "Epoch 85/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 382.0235 - val_loss: 3760.3066\n",
      "Epoch 86/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 413.5997 - val_loss: 2950.1992\n",
      "Epoch 87/1000\n",
      "166/166 [==============================] - 0s 168us/step - loss: 459.4883 - val_loss: 3485.5955\n",
      "Epoch 88/1000\n",
      "166/166 [==============================] - 0s 168us/step - loss: 481.0162 - val_loss: 3056.0648\n",
      "Epoch 89/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 408.6233 - val_loss: 3167.0263\n",
      "Epoch 90/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 483.3059 - val_loss: 3782.0544\n",
      "Epoch 91/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 447.4713 - val_loss: 3678.7295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f021ba82f10>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_09 = Sequential()\n",
    "\n",
    "model_09.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_09.add(Dropout(0.1))\n",
    "model_09.add(layers.Dense(units=1024))\n",
    "model_09.add(Dropout(0.1))\n",
    "\n",
    "model_09.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_09.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_09.fit(train_input_09, train_target_09, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 166 samples, validate on 72 samples\n",
      "Epoch 1/1000\n",
      "166/166 [==============================] - 0s 1ms/step - loss: 1116647.0945 - val_loss: 3655267.8056\n",
      "Epoch 2/1000\n",
      "166/166 [==============================] - 0s 132us/step - loss: 433518.8202 - val_loss: 4672765.3889\n",
      "Epoch 3/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 293202.9415 - val_loss: 3614564.9444\n",
      "Epoch 4/1000\n",
      "166/166 [==============================] - 0s 145us/step - loss: 196808.0195 - val_loss: 2643912.4167\n",
      "Epoch 5/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 124102.8092 - val_loss: 1793210.0556\n",
      "Epoch 6/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 67866.5727 - val_loss: 1119008.7639\n",
      "Epoch 7/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 41607.1587 - val_loss: 698225.0556\n",
      "Epoch 8/1000\n",
      "166/166 [==============================] - 0s 153us/step - loss: 24136.9485 - val_loss: 430933.3021\n",
      "Epoch 9/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 16726.9160 - val_loss: 433889.8681\n",
      "Epoch 10/1000\n",
      "166/166 [==============================] - 0s 148us/step - loss: 16340.8506 - val_loss: 243580.7292\n",
      "Epoch 11/1000\n",
      "166/166 [==============================] - 0s 144us/step - loss: 11574.1027 - val_loss: 177185.9497\n",
      "Epoch 12/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 9234.6603 - val_loss: 137633.1076\n",
      "Epoch 13/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 7650.0463 - val_loss: 99340.4271\n",
      "Epoch 14/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 5956.1172 - val_loss: 93654.7170\n",
      "Epoch 15/1000\n",
      "166/166 [==============================] - 0s 152us/step - loss: 4965.2941 - val_loss: 70270.8472\n",
      "Epoch 16/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 4683.8717 - val_loss: 66921.4236\n",
      "Epoch 17/1000\n",
      "166/166 [==============================] - 0s 146us/step - loss: 3851.3874 - val_loss: 43980.6189\n",
      "Epoch 18/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 3194.9648 - val_loss: 51505.9540\n",
      "Epoch 19/1000\n",
      "166/166 [==============================] - 0s 147us/step - loss: 3495.0870 - val_loss: 34020.4644\n",
      "Epoch 20/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 2504.5379 - val_loss: 34248.6949\n",
      "Epoch 21/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 2574.9687 - val_loss: 28712.4405\n",
      "Epoch 22/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 1832.1290 - val_loss: 20068.5918\n",
      "Epoch 23/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 1854.0447 - val_loss: 14674.0269\n",
      "Epoch 24/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 1651.7584 - val_loss: 9887.2322\n",
      "Epoch 25/1000\n",
      "166/166 [==============================] - 0s 152us/step - loss: 1195.6256 - val_loss: 11813.8240\n",
      "Epoch 26/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 1301.0643 - val_loss: 9019.3423\n",
      "Epoch 27/1000\n",
      "166/166 [==============================] - 0s 153us/step - loss: 1024.1095 - val_loss: 6766.0562\n",
      "Epoch 28/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 996.8286 - val_loss: 6172.8401\n",
      "Epoch 29/1000\n",
      "166/166 [==============================] - 0s 154us/step - loss: 923.2203 - val_loss: 6883.8976\n",
      "Epoch 30/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 830.7021 - val_loss: 4883.4545\n",
      "Epoch 31/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 768.2126 - val_loss: 2666.9665\n",
      "Epoch 32/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 802.2412 - val_loss: 2372.3065\n",
      "Epoch 33/1000\n",
      "166/166 [==============================] - 0s 168us/step - loss: 679.0573 - val_loss: 1935.7101\n",
      "Epoch 34/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 802.9411 - val_loss: 1811.0075\n",
      "Epoch 35/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 666.5566 - val_loss: 1388.6848\n",
      "Epoch 36/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 735.7933 - val_loss: 1363.4539\n",
      "Epoch 37/1000\n",
      "166/166 [==============================] - 0s 147us/step - loss: 601.2609 - val_loss: 1421.9902\n",
      "Epoch 38/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 665.4034 - val_loss: 1064.9151\n",
      "Epoch 39/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 501.7842 - val_loss: 883.9491\n",
      "Epoch 40/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 576.2867 - val_loss: 816.8476\n",
      "Epoch 41/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 615.6837 - val_loss: 1057.3721\n",
      "Epoch 42/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 568.9465 - val_loss: 856.3284\n",
      "Epoch 43/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 556.7609 - val_loss: 936.9437\n",
      "Epoch 44/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 517.0906 - val_loss: 589.6126\n",
      "Epoch 45/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 608.2728 - val_loss: 645.0354\n",
      "Epoch 46/1000\n",
      "166/166 [==============================] - 0s 146us/step - loss: 572.9242 - val_loss: 526.2329\n",
      "Epoch 47/1000\n",
      "166/166 [==============================] - 0s 167us/step - loss: 511.0423 - val_loss: 454.7318\n",
      "Epoch 48/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 573.5496 - val_loss: 531.5599\n",
      "Epoch 49/1000\n",
      "166/166 [==============================] - 0s 151us/step - loss: 566.0476 - val_loss: 588.7218\n",
      "Epoch 50/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 558.6937 - val_loss: 652.4223\n",
      "Epoch 51/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 557.0037 - val_loss: 500.2008\n",
      "Epoch 52/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 468.3340 - val_loss: 457.1877\n",
      "Epoch 53/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 476.0812 - val_loss: 415.2264\n",
      "Epoch 54/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 592.7532 - val_loss: 507.7525\n",
      "Epoch 55/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 569.5972 - val_loss: 485.3548\n",
      "Epoch 56/1000\n",
      "166/166 [==============================] - 0s 175us/step - loss: 664.1475 - val_loss: 531.9415\n",
      "Epoch 57/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 451.0163 - val_loss: 507.1810\n",
      "Epoch 58/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 575.5736 - val_loss: 449.3912\n",
      "Epoch 59/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 408.6346 - val_loss: 607.0304\n",
      "Epoch 60/1000\n",
      "166/166 [==============================] - 0s 149us/step - loss: 560.6130 - val_loss: 376.0884\n",
      "Epoch 61/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 502.2856 - val_loss: 623.8652\n",
      "Epoch 62/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 536.5694 - val_loss: 718.3074\n",
      "Epoch 63/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 513.4505 - val_loss: 436.2079\n",
      "Epoch 64/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 483.1723 - val_loss: 802.5902\n",
      "Epoch 65/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 457.8409 - val_loss: 305.7414\n",
      "Epoch 66/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 497.9674 - val_loss: 412.4309\n",
      "Epoch 67/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 418.3017 - val_loss: 395.6305\n",
      "Epoch 68/1000\n",
      "166/166 [==============================] - 0s 155us/step - loss: 456.8880 - val_loss: 245.5849\n",
      "Epoch 69/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 536.9536 - val_loss: 383.0139\n",
      "Epoch 70/1000\n",
      "166/166 [==============================] - 0s 156us/step - loss: 537.0508 - val_loss: 516.9529\n",
      "Epoch 71/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 493.9724 - val_loss: 631.3735\n",
      "Epoch 72/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 502.7346 - val_loss: 507.9877\n",
      "Epoch 73/1000\n",
      "166/166 [==============================] - 0s 151us/step - loss: 604.9631 - val_loss: 319.6282\n",
      "Epoch 74/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 534.3541 - val_loss: 388.8477\n",
      "Epoch 75/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 506.6182 - val_loss: 346.5754\n",
      "Epoch 76/1000\n",
      "166/166 [==============================] - 0s 160us/step - loss: 545.2534 - val_loss: 368.0469\n",
      "Epoch 77/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 530.6560 - val_loss: 772.3139\n",
      "Epoch 78/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 489.2311 - val_loss: 889.7812\n",
      "Epoch 79/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 565.7274 - val_loss: 382.9850\n",
      "Epoch 80/1000\n",
      "166/166 [==============================] - 0s 171us/step - loss: 662.4326 - val_loss: 566.3312\n",
      "Epoch 81/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 535.3364 - val_loss: 509.9331\n",
      "Epoch 82/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 536.1982 - val_loss: 527.2300\n",
      "Epoch 83/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 638.3194 - val_loss: 284.9457\n",
      "Epoch 84/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 479.2948 - val_loss: 467.1131\n",
      "Epoch 85/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 486.0129 - val_loss: 437.2227\n",
      "Epoch 86/1000\n",
      "166/166 [==============================] - 0s 167us/step - loss: 567.9423 - val_loss: 583.8421\n",
      "Epoch 87/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 537.1546 - val_loss: 235.1813\n",
      "Epoch 88/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 524.8205 - val_loss: 346.6009\n",
      "Epoch 89/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 542.4115 - val_loss: 254.6902\n",
      "Epoch 90/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 548.7400 - val_loss: 626.3477\n",
      "Epoch 91/1000\n",
      "166/166 [==============================] - 0s 168us/step - loss: 531.3336 - val_loss: 664.8160\n",
      "Epoch 92/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 537.8657 - val_loss: 565.2519\n",
      "Epoch 93/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 532.4243 - val_loss: 610.4135\n",
      "Epoch 94/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 480.7929 - val_loss: 1075.5117\n",
      "Epoch 95/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 557.1952 - val_loss: 844.2422\n",
      "Epoch 96/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 554.6876 - val_loss: 601.2314\n",
      "Epoch 97/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 507.4179 - val_loss: 601.2901\n",
      "Epoch 98/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 625.1435 - val_loss: 688.6122\n",
      "Epoch 99/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 505.5381 - val_loss: 492.6973\n",
      "Epoch 100/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 507.9080 - val_loss: 255.0921\n",
      "Epoch 101/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 437.0588 - val_loss: 167.3260\n",
      "Epoch 102/1000\n",
      "166/166 [==============================] - 0s 150us/step - loss: 608.5414 - val_loss: 181.0665\n",
      "Epoch 103/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 466.1662 - val_loss: 212.7781\n",
      "Epoch 104/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 584.3810 - val_loss: 339.6975\n",
      "Epoch 105/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 519.3250 - val_loss: 277.2175\n",
      "Epoch 106/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 475.1587 - val_loss: 322.0457\n",
      "Epoch 107/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 460.5291 - val_loss: 334.8669\n",
      "Epoch 108/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 641.7538 - val_loss: 524.2876\n",
      "Epoch 109/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 508.8573 - val_loss: 705.5162\n",
      "Epoch 110/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 538.9300 - val_loss: 414.8835\n",
      "Epoch 111/1000\n",
      "166/166 [==============================] - 0s 171us/step - loss: 650.3503 - val_loss: 518.9917\n",
      "Epoch 112/1000\n",
      "166/166 [==============================] - 0s 173us/step - loss: 553.2603 - val_loss: 371.6279\n",
      "Epoch 113/1000\n",
      "166/166 [==============================] - 0s 163us/step - loss: 531.6414 - val_loss: 1078.5792\n",
      "Epoch 114/1000\n",
      "166/166 [==============================] - 0s 173us/step - loss: 494.6878 - val_loss: 1329.5400\n",
      "Epoch 115/1000\n",
      "166/166 [==============================] - 0s 172us/step - loss: 574.8438 - val_loss: 1645.3820\n",
      "Epoch 116/1000\n",
      "166/166 [==============================] - 0s 164us/step - loss: 534.8474 - val_loss: 641.8964\n",
      "Epoch 117/1000\n",
      "166/166 [==============================] - 0s 160us/step - loss: 537.0976 - val_loss: 529.9049\n",
      "Epoch 118/1000\n",
      "166/166 [==============================] - 0s 177us/step - loss: 430.1766 - val_loss: 278.5509\n",
      "Epoch 119/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 575.4015 - val_loss: 406.5212\n",
      "Epoch 120/1000\n",
      "166/166 [==============================] - 0s 158us/step - loss: 545.1835 - val_loss: 626.2491\n",
      "Epoch 121/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 548.0004 - val_loss: 537.9957\n",
      "Epoch 122/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 444.8000 - val_loss: 942.6569\n",
      "Epoch 123/1000\n",
      "166/166 [==============================] - 0s 173us/step - loss: 555.2239 - val_loss: 1300.3386\n",
      "Epoch 124/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 508.6543 - val_loss: 497.3830\n",
      "Epoch 125/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 539.9633 - val_loss: 482.9115\n",
      "Epoch 126/1000\n",
      "166/166 [==============================] - 0s 176us/step - loss: 561.8199 - val_loss: 944.3131\n",
      "Epoch 127/1000\n",
      "166/166 [==============================] - 0s 171us/step - loss: 613.8495 - val_loss: 1036.3733\n",
      "Epoch 128/1000\n",
      "166/166 [==============================] - 0s 153us/step - loss: 499.5870 - val_loss: 461.9151\n",
      "Epoch 129/1000\n",
      "166/166 [==============================] - 0s 180us/step - loss: 454.6057 - val_loss: 927.4278\n",
      "Epoch 130/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 469.9616 - val_loss: 523.8858\n",
      "Epoch 131/1000\n",
      "166/166 [==============================] - 0s 160us/step - loss: 525.2745 - val_loss: 798.6437\n",
      "Epoch 132/1000\n",
      "166/166 [==============================] - 0s 173us/step - loss: 521.5613 - val_loss: 629.8119\n",
      "Epoch 133/1000\n",
      "166/166 [==============================] - 0s 161us/step - loss: 540.2970 - val_loss: 774.9652\n",
      "Epoch 134/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 508.4708 - val_loss: 762.5651\n",
      "Epoch 135/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 454.4115 - val_loss: 823.2371\n",
      "Epoch 136/1000\n",
      "166/166 [==============================] - 0s 177us/step - loss: 581.1512 - val_loss: 839.9601\n",
      "Epoch 137/1000\n",
      "166/166 [==============================] - 0s 172us/step - loss: 597.6057 - val_loss: 822.8380\n",
      "Epoch 138/1000\n",
      "166/166 [==============================] - 0s 165us/step - loss: 473.8538 - val_loss: 587.9909\n",
      "Epoch 139/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 496.7363 - val_loss: 458.3435\n",
      "Epoch 140/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 598.5433 - val_loss: 497.6321\n",
      "Epoch 141/1000\n",
      "166/166 [==============================] - 0s 157us/step - loss: 451.3444 - val_loss: 414.6169\n",
      "Epoch 142/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 489.1396 - val_loss: 670.6297\n",
      "Epoch 143/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 594.6978 - val_loss: 588.1431\n",
      "Epoch 144/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 513.1380 - val_loss: 405.8696\n",
      "Epoch 145/1000\n",
      "166/166 [==============================] - 0s 170us/step - loss: 450.1293 - val_loss: 631.4801\n",
      "Epoch 146/1000\n",
      "166/166 [==============================] - 0s 169us/step - loss: 524.8066 - val_loss: 358.0365\n",
      "Epoch 147/1000\n",
      "166/166 [==============================] - 0s 162us/step - loss: 478.3564 - val_loss: 868.8576\n",
      "Epoch 148/1000\n",
      "166/166 [==============================] - 0s 167us/step - loss: 473.7013 - val_loss: 603.1813\n",
      "Epoch 149/1000\n",
      "166/166 [==============================] - 0s 160us/step - loss: 565.1930 - val_loss: 796.6343\n",
      "Epoch 150/1000\n",
      "166/166 [==============================] - 0s 166us/step - loss: 478.7820 - val_loss: 702.9844\n",
      "Epoch 151/1000\n",
      "166/166 [==============================] - 0s 159us/step - loss: 589.7990 - val_loss: 1160.4455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f02046aaa90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "\n",
    "model_10.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_10.add(Dropout(0.1))\n",
    "model_10.add(layers.Dense(units=1024))\n",
    "model_10.add(Dropout(0.1))\n",
    "\n",
    "model_10.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_10.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_10.fit(train_input_10, train_target_10, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 165 samples, validate on 72 samples\n",
      "Epoch 1/1000\n",
      "165/165 [==============================] - 0s 1ms/step - loss: 1166259.5528 - val_loss: 2976127.8056\n",
      "Epoch 2/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 606881.2803 - val_loss: 2590182.1667\n",
      "Epoch 3/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 357603.7542 - val_loss: 1428083.6389\n",
      "Epoch 4/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 136691.7779 - val_loss: 713690.4514\n",
      "Epoch 5/1000\n",
      "165/165 [==============================] - 0s 140us/step - loss: 41656.3385 - val_loss: 282522.1111\n",
      "Epoch 6/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 13277.5911 - val_loss: 138169.2049\n",
      "Epoch 7/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 8845.7679 - val_loss: 116867.6441\n",
      "Epoch 8/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 7395.9075 - val_loss: 83480.3655\n",
      "Epoch 9/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 6671.8578 - val_loss: 67147.4857\n",
      "Epoch 10/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 4917.8404 - val_loss: 72369.7431\n",
      "Epoch 11/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 5076.3335 - val_loss: 69538.4262\n",
      "Epoch 12/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 3866.9041 - val_loss: 67015.8498\n",
      "Epoch 13/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 4002.0988 - val_loss: 55567.0022\n",
      "Epoch 14/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 3347.8943 - val_loss: 50010.5582\n",
      "Epoch 15/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 2838.8998 - val_loss: 47441.9102\n",
      "Epoch 16/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 2526.8431 - val_loss: 43078.4223\n",
      "Epoch 17/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 2363.6509 - val_loss: 38124.6011\n",
      "Epoch 18/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 2185.1491 - val_loss: 38410.2426\n",
      "Epoch 19/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 1767.4459 - val_loss: 37381.9670\n",
      "Epoch 20/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 1974.0382 - val_loss: 36449.3398\n",
      "Epoch 21/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 1930.8466 - val_loss: 32779.1727\n",
      "Epoch 22/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 1786.4694 - val_loss: 29866.4779\n",
      "Epoch 23/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 1443.5719 - val_loss: 24514.8908\n",
      "Epoch 24/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 1415.8611 - val_loss: 26517.7504\n",
      "Epoch 25/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 1535.7561 - val_loss: 20633.0178\n",
      "Epoch 26/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 1344.6526 - val_loss: 19654.5033\n",
      "Epoch 27/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 1254.6854 - val_loss: 25543.0445\n",
      "Epoch 28/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 1597.6033 - val_loss: 18467.6929\n",
      "Epoch 29/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 1387.3646 - val_loss: 16347.0451\n",
      "Epoch 30/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 1457.7155 - val_loss: 18347.8186\n",
      "Epoch 31/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 1200.8041 - val_loss: 13970.3498\n",
      "Epoch 32/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 1214.9345 - val_loss: 15120.2303\n",
      "Epoch 33/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 1121.1883 - val_loss: 13230.5139\n",
      "Epoch 34/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 977.0750 - val_loss: 16508.3555\n",
      "Epoch 35/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 898.7274 - val_loss: 12473.1006\n",
      "Epoch 36/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 1145.0836 - val_loss: 14542.4763\n",
      "Epoch 37/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 1040.5569 - val_loss: 8610.2577\n",
      "Epoch 38/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 1135.3342 - val_loss: 9159.9964\n",
      "Epoch 39/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 925.1950 - val_loss: 10317.6974\n",
      "Epoch 40/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 834.1551 - val_loss: 8832.5179\n",
      "Epoch 41/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 988.6987 - val_loss: 6981.2820\n",
      "Epoch 42/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 880.1347 - val_loss: 6878.6749\n",
      "Epoch 43/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 1065.5908 - val_loss: 7668.2492\n",
      "Epoch 44/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 823.2791 - val_loss: 14497.5265\n",
      "Epoch 45/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 1020.6364 - val_loss: 7440.3505\n",
      "Epoch 46/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 797.0835 - val_loss: 7948.5145\n",
      "Epoch 47/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 934.8625 - val_loss: 8603.5928\n",
      "Epoch 48/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 766.9758 - val_loss: 5767.1464\n",
      "Epoch 49/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 848.5088 - val_loss: 7136.4544\n",
      "Epoch 50/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 806.3996 - val_loss: 5283.6697\n",
      "Epoch 51/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 755.1501 - val_loss: 6906.1008\n",
      "Epoch 52/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 618.6912 - val_loss: 6350.0770\n",
      "Epoch 53/1000\n",
      "165/165 [==============================] - 0s 145us/step - loss: 788.4958 - val_loss: 3942.2564\n",
      "Epoch 54/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 726.1807 - val_loss: 3517.2362\n",
      "Epoch 55/1000\n",
      "165/165 [==============================] - 0s 147us/step - loss: 797.1359 - val_loss: 7234.5801\n",
      "Epoch 56/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 936.8437 - val_loss: 3133.4351\n",
      "Epoch 57/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 836.8191 - val_loss: 3625.5155\n",
      "Epoch 58/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 740.7485 - val_loss: 2356.5537\n",
      "Epoch 59/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 741.5073 - val_loss: 2766.9422\n",
      "Epoch 60/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 633.3352 - val_loss: 2651.2454\n",
      "Epoch 61/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 852.3189 - val_loss: 2780.8758\n",
      "Epoch 62/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 703.7168 - val_loss: 2409.3405\n",
      "Epoch 63/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 645.7402 - val_loss: 3093.7314\n",
      "Epoch 64/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 706.2796 - val_loss: 2695.2110\n",
      "Epoch 65/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 645.9144 - val_loss: 2393.7372\n",
      "Epoch 66/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 668.3232 - val_loss: 2762.2669\n",
      "Epoch 67/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 525.5471 - val_loss: 3724.9353\n",
      "Epoch 68/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 774.3354 - val_loss: 2109.8349\n",
      "Epoch 69/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 661.3396 - val_loss: 2879.5754\n",
      "Epoch 70/1000\n",
      "165/165 [==============================] - 0s 170us/step - loss: 704.8073 - val_loss: 2180.4370\n",
      "Epoch 71/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 655.4524 - val_loss: 2230.3222\n",
      "Epoch 72/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 602.6642 - val_loss: 2091.2604\n",
      "Epoch 73/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 791.1984 - val_loss: 1557.5334\n",
      "Epoch 74/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 683.9101 - val_loss: 1032.8226\n",
      "Epoch 75/1000\n",
      "165/165 [==============================] - 0s 144us/step - loss: 666.7674 - val_loss: 1004.6004\n",
      "Epoch 76/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 556.6042 - val_loss: 1786.0350\n",
      "Epoch 77/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 641.1774 - val_loss: 1207.7423\n",
      "Epoch 78/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 626.7923 - val_loss: 1151.3729\n",
      "Epoch 79/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 540.5955 - val_loss: 1284.5949\n",
      "Epoch 80/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 526.0546 - val_loss: 1977.6570\n",
      "Epoch 81/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 673.1569 - val_loss: 1796.8839\n",
      "Epoch 82/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 671.3136 - val_loss: 777.3282\n",
      "Epoch 83/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 560.1827 - val_loss: 1356.6256\n",
      "Epoch 84/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 591.1424 - val_loss: 1476.5572\n",
      "Epoch 85/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 599.8813 - val_loss: 908.9935\n",
      "Epoch 86/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 628.2739 - val_loss: 1076.4857\n",
      "Epoch 87/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 543.7435 - val_loss: 1101.6657\n",
      "Epoch 88/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 635.1696 - val_loss: 743.9803\n",
      "Epoch 89/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 643.8047 - val_loss: 1936.1239\n",
      "Epoch 90/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 670.7437 - val_loss: 966.2521\n",
      "Epoch 91/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 608.3092 - val_loss: 710.7210\n",
      "Epoch 92/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 555.2008 - val_loss: 1274.2590\n",
      "Epoch 93/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 677.2767 - val_loss: 678.0274\n",
      "Epoch 94/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 609.1001 - val_loss: 667.9050\n",
      "Epoch 95/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 488.9309 - val_loss: 552.2252\n",
      "Epoch 96/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 563.2113 - val_loss: 486.3560\n",
      "Epoch 97/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 516.4841 - val_loss: 299.2600\n",
      "Epoch 98/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 709.7128 - val_loss: 416.9200\n",
      "Epoch 99/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 640.1700 - val_loss: 933.7178\n",
      "Epoch 100/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 631.2806 - val_loss: 672.2956\n",
      "Epoch 101/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 725.0985 - val_loss: 532.9185\n",
      "Epoch 102/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 507.1717 - val_loss: 226.9232\n",
      "Epoch 103/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 543.6878 - val_loss: 326.8871\n",
      "Epoch 104/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 628.9844 - val_loss: 1564.2849\n",
      "Epoch 105/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 739.0913 - val_loss: 386.7028\n",
      "Epoch 106/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 563.3081 - val_loss: 378.1150\n",
      "Epoch 107/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 564.7298 - val_loss: 214.9070\n",
      "Epoch 108/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 657.8039 - val_loss: 397.4789\n",
      "Epoch 109/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 558.1076 - val_loss: 377.4744\n",
      "Epoch 110/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 623.5801 - val_loss: 151.5047\n",
      "Epoch 111/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 523.1968 - val_loss: 187.4724\n",
      "Epoch 112/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 587.7387 - val_loss: 233.8381\n",
      "Epoch 113/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 584.6177 - val_loss: 233.6330\n",
      "Epoch 114/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 491.4910 - val_loss: 287.7826\n",
      "Epoch 115/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 567.2613 - val_loss: 201.2561\n",
      "Epoch 116/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 691.6157 - val_loss: 153.1519\n",
      "Epoch 117/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 513.3999 - val_loss: 270.7028\n",
      "Epoch 118/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 475.3807 - val_loss: 393.4548\n",
      "Epoch 119/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 450.3996 - val_loss: 361.1825\n",
      "Epoch 120/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 575.4290 - val_loss: 142.9523\n",
      "Epoch 121/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 477.5254 - val_loss: 248.0493\n",
      "Epoch 122/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 523.6089 - val_loss: 134.7628\n",
      "Epoch 123/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 532.5635 - val_loss: 129.8623\n",
      "Epoch 124/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 549.9610 - val_loss: 297.2745\n",
      "Epoch 125/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 600.5861 - val_loss: 157.6273\n",
      "Epoch 126/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 480.8921 - val_loss: 182.2758\n",
      "Epoch 127/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 621.6907 - val_loss: 155.3867\n",
      "Epoch 128/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 514.5454 - val_loss: 124.3169\n",
      "Epoch 129/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 431.3374 - val_loss: 198.6357\n",
      "Epoch 130/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 636.5775 - val_loss: 203.3286\n",
      "Epoch 131/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 618.0919 - val_loss: 167.0247\n",
      "Epoch 132/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 605.4619 - val_loss: 367.4538\n",
      "Epoch 133/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 590.1186 - val_loss: 260.2910\n",
      "Epoch 134/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 438.1269 - val_loss: 265.9039\n",
      "Epoch 135/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 556.8593 - val_loss: 190.8476\n",
      "Epoch 136/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 528.0759 - val_loss: 134.2633\n",
      "Epoch 137/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 496.6569 - val_loss: 194.1915\n",
      "Epoch 138/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 468.5153 - val_loss: 124.8526\n",
      "Epoch 139/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 470.3453 - val_loss: 137.2769\n",
      "Epoch 140/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 618.6436 - val_loss: 109.7444\n",
      "Epoch 141/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 604.9966 - val_loss: 156.4046\n",
      "Epoch 142/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 617.1320 - val_loss: 161.8390\n",
      "Epoch 143/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 762.9869 - val_loss: 272.3678\n",
      "Epoch 144/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 559.7727 - val_loss: 179.4639\n",
      "Epoch 145/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 564.9299 - val_loss: 130.9858\n",
      "Epoch 146/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 459.5034 - val_loss: 134.8506\n",
      "Epoch 147/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 485.6671 - val_loss: 115.5624\n",
      "Epoch 148/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 565.1470 - val_loss: 140.5519\n",
      "Epoch 149/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 527.9553 - val_loss: 145.0980\n",
      "Epoch 150/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 523.8134 - val_loss: 158.3864\n",
      "Epoch 151/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 547.4410 - val_loss: 136.8413\n",
      "Epoch 152/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 565.0778 - val_loss: 270.6777\n",
      "Epoch 153/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 498.9387 - val_loss: 118.7219\n",
      "Epoch 154/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 481.3914 - val_loss: 112.9495\n",
      "Epoch 155/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 556.4747 - val_loss: 141.8450\n",
      "Epoch 156/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 458.1456 - val_loss: 154.4529\n",
      "Epoch 157/1000\n",
      "165/165 [==============================] - 0s 172us/step - loss: 600.1401 - val_loss: 168.8170\n",
      "Epoch 158/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 548.7124 - val_loss: 143.4503\n",
      "Epoch 159/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 557.3370 - val_loss: 157.8556\n",
      "Epoch 160/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 519.6963 - val_loss: 230.7382\n",
      "Epoch 161/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 534.1907 - val_loss: 127.2096\n",
      "Epoch 162/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 493.9301 - val_loss: 135.4301\n",
      "Epoch 163/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 526.8492 - val_loss: 117.3771\n",
      "Epoch 164/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 591.9530 - val_loss: 301.1530\n",
      "Epoch 165/1000\n",
      "165/165 [==============================] - 0s 173us/step - loss: 553.9179 - val_loss: 353.4008\n",
      "Epoch 166/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 569.1007 - val_loss: 219.4862\n",
      "Epoch 167/1000\n",
      "165/165 [==============================] - 0s 163us/step - loss: 526.1962 - val_loss: 195.8048\n",
      "Epoch 168/1000\n",
      "165/165 [==============================] - 0s 149us/step - loss: 515.6263 - val_loss: 254.0007\n",
      "Epoch 169/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 593.0386 - val_loss: 121.4916\n",
      "Epoch 170/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 592.4527 - val_loss: 205.6159\n",
      "Epoch 171/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 503.3878 - val_loss: 229.3839\n",
      "Epoch 172/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 512.3275 - val_loss: 121.0447\n",
      "Epoch 173/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 527.0145 - val_loss: 96.4844\n",
      "Epoch 174/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 568.5491 - val_loss: 91.1548\n",
      "Epoch 175/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 518.6820 - val_loss: 184.5911\n",
      "Epoch 176/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 577.2780 - val_loss: 282.8897\n",
      "Epoch 177/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 546.6574 - val_loss: 111.7189\n",
      "Epoch 178/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 519.8353 - val_loss: 206.0310\n",
      "Epoch 179/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 505.6327 - val_loss: 131.4335\n",
      "Epoch 180/1000\n",
      "165/165 [==============================] - 0s 151us/step - loss: 509.2437 - val_loss: 197.8960\n",
      "Epoch 181/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 624.7843 - val_loss: 316.8003\n",
      "Epoch 182/1000\n",
      "165/165 [==============================] - 0s 154us/step - loss: 581.9878 - val_loss: 131.4006\n",
      "Epoch 183/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 605.2905 - val_loss: 135.2232\n",
      "Epoch 184/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 652.5547 - val_loss: 291.4458\n",
      "Epoch 185/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 635.3317 - val_loss: 413.2924\n",
      "Epoch 186/1000\n",
      "165/165 [==============================] - 0s 158us/step - loss: 495.6908 - val_loss: 142.7857\n",
      "Epoch 187/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 499.0712 - val_loss: 146.4733\n",
      "Epoch 188/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 548.8244 - val_loss: 188.4544\n",
      "Epoch 189/1000\n",
      "165/165 [==============================] - 0s 166us/step - loss: 625.7000 - val_loss: 187.1149\n",
      "Epoch 190/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 613.0939 - val_loss: 141.4661\n",
      "Epoch 191/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 713.6291 - val_loss: 243.2241\n",
      "Epoch 192/1000\n",
      "165/165 [==============================] - 0s 168us/step - loss: 444.3637 - val_loss: 208.5724\n",
      "Epoch 193/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 584.1641 - val_loss: 141.7790\n",
      "Epoch 194/1000\n",
      "165/165 [==============================] - 0s 164us/step - loss: 557.7383 - val_loss: 209.6398\n",
      "Epoch 195/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 557.7879 - val_loss: 244.5611\n",
      "Epoch 196/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 498.0999 - val_loss: 168.4685\n",
      "Epoch 197/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 526.8284 - val_loss: 285.6948\n",
      "Epoch 198/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 476.4308 - val_loss: 206.2502\n",
      "Epoch 199/1000\n",
      "165/165 [==============================] - 0s 150us/step - loss: 552.0775 - val_loss: 122.9508\n",
      "Epoch 200/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 530.5548 - val_loss: 245.1470\n",
      "Epoch 201/1000\n",
      "165/165 [==============================] - 0s 153us/step - loss: 546.7997 - val_loss: 258.4630\n",
      "Epoch 202/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 516.1181 - val_loss: 265.9212\n",
      "Epoch 203/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 504.3754 - val_loss: 207.4334\n",
      "Epoch 204/1000\n",
      "165/165 [==============================] - 0s 160us/step - loss: 504.3705 - val_loss: 238.3430\n",
      "Epoch 205/1000\n",
      "165/165 [==============================] - 0s 175us/step - loss: 608.2757 - val_loss: 129.4192\n",
      "Epoch 206/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 541.9158 - val_loss: 208.1278\n",
      "Epoch 207/1000\n",
      "165/165 [==============================] - 0s 155us/step - loss: 532.0386 - val_loss: 183.2785\n",
      "Epoch 208/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 530.7833 - val_loss: 115.8484\n",
      "Epoch 209/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 528.2251 - val_loss: 236.2073\n",
      "Epoch 210/1000\n",
      "165/165 [==============================] - 0s 148us/step - loss: 589.2865 - val_loss: 255.3221\n",
      "Epoch 211/1000\n",
      "165/165 [==============================] - 0s 167us/step - loss: 575.3618 - val_loss: 110.5337\n",
      "Epoch 212/1000\n",
      "165/165 [==============================] - 0s 157us/step - loss: 547.7611 - val_loss: 317.8820\n",
      "Epoch 213/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 592.6140 - val_loss: 226.8004\n",
      "Epoch 214/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 566.6225 - val_loss: 306.2216\n",
      "Epoch 215/1000\n",
      "165/165 [==============================] - 0s 161us/step - loss: 437.6273 - val_loss: 245.8834\n",
      "Epoch 216/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 510.2753 - val_loss: 166.3106\n",
      "Epoch 217/1000\n",
      "165/165 [==============================] - 0s 169us/step - loss: 495.3968 - val_loss: 244.8711\n",
      "Epoch 218/1000\n",
      "165/165 [==============================] - 0s 171us/step - loss: 563.3677 - val_loss: 277.6342\n",
      "Epoch 219/1000\n",
      "165/165 [==============================] - 0s 159us/step - loss: 447.7633 - val_loss: 132.7728\n",
      "Epoch 220/1000\n",
      "165/165 [==============================] - 0s 162us/step - loss: 555.7136 - val_loss: 211.4299\n",
      "Epoch 221/1000\n",
      "165/165 [==============================] - 0s 165us/step - loss: 541.4553 - val_loss: 255.4068\n",
      "Epoch 222/1000\n",
      "165/165 [==============================] - 0s 178us/step - loss: 446.8861 - val_loss: 137.7236\n",
      "Epoch 223/1000\n",
      "165/165 [==============================] - 0s 156us/step - loss: 540.6086 - val_loss: 126.7645\n",
      "Epoch 224/1000\n",
      "165/165 [==============================] - 0s 152us/step - loss: 574.9728 - val_loss: 148.0144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f020444e7d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_11 = Sequential()\n",
    "\n",
    "model_11.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "model_11.add(Dropout(0.1))\n",
    "model_11.add(layers.Dense(units=1024))\n",
    "model_11.add(Dropout(0.1))\n",
    "\n",
    "model_11.add(Dense(units=1, kernel_regularizer=tf.keras.regularizers.l1(0.01)))\n",
    "\n",
    "model_11.compile(optimizer='Adagrad', loss='mean_squared_error')\n",
    "model_11.fit(train_input_11, train_target_11, epochs=1000, batch_size=32, validation_split=0.3, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_12 = Sequential()\n",
    "\n",
    "# model_12.add(layers.Dense(units=1024, input_shape=(4,)))\n",
    "# model_12.add(Dropout(0.1))\n",
    "\n",
    "# model_12.add(layers.Dense(units=128, input_shape=(4,)))\n",
    "# model_12.add(Dropout(0.1))\n",
    "\n",
    "# model_12.add(Dense(units=1))\n",
    "\n",
    "# model_12.compile(optimizer='Adagrad', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE=[]\n",
    "\n",
    "predicted_01 = model_01.predict(test_input_01)\n",
    "actual_01=data_test_01['y']\n",
    "RMSE_01 = mean_squared_error(actual_01, predicted_01)**0.5    \n",
    "RMSE.append(RMSE_01)\n",
    "predicted_02 = model_02.predict(test_input_02)\n",
    "actual_02=data_test_02['y']\n",
    "RMSE_02 = mean_squared_error(actual_02, predicted_02)**0.5    \n",
    "RMSE.append(RMSE_02)\n",
    "predicted_03 = model_03.predict(test_input_03)\n",
    "actual_03=data_test_03['y']\n",
    "RMSE_03 = mean_squared_error(actual_03, predicted_03)**0.5    \n",
    "RMSE.append(RMSE_03)\n",
    "predicted_04 = model_04.predict(test_input_04)\n",
    "actual_04=data_test_04['y']\n",
    "RMSE_04 = mean_squared_error(actual_04, predicted_04)**0.5    \n",
    "RMSE.append(RMSE_04)\n",
    "predicted_05 = model_05.predict(test_input_05)\n",
    "actual_05=data_test_05['y']\n",
    "RMSE_05 = mean_squared_error(actual_05, predicted_05)**0.5    \n",
    "RMSE.append(RMSE_05)\n",
    "predicted_06 = model_06.predict(test_input_06)\n",
    "actual_06=data_test_06['y']\n",
    "RMSE_06 = mean_squared_error(actual_06, predicted_06)**0.5    \n",
    "RMSE.append(RMSE_06)\n",
    "predicted_07= model_07.predict(test_input_07)\n",
    "actual_07=data_test_07['y']\n",
    "RMSE_07 = mean_squared_error(actual_07, predicted_07)**0.5    \n",
    "RMSE.append(RMSE_07)\n",
    "predicted_08 = model_08.predict(test_input_08)\n",
    "actual_08=data_test_08['y']\n",
    "RMSE_08 = mean_squared_error(actual_08, predicted_08)**0.5    \n",
    "RMSE.append(RMSE_08)\n",
    "predicted_09 = model_09.predict(test_input_09)\n",
    "actual_09=data_test_09['y']\n",
    "RMSE_09 = mean_squared_error(actual_09, predicted_09)**0.5    \n",
    "RMSE.append(RMSE_09)\n",
    "predicted_10 = model_10.predict(test_input_10)\n",
    "actual_10=data_test_10['y']\n",
    "RMSE_10 = mean_squared_error(actual_10, predicted_10)**0.5    \n",
    "RMSE.append(RMSE_10)\n",
    "predicted_11 = model_11.predict(test_input_11)\n",
    "actual_11=data_test_11['y']\n",
    "RMSE_11 = mean_squared_error(actual_11, predicted_11)**0.5    \n",
    "RMSE.append(RMSE_11)\n",
    "#predicted_12 = model_12.predict(test_input_12)\n",
    "#actual_12=data_test_12['y']\n",
    "\n",
    "#RMSE_12 = mean_squared_error(actual_12, predicted_12)**0.5    \n",
    "#RMSE_12\n",
    "\n",
    "predicted_01 = sum(predicted_01.tolist(), [])\n",
    "actual_01=actual_01.tolist()\n",
    "final_01=pd.DataFrame([predicted_01, actual_01]).T\n",
    "final_01.columns=['pred','actual']\n",
    "final_01=final_01.set_index(data_test_01.index)\n",
    "\n",
    "predicted_02 = sum(predicted_02.tolist(), [])\n",
    "actual_02=actual_02.tolist()\n",
    "final_02=pd.DataFrame([predicted_02, actual_02]).T\n",
    "final_02.columns=['pred','actual']\n",
    "final_02=final_02.set_index(data_test_02.index)\n",
    "\n",
    "predicted_03 = sum(predicted_03.tolist(), [])\n",
    "actual_03=actual_03.tolist()\n",
    "final_03=pd.DataFrame([predicted_03, actual_03]).T\n",
    "final_03.columns=['pred','actual']\n",
    "final_03=final_03.set_index(data_test_03.index)\n",
    "\n",
    "predicted_04 = sum(predicted_04.tolist(), [])\n",
    "actual_04=actual_04.tolist()\n",
    "final_04=pd.DataFrame([predicted_04, actual_04]).T\n",
    "final_04.columns=['pred','actual']\n",
    "final_04=final_04.set_index(data_test_04.index)\n",
    "\n",
    "predicted_05 = sum(predicted_05.tolist(), [])\n",
    "actual_05=actual_05.tolist()\n",
    "final_05=pd.DataFrame([predicted_05, actual_05]).T\n",
    "final_05.columns=['pred','actual']\n",
    "final_05=final_05.set_index(data_test_05.index)\n",
    "\n",
    "predicted_06 = sum(predicted_06.tolist(), [])\n",
    "actual_06=actual_06.tolist()\n",
    "final_06=pd.DataFrame([predicted_06, actual_06]).T\n",
    "final_06.columns=['pred','actual']\n",
    "final_06=final_06.set_index(data_test_06.index)\n",
    "\n",
    "predicted_07 = sum(predicted_07.tolist(), [])\n",
    "actual_07=actual_07.tolist()\n",
    "final_07=pd.DataFrame([predicted_07, actual_07]).T\n",
    "final_07.columns=['pred','actual']\n",
    "final_07=final_07.set_index(data_test_07.index)\n",
    "\n",
    "predicted_08 = sum(predicted_08.tolist(), [])\n",
    "actual_08=actual_08.tolist()\n",
    "final_08=pd.DataFrame([predicted_08, actual_08]).T\n",
    "final_08.columns=['pred','actual']\n",
    "final_08=final_08.set_index(data_test_08.index)\n",
    "\n",
    "predicted_09 = sum(predicted_09.tolist(), [])\n",
    "actual_09=actual_09.tolist()\n",
    "final_09=pd.DataFrame([predicted_09, actual_09]).T\n",
    "final_09.columns=['pred','actual']\n",
    "final_09=final_09.set_index(data_test_09.index)\n",
    "\n",
    "predicted_10 = sum(predicted_10.tolist(), [])\n",
    "actual_10=actual_10.tolist()\n",
    "final_10=pd.DataFrame([predicted_10, actual_10]).T\n",
    "final_10.columns=['pred','actual']\n",
    "final_10=final_10.set_index(data_test_10.index)\n",
    "\n",
    "predicted_11 = sum(predicted_11.tolist(), [])\n",
    "actual_11=actual_11.tolist()\n",
    "final_11=pd.DataFrame([predicted_11, actual_11]).T\n",
    "final_11.columns=['pred','actual']\n",
    "final_11=final_11.set_index(data_test_11.index)\n",
    "\n",
    "#redicted_12 = sum(predicted_12.tolist(), [])\n",
    "#ctual_12=actual_12.tolist()\n",
    "#inal_12=pd.DataFrame([predicted_12, actual_12]).T\n",
    "#inal_12.columns=['pred','actual']\n",
    "#inal_12=final_12.set_index(data_test_12.index)\n",
    "\n",
    "#train set의 마지막 데이터\n",
    "lastday_01=rate_train_01[-1:]['rate']\n",
    "lastday_02=rate_train_02[-1:]['rate']\n",
    "lastday_03=rate_train_03[-1:]['rate']\n",
    "lastday_04=rate_train_04[-1:]['rate']\n",
    "lastday_05=rate_train_05[-1:]['rate']\n",
    "lastday_06=rate_train_06[-1:]['rate']\n",
    "lastday_07=rate_train_07[-1:]['rate']\n",
    "lastday_08=rate_train_08[-1:]['rate']\n",
    "lastday_09=rate_train_09[-1:]['rate']\n",
    "lastday_10=rate_train_10[-1:]['rate']\n",
    "lastday_11=rate_train_11[-1:]['rate']\n",
    "#lastday_12=rate_train_12[-1:]['rate']\n",
    "actual_updown_01=[]\n",
    "actual_updown_02=[]\n",
    "actual_updown_03=[]\n",
    "actual_updown_04=[]\n",
    "actual_updown_05=[]\n",
    "actual_updown_06=[]\n",
    "actual_updown_07=[]\n",
    "actual_updown_08=[]\n",
    "actual_updown_09=[]\n",
    "actual_updown_10=[]\n",
    "actual_updown_11=[]\n",
    "actual_updown_12=[]\n",
    "pred_updown_01=[]\n",
    "pred_updown_02=[]\n",
    "pred_updown_03=[]\n",
    "pred_updown_04=[]\n",
    "pred_updown_05=[]\n",
    "pred_updown_06=[]\n",
    "pred_updown_07=[]\n",
    "pred_updown_08=[]\n",
    "pred_updown_09=[]\n",
    "pred_updown_10=[]\n",
    "pred_updown_11=[]\n",
    "pred_updown_12=[]\n",
    "if final_01['actual'][0] > lastday_01[0]:\n",
    "    actual_updown_01.append('up')\n",
    "else :\n",
    "    actual_updown_01.append('down')\n",
    "    \n",
    "if final_01['pred'][0] > lastday_01[0]:\n",
    "    pred_updown_01.append('up')\n",
    "else :\n",
    "    pred_updown_01.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_01)-1):\n",
    "    \n",
    "    if final_01['actual'][i+1] > final_01['actual'][i]:\n",
    "        actual_updown_01.append('up')\n",
    "    else :\n",
    "        actual_updown_01.append('down')\n",
    "        \n",
    "for i in range(len(final_01)-1):\n",
    "    \n",
    "    if final_01['pred'][i+1] > final_01['actual'][i]:\n",
    "        pred_updown_01.append('up')\n",
    "    else :\n",
    "        pred_updown_01.append('down')\n",
    "        \n",
    "\n",
    "        ##\n",
    "        \n",
    "        \n",
    "if final_02['actual'][0] > lastday_02[0]:\n",
    "    actual_updown_02.append('up')\n",
    "else :\n",
    "    actual_updown_02.append('down')\n",
    "    \n",
    "if final_02['pred'][0] > lastday_02[0]:\n",
    "    pred_updown_02.append('up')\n",
    "else :\n",
    "    pred_updown_02.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_02)-1):\n",
    "    \n",
    "    if final_02['actual'][i+1] > final_02['actual'][i]:\n",
    "        actual_updown_02.append('up')\n",
    "    else :\n",
    "        actual_updown_02.append('down')\n",
    "        \n",
    "for i in range(len(final_02)-1):\n",
    "    \n",
    "    if final_02['pred'][i+1] > final_02['actual'][i]:\n",
    "        pred_updown_02.append('up')\n",
    "    else :\n",
    "        pred_updown_02.append('down')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "if final_03['actual'][0] > lastday_03[0]:\n",
    "    actual_updown_03.append('up')\n",
    "else :\n",
    "    actual_updown_03.append('down')\n",
    "    \n",
    "if final_03['pred'][0] > lastday_03[0]:\n",
    "    pred_updown_03.append('up')\n",
    "else :\n",
    "    pred_updown_03.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_03)-1):\n",
    "    \n",
    "    if final_03['actual'][i+1] > final_03['actual'][i]:\n",
    "        actual_updown_03.append('up')\n",
    "    else :\n",
    "        actual_updown_03.append('down')\n",
    "        \n",
    "for i in range(len(final_03)-1):\n",
    "    \n",
    "    if final_03['pred'][i+1] > final_03['actual'][i]:\n",
    "        pred_updown_03.append('up')\n",
    "    else :\n",
    "        pred_updown_03.append('down')\n",
    "        \n",
    "        \n",
    "                ##\n",
    "\n",
    "\n",
    "if final_04['actual'][0] > lastday_04[0]:\n",
    "    actual_updown_04.append('up')\n",
    "else :\n",
    "    actual_updown_04.append('down')\n",
    "    \n",
    "if final_04['pred'][0] > lastday_04[0]:\n",
    "    pred_updown_04.append('up')\n",
    "else :\n",
    "    pred_updown_04.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_04)-1):\n",
    "    \n",
    "    if final_04['actual'][i+1] > final_04['actual'][i]:\n",
    "        actual_updown_04.append('up')\n",
    "    else :\n",
    "        actual_updown_04.append('down')\n",
    "        \n",
    "for i in range(len(final_04)-1):\n",
    "    \n",
    "    if final_04['pred'][i+1] > final_04['actual'][i]:\n",
    "        pred_updown_04.append('up')\n",
    "    else :\n",
    "        pred_updown_04.append('down')\n",
    "        \n",
    "        \n",
    "                ##\n",
    "\n",
    "\n",
    "if final_05['actual'][0] > lastday_05[0]:\n",
    "    actual_updown_05.append('up')\n",
    "else :\n",
    "    actual_updown_05.append('down')\n",
    "    \n",
    "if final_05['pred'][0] > lastday_05[0]:\n",
    "    pred_updown_05.append('up')\n",
    "else :\n",
    "    pred_updown_05.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_05)-1):\n",
    "    \n",
    "    if final_05['actual'][i+1] > final_05['actual'][i]:\n",
    "        actual_updown_05.append('up')\n",
    "    else :\n",
    "        actual_updown_05.append('down')\n",
    "        \n",
    "for i in range(len(final_05)-1):\n",
    "    \n",
    "    if final_05['pred'][i+1] > final_05['actual'][i]:\n",
    "        pred_updown_05.append('up')\n",
    "    else :\n",
    "        pred_updown_05.append('down')\n",
    "        \n",
    "        \n",
    "                ##\n",
    "\n",
    "\n",
    "if final_06['actual'][0] > lastday_06[0]:\n",
    "    actual_updown_06.append('up')\n",
    "else :\n",
    "    actual_updown_06.append('down')\n",
    "    \n",
    "if final_06['pred'][0] > lastday_06[0]:\n",
    "    pred_updown_06.append('up')\n",
    "else :\n",
    "    pred_updown_06.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_06)-1):\n",
    "    \n",
    "    if final_06['actual'][i+1] > final_06['actual'][i]:\n",
    "        actual_updown_06.append('up')\n",
    "    else :\n",
    "        actual_updown_06.append('down')\n",
    "        \n",
    "for i in range(len(final_06)-1):\n",
    "    \n",
    "    if final_06['pred'][i+1] > final_06['actual'][i]:\n",
    "        pred_updown_06.append('up')\n",
    "    else :\n",
    "        pred_updown_06.append('down')\n",
    "        \n",
    "\n",
    "        \n",
    "                ##\n",
    "\n",
    "            \n",
    "if final_07['actual'][0] > lastday_07[0]:\n",
    "    actual_updown_07.append('up')\n",
    "else :\n",
    "    actual_updown_07.append('down')\n",
    "    \n",
    "if final_07['pred'][0] > lastday_07[0]:\n",
    "    pred_updown_07.append('up')\n",
    "else :\n",
    "    pred_updown_07.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_07)-1):\n",
    "    \n",
    "    if final_07['actual'][i+1] > final_07['actual'][i]:\n",
    "        actual_updown_07.append('up')\n",
    "    else :\n",
    "        actual_updown_07.append('down')\n",
    "        \n",
    "for i in range(len(final_07)-1):\n",
    "    \n",
    "    if final_07['pred'][i+1] > final_07['actual'][i]:\n",
    "        pred_updown_07.append('up')\n",
    "    else :\n",
    "        pred_updown_07.append('down')\n",
    "        \n",
    "\n",
    "                ##\n",
    "\n",
    "        \n",
    "        \n",
    "if final_08['actual'][0] > lastday_08[0]:\n",
    "    actual_updown_08.append('up')\n",
    "else :\n",
    "    actual_updown_08.append('down')\n",
    "    \n",
    "if final_08['pred'][0] > lastday_08[0]:\n",
    "    pred_updown_08.append('up')\n",
    "else :\n",
    "    pred_updown_08.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_08)-1):\n",
    "    \n",
    "    if final_08['actual'][i+1] > final_08['actual'][i]:\n",
    "        actual_updown_08.append('up')\n",
    "    else :\n",
    "        actual_updown_08.append('down')\n",
    "        \n",
    "for i in range(len(final_08)-1):\n",
    "    \n",
    "    if final_08['pred'][i+1] > final_08['actual'][i]:\n",
    "        pred_updown_08.append('up')\n",
    "    else :\n",
    "        pred_updown_08.append('down')\n",
    "        \n",
    "\n",
    "        \n",
    "                ##\n",
    "\n",
    "        \n",
    "if final_09['actual'][0] > lastday_09[0]:\n",
    "    actual_updown_09.append('up')\n",
    "else :\n",
    "    actual_updown_09.append('down')\n",
    "    \n",
    "if final_09['pred'][0] > lastday_09[0]:\n",
    "    pred_updown_09.append('up')\n",
    "else :\n",
    "    pred_updown_09.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_09)-1):\n",
    "    \n",
    "    if final_09['actual'][i+1] > final_09['actual'][i]:\n",
    "        actual_updown_09.append('up')\n",
    "    else :\n",
    "        actual_updown_09.append('down')\n",
    "        \n",
    "for i in range(len(final_09)-1):\n",
    "    \n",
    "    if final_09['pred'][i+1] > final_09['actual'][i]:\n",
    "        pred_updown_09.append('up')\n",
    "    else :\n",
    "        pred_updown_09.append('down')\n",
    "        \n",
    "        \n",
    "        \n",
    "                ##\n",
    "\n",
    "\n",
    "if final_10['actual'][0] > lastday_10[0]:\n",
    "    actual_updown_10.append('up')\n",
    "else :\n",
    "    actual_updown_10.append('down')\n",
    "    \n",
    "if final_10['pred'][0] > lastday_10[0]:\n",
    "    pred_updown_10.append('up')\n",
    "else :\n",
    "    pred_updown_10.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_10)-1):\n",
    "    \n",
    "    if final_10['actual'][i+1] > final_10['actual'][i]:\n",
    "        actual_updown_10.append('up')\n",
    "    else :\n",
    "        actual_updown_10.append('down')\n",
    "        \n",
    "for i in range(len(final_10)-1):\n",
    "    \n",
    "    if final_10['pred'][i+1] > final_10['actual'][i]:\n",
    "        pred_updown_10.append('up')\n",
    "    else :\n",
    "        pred_updown_10.append('down')\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "                ##\n",
    "\n",
    "            \n",
    "            \n",
    "if final_11['actual'][0] > lastday_11[0]:\n",
    "    actual_updown_11.append('up')\n",
    "else :\n",
    "    actual_updown_11.append('down')\n",
    "    \n",
    "if final_11['pred'][0] > lastday_11[0]:\n",
    "    pred_updown_11.append('up')\n",
    "else :\n",
    "    pred_updown_11.append('down')\n",
    "\n",
    "\n",
    "for i in range(len(final_11)-1):\n",
    "    \n",
    "    if final_11['actual'][i+1] > final_11['actual'][i]:\n",
    "        actual_updown_11.append('up')\n",
    "    else :\n",
    "        actual_updown_11.append('down')\n",
    "        \n",
    "for i in range(len(final_11)-1):\n",
    "    \n",
    "    if final_11['pred'][i+1] > final_11['actual'][i]:\n",
    "        pred_updown_11.append('up')\n",
    "    else :\n",
    "        pred_updown_11.append('down')\n",
    "# if final_12['actual'][0] > lastday_12[0]:\n",
    "#     actual_updown_12.append('up')\n",
    "# else :\n",
    "#     actual_updown_12.appedn('down')\n",
    "    \n",
    "# if final_12['pred'][0] > lastday_12[0]:\n",
    "#     pred_updown_12.append('up')\n",
    "# else :\n",
    "#     pred_updown_12.append('down')\n",
    "\n",
    "\n",
    "# for i in range(len(final_12)-1):\n",
    "    \n",
    "#     if final_12['actual'][i+1] > final_12['actual'][i]:\n",
    "#         actual_updown_12.append('up')\n",
    "#     else :\n",
    "#         actual_updown_12.append('down')\n",
    "        \n",
    "# for i in range(len(final_12)-1):\n",
    "    \n",
    "#     if final_12['pred'][i+1] > final_12['actual'][i]:\n",
    "#         pred_updown_12.append('up')\n",
    "#     else :\n",
    "#         pred_updown_12.append('down')\n",
    "# '\n",
    "\n",
    "final_01['pred_updown']=pred_updown_01\n",
    "final_01['actual_updown']=actual_updown_01\n",
    "\n",
    "final_02['pred_updown']=pred_updown_02\n",
    "final_02['actual_updown']=actual_updown_02\n",
    "\n",
    "final_03['pred_updown']=pred_updown_03\n",
    "final_03['actual_updown']=actual_updown_03\n",
    "\n",
    "final_04['pred_updown']=pred_updown_04\n",
    "final_04['actual_updown']=actual_updown_04\n",
    "\n",
    "final_05['pred_updown']=pred_updown_05\n",
    "final_05['actual_updown']=actual_updown_05\n",
    "\n",
    "final_06['pred_updown']=pred_updown_06\n",
    "final_06['actual_updown']=actual_updown_06\n",
    "\n",
    "final_07['pred_updown']=pred_updown_07\n",
    "final_07['actual_updown']=actual_updown_07\n",
    "\n",
    "final_08['pred_updown']=pred_updown_08\n",
    "final_08['actual_updown']=actual_updown_08\n",
    "\n",
    "final_09['pred_updown']=pred_updown_09\n",
    "final_09['actual_updown']=actual_updown_09\n",
    "\n",
    "final_10['pred_updown']=pred_updown_10\n",
    "final_10['actual_updown']=actual_updown_10\n",
    "\n",
    "final_11['pred_updown']=pred_updown_11\n",
    "final_11['actual_updown']=actual_updown_11\n",
    "\n",
    "#final_12['pred_updown']=pred_updown_12\n",
    "#final_12['actual_updown']=actual_updown_12\n",
    "\n",
    "day5=[]\n",
    "month=[]\n",
    "month.append(round(sum(final_01['pred_updown'] == final_01['actual_updown'])/len(final_01)*100,2))\n",
    "final_5_01=final_01[:5]\n",
    "day5.append(round(sum(final_5_01['pred_updown'] == final_5_01['actual_updown'])/len(final_5_01)*100,2))\n",
    "month.append(round(sum(final_02['pred_updown'] == final_02['actual_updown'])/len(final_02)*100,2))\n",
    "final_5_02=final_02[:5]\n",
    "day5.append(round(sum(final_5_02['pred_updown'] == final_5_02['actual_updown'])/len(final_5_02)*100,2))\n",
    "month.append(round(sum(final_03['pred_updown'] == final_03['actual_updown'])/len(final_03)*100,2))\n",
    "final_5_03=final_03[:5]\n",
    "day5.append(round(sum(final_5_03['pred_updown'] == final_5_03['actual_updown'])/len(final_5_03)*100,2))\n",
    "month.append(round(sum(final_04['pred_updown'] == final_04['actual_updown'])/len(final_04)*100,2))\n",
    "final_5_04=final_04[:5]\n",
    "day5.append(round(sum(final_5_04['pred_updown'] == final_5_04['actual_updown'])/len(final_5_04)*100,2))\n",
    "month.append(round(sum(final_05['pred_updown'] == final_05['actual_updown'])/len(final_05)*100,2))\n",
    "final_5_05=final_05[:5]\n",
    "day5.append(round(sum(final_5_05['pred_updown'] == final_5_05['actual_updown'])/len(final_5_05)*100,2))\n",
    "month.append(round(sum(final_06['pred_updown'] == final_06['actual_updown'])/len(final_06)*100,2))\n",
    "final_5_06=final_06[:5]\n",
    "day5.append(round(sum(final_5_06['pred_updown'] == final_5_06['actual_updown'])/len(final_5_06)*100,2))\n",
    "month.append(round(sum(final_07['pred_updown'] == final_07['actual_updown'])/len(final_07)*100,2))\n",
    "final_5_07=final_07[:5]\n",
    "day5.append(round(sum(final_5_07['pred_updown'] == final_5_07['actual_updown'])/len(final_5_07)*100,2))\n",
    "month.append(round(sum(final_08['pred_updown'] == final_08['actual_updown'])/len(final_08)*100,2))\n",
    "final_5_08=final_08[:5]\n",
    "day5.append(round(sum(final_5_08['pred_updown'] == final_5_08['actual_updown'])/len(final_5_08)*100,2))\n",
    "month.append(round(sum(final_09['pred_updown'] == final_09['actual_updown'])/len(final_09)*100,2))\n",
    "final_5_09=final_09[:5]\n",
    "day5.append(round(sum(final_5_09['pred_updown'] == final_5_09['actual_updown'])/len(final_5_09)*100,2))\n",
    "month.append(round(sum(final_10['pred_updown'] == final_10['actual_updown'])/len(final_10)*100,2))\n",
    "final_5_10=final_10[:5]\n",
    "day5.append(round(sum(final_5_10['pred_updown'] == final_5_10['actual_updown'])/len(final_5_10)*100,2))\n",
    "month.append(round(sum(final_11['pred_updown'] == final_11['actual_updown'])/len(final_11)*100,2))\n",
    "final_5_11=final_11[:5]\n",
    "day5.append(round(sum(final_5_11['pred_updown'] == final_5_11['actual_updown'])/len(final_5_11)*100,2))\n",
    "\n",
    "# print('set12')\n",
    "# # 한달간 예측\n",
    "# print('한달간 상승,하락 정확도 : ',round(sum(final_12['pred_updown'] == final_12['actual_updown'])/len(final_12)*100,2), '%')\n",
    "# #5일만 예측\n",
    "# final_5_12=final_12[:5]\n",
    "# print('5일간 상승,하락 정확도 : ',round(sum(final_5_12['pred_updown'] == final_5_12['actual_updown'])/len(final_5_12)*100,2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE평균 :  31.165023339964353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.544493305558763,\n",
       " 18.869483424519995,\n",
       " 12.597135713120279,\n",
       " 37.443381853528024,\n",
       " 21.611272000935138,\n",
       " 43.046466893297776,\n",
       " 53.48147545351258,\n",
       " 33.55545207135169,\n",
       " 75.56843884665943,\n",
       " 34.654405143117266,\n",
       " 7.443252034006965]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('RMSE평균 : ',np.mean(RMSE))\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5일 업다운 정확도 :  36.36363636363637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[60.0, 100.0, 40.0, 0.0, 80.0, 20.0, 0.0, 0.0, 20.0, 20.0, 60.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('5일 업다운 정확도 : ' ,np.mean(day5))\n",
    "day5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한달 업다운 정확도:  44.89272727272727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[46.67, 68.42, 30.0, 15.79, 77.78, 38.1, 35.0, 47.06, 35.0, 52.63, 47.37]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('한달 업다운 정확도: ' ,np.mean(month))\n",
    "month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
